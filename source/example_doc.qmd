---
toc-depth: 2
---

<div style="text-align: justify">

## An example notebook

If you want to see an example for documented code, check out [an example](https://github.com/ndombrowski/cli_workshop/example_doc) for how I would might have recorded how I analysed some sequence data myself.

The link above leads you to an example for:

-   How could I use github to make my code available to others
-   How could a "code book" look like? The example you see in the folder is provided a markdown file [here](https://github.com/ndombrowski/cli_workshop/example_doc/example_notebook.qmd) and for convience the report was also rendered as a html to send it to collaborators.

The example is based on the data you analyse throughout this tutorial, so the actual code might only make sense after you have finished the tutorial. Feel free to revisit this page after you have written some code yourself.

Please note that this is just an example to get you started and such a report might look different depending on your needs.

Below you find some general hints for what to put into different sections that you can see in the qmd file.

### YAML headers

![](/img/yaml_header.png){width="753"}

### Setting up your working directory 

![](/img/general_setup.png){width="549"}

In this first section I add everything that a user that wants to use my workflow has to change and I try to standardize the code below, so that another user could just run this as is without having to edit anything. Typically things to add are:

-   From where to start the analyses
-   Custom paths, to for example databases that need to be downloaded from elsewhere
-   Custom variables: In the example above I store the link to the data in a variable called `download_link`, I then use the variable in the code below to download the data. By doing it this way I have one location in the code I need to change things when for example the path to the data changes, the code below stays the same even if I would need to use the `download_link` multiple times
-   ...

I tend to NOT add the information how I use `scp` to log into an HPC to keep my user name and login information private.

### Sanity checks

![](/img/sanity_check.png){width="561"}

When I first start working with new data, I try to add as many sanity checks as possible to ensure that my data looks good. That way I avoid that I don't notice an issue and run into trouble further down the line. I at the same time understand my data more and learn with how many samples and how much data I am working with.

I also add such sanity checks whenever I modify my data. For example, when I merge individual files into a large file I might count the lines for the individual files and the combined file simply to ensure that I used my wildcards correctly.

Remember: The computer is only as smart as the person using it and will blindly run your commands.

### Documenting external scripts

![](/img/external_scripts.png){width="559"}

This part might make more sense after you have worked through the part of the tutorial about using an HPC. But what you see here is how I have written down code that simply says that I submitted a script to a HPC but it does not actually say how I ran the FastQC software. The actual code is "hidden" inside of the run_fastqc.sh script. This also means that a person reading your workflow does not have the code right away. You can deal with this in two ways.

1.  Instead of the sbatch command, you can add the actual line of code that was run on the compute node.
2.  When publishing your code with your manuscript, add the whole scripts folder to where you publish the main code, i.e. on [github](https://github.com/ndombrowski/cli_workshop/example_doc) or [zenodo](https://zenodo.org/records/3839790)

I tend to prefer number 2 because I like to record the code in the readme exactly as I ran it but you can do it differently as long as all the code you ran is recorded and accessible to others once you publish your data.