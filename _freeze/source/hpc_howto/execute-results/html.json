{
  "hash": "cd7eba082ab6308cac633387e23c3f6b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\nexecute:\n  eval: false\nengine: knitr\nbibliography: references.bib\n---\n\n\n<div style=\"text-align: justify\">\n\n# Using an HPC\n\nNow, that we are familiar with using the cli, let's upload our sequencing data to an HPC an run some software to check the quality of our reads.\n\n## `ssh`: Connecting to a sever\n\n**SSH** (Secure Shell) is a network protocol that enables secure remote connections between two systems. The basic command to login looks like this:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#connect to a server\nssh -X username@server\n```\n:::\n\n\nOptions:\n\n-   `-X` option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. We use it since it enables users to run graphical applications on a remote server \n\nIf you have access to and want to connect to crunchomics you would connect like this:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nssh -X uvanetid1@omics-h0.science.uva.nl\n```\n:::\n\n\n::: callout-important\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. \nIf you have not set that up and/or have trouble doing so, contact ICT.\n:::\n\n\n## Crunchomics: Preparing your account\n\nIf you have not used Crunchomics before, then you want to run a small Bash script that:\n\n- Allows you to use system-wide installed software, by adding `/zfs/omics/software/bin` to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is already installed\n- Sets up a python3 environment and some usefull python packages\n- Have a link for your 500 GB personal directory in your home directory \n\nTo set this up, run the following in the cli:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n/zfs/omics/software/script/omics_install_script\n```\n:::\n\n\n\n## `scp`: Transferring data from/to a server\n\n`scp` stands for Secure Copy Protocol and allows us to securely copy files and directories between remote hosts. When transferring data the transfer is prepared from the terminal of your local computer and not from the HPCs login node.\n\nThe basic syntax is:\n\n`scp [options] SOURCE DESTINATION`\n\nTo start analysing our data, we want to move our fastq.gz files from our local folder, the source, to a folder on Cruncomics, the destination. Let's start by:\n\n- Moving from our home directory into our personal directory. We move there since we have more space in the personal directory. Notice, for bash to find the personal directory, we need to have run the `omics_install_script` script above first\n- Make a project folder with a descriptive file name\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncd personal/\nmkdir projectX\ncd projectX\n\n```\n:::\n\n\nInto our project directory, we the want to move the data we have before downloaded with our sequencing data. We do this by moving the `data` folder we generated before from our local computer to the HPC. Therefore, it is important that:\n\n- we run the following command from the cli on our own computer and not from the cli while being logged into the HPC! \n- you exchange the two instances of `username` in the code below with your username\n  \nI am running the code from the `data_analysis` folder that we have generated in the previous tutorial  and I use the `-r` option since we are moving a folder, not a file.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/projectX\n\n#check if that worked \nll data/seq_project/*/*fastq.gz\n```\n:::\n\n\n::: {.callout-tip title=\"Tip: moving data from the HPC to our own computer\" collapse=\"true\"}\n\nWe can also move data from the HPC to our own computer. For example, let's assume we want to move a single sequencing file. In this case, \n\n- We do not need `-r` since we only move a single file.\n- We again run this command from a terminal on our computer, not while being logged in the HPC\n- We use `.` to indicate that we want to move the file into the directory we are currently in. If we want to move the file elsewhere we can use any absolute or relative path as needed\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz .\n```\n:::\n\n\n:::\n\n\n::: {.callout-tip title=\"Tip: moving data from the HPC using wildcards\" collapse=\"true\"}\n\nWe can also move several files at once and ignore the whole folder structure by using wildcards when using `scp`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#make a random directory to move our data into for testing purposes\nmkdir transfer_test\n\nscp data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\n#view the data, see how the folder structure is different compared to our first example? \nll transfer_test/*\n```\n:::\n\n\n**Notice for MAC users**:\n\nFor Mac users that work with an zsh shell this might not work and they might get an error like \"file not found\", \"no matches found\" or something the like.\nWithout going into details zsh has a slightly different way of handling wildcards and tries to interpret the wildcard literally and tries not to extend the wildcard.\nIf you see the error and you are sure the file exists it should work to edit your line of code as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\\scp  data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n```\n:::\n\n\nIf that does not work, these are some other things to try (different zsh environments might need slightly different solutions):\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nnoglob scp  data/seq_project/barcode00*/*fastq.gz ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\nscp  'data/seq_project/barcode00*/*fastq.gz' ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n```\n:::\n\n:::\n\n\n## Slurm basics\n\n### Get information about the cluster\n\nNow, that we have our data prepared we want to run a tool to assess the quality of our reads. Before doing that, let's talk about submitting jobs to an HPC.\n\nWhen getting started on a new HPC it is good to know how to get basic information about what nodes are avaiable on a cluster by typing the following command into the cli:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsinfo\n```\n:::\n\n\n\nWe will see something like this:\n\n![](../img/sinfo.png){width=\"70%\" fig-align=\"left\"}\n\nHere, you see information about the:\n\n-   partition: the queues that are available\n-   state: if a node is busy or not\n    -   mix : consumable resources partially allocated\n    -   idle : available to requests consumable resources\n    -   drain : unavailable for use per system administrator request\n    -   alloc : consumable resources fully allocated\n    -   down : unavailable for use.\n-   Nodes: The number of nodes\n-   NodeList: the names of the nodes omics-cn001 to omics-cn005\n\n\n### View info about jobs in the queue\n\nThe following command gives us some information about how busy the HPC is:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsqueue\n```\n:::\n\n\nAfter running this, you can see all jobs scheduled on the HPC:\n\n![](../img/squeue.png){width=\"50%\" fig-align=\"left\"}\n\n-   JOBID: every job gets a number and you can manipulate jobs via this number\n-   ST: Job state codes that describe the current state of the job. The full list of abbreviations can be found [here](https://curc.readthedocs.io/en/latest/running-jobs/squeue-status-codes.html)\n\nIf we would have submitted a job, we also should see it running.\n\n\n\n## `srun`: submitting a job interactively\n\n`srun` is used when you want to run tasks interactively or have more control over the execution. You directly issue srun commands in the terminal, specifying the tasks to be executed and their resource requirements.\n\nUse `srun` when:\n\n-   You want to run tasks interactively and need immediate feedback.\n-   You are testing or debugging your commands before incorporating them into a script.\n-   You need more control over the execution of tasks.\n\nLet's submit a very simple example for which we would not even need to submit a job, but just to get you started.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsrun echo \"Hello interactively\"\n```\n:::\n\n\nYou should see the output of echo printed to the screen and if you would run `squeue` you won't even see your job since everything ran so fast. Now assume you want to run a more complex interactive task with srun then it is good to specify the resources your job needs by adding flags, i.e.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsrun --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=1G echo \"Hello interactively\"\n```\n:::\n\n\nHere, each flag means the following:\n\n-   `--nodes=1`: Specifies the number of nodes. In this case, it's set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit.\n-   `--ntasks=1`: Defines the number of tasks to run. Here, it's set to 1 since we want to use echo once\n-   `--cpus-per-task=1`: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task\n-   `--mem=1G`: Sets the memory requirement for the task. Modify this based on your task's memory needs\n-   `echo \"Hello interactively`: The actual command you want to run interactively\n\n\n### Choosing the right amount of resources\n\nWhen you're just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\n-   Use default settings: Begin by working with the default settings provided by the HPC cluster. These are often set to provide a balanced resource allocation for a wide range of tasks\n-   Check the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\n-   Testing with Small Datasets: For initial testing and debugging, consider working with a smaller subset of your dataset. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\n-   Monitor the resources usage:\n    -   Use `sacct` to check what resources a finished job has used. Look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (--mem) was appropriate. An example command would be `sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,nnodes,ncpus,nodelist`\n    -   For instance, if you would have used `--cpus-per-task=4 --mem=4G`, you would expect to use a total of 16 GB of memory (4 CPUs \\* 4 GB). Verify this with sacct to ensure your job's resource requirements align with its actual usage.\n-   Fine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n### Run FastQC with srun\n\n[FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) is a quality control tool for high throughput sequence data that is already installed on crunchomics. Let's use this to run an actually software on our data.\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir -p results/fastqc \n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\nll results/fastqc /*\n```\n:::\n\n\nSince we work with little data this will run extremely fast, however, if you would be logged into Crunchomics via a second window and run `squeue` you should see that your job is actively running:\n\n![](../img/squeue2.png){width=\"50%\" fig-align=\"left\"}\n\nAdditionally, after the run is completed, you should see that several html files were generated in our fastqc folder.\n\n::: {.callout-caution collapse=\"false\" title=\"Exercise\"}\n\nUse scp to download the data to your own computer and view one of the html files.\n\n<details>\n<summary>Click me to see an answer</summary>\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/results/fastqc/*html results/fastqc/\n```\n:::\n\n\nYou could also open a file on Crunchomics with `firefox results/fastqc/Sample-DUMMY1_R1_fastqc.html`. However, connecting to the internet via a cli on a remote server tends to be rather slow and its often better to view files to your own computer especially if they are large files.\n\nIf you want to know more about how to to interprete the output, you can visit [the fastqc website](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), which gives some examples for interpreting good and bad reports.\n\n</details>\n:::\n\n\n## `screen`: Submitting long running jobs via srun\n\nOne down-side of `srun` for long running jobs is that your terminal gets \"blocked\" as long as the job is running and that your job is lost if you loose the ssh connection to the HPC. For long running jobs, there are two ways to deal with this:\n\n1. submit a srun job in a screen\n2. use sbatch \n\nIn this section, we will cover how to use Screen. Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\n\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\n\nWe start a screen as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nscreen\n```\n:::\n\n\nWe detach (go outside of a screen but keep the screen running in the background) from a screen with `control+a+d`.\n\nIf you run multiple things, it can be useful to give your screens more descriptive names. You can do this as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#start a screen and give it a name\nscreen -S run_fastqc\n```\n:::\n\n\nAfter detaching from a screen you can list all currently running screens with:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nscreen -ls\n```\n:::\n\n\nYou can restart a screen like this:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#restart an existing screen\nscreen -r run_fastqc\n```\n:::\n\n\nNow inside our screen, we can run fastqc same as we did before:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1 \n```\n:::\n\n\nAnd for long-running jobs we can jump inside and outside of the job, while it is running and at the same time do other things from the cli.\n\nIf you want to completely close and remove a screen, type the following while being inside of the screen:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nexit\n```\n:::\n\n\n\n## `sbatch`: submitting a job\n\n`sbatch` is your go-to command when you have a script (a batch script) that needs to be executed without direct user interaction.\n\nUse sbatch when:\n\n-   You have long-running or resource-intensive tasks.\n-   You want to submit jobs that can run independently without your immediate supervision\n-   You want to submit multiple jobs at once\n\nTo run a job script, you:\n\n-   create a script that contains all the commands and configurations needed for your job\n-   use sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nLet's start with having some good folder organization to keep our project folder organized:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir scripts \nmkdir logs\n```\n:::\n\n\nTo get started, assume we have created a script named `run_fastqc.sh` with the following content in which we want to run fastqc. Notice, how in this script I added some additional commands. Here, I just use this to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you could do it.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n```\n:::\n\n\nTo prepare the script:\n\n- run `nano scripts/run_fastqc.sh`. \n- Copy and paste the content you see above. \n- Press `ctrl+x` to exit nano \n- Type `Y` when prompted if the changes should be saved. \n- Confirm the name by pressing enter\n\nThe we can submit `run_fastqc.sh` with:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#submit job: 754\nsbatch sbatch scripts/run_fastqc.sh\n\n#check if job is running correctly\nsqueue\n```\n:::\n\n\nAfter running this, you should see that the job was submitted and something like this printed to the screen `Submitted batch job 754`.\n\nYou will also see that a new file is generated that will look something like this `slurm-425707.out`. When you submit a batch job using sbatch, Slurm redirects the standard output and standard error streams of your job to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\n\nThis file is useful as it:\n\n-   Captures the output of our batch scripts and stores them in a file\n-   Can be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here.\n\nFeel free to explore the content of the log file, do you see how the echo commands are used as well?\n\n\n::: {.callout-tip title=\"Tip: sbatch and better log files\" collapse=\"true\"}\n\nWe have seen that by default sbatch redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important, you find below an example to:\n\n- Store the standard output and error in two separate files \n- Redirect the output into another folder, the logs folder\n- In the code below,  the `%j` is replaced with the job allocation number\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --job-name=our_fastqc_job\n#SBATCH --output=logs/fastqc_%j.out\n#SBATCH --error=logs/fastqc_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n```\n:::\n\n\n:::\n\n\n\n::: {.callout-tip title=\"Advanced tip: sbatch and multiple files\" collapse=\"true\"}\n\nWith fastqc we are very lucky that it can identify all the fastq files in the directory we specify with `-o` and use a wildcard. This is extremely useful for us but by far not all programs work this way.\n\nFor this section, lets assume that we need to provide each individual file we want to analyse, one by one. How would we run such a job effectively?\n\nWhat we want to do is created what is called a job array that allows us to:\n\n- Run multiple jobs that have the same job definition, i.e. cpus, memory and software used\n- Run these job in the most optimal way. I.e. we do not want to run one job after each other but we also want to run jobs in parallel at the same time to optimize resource usage. \n\nLet's start with making a list with files we want to work with based on what we have already learned:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nls data/seq_project/*/*.gz | cut -f4 -d \"/\" > samples.txt\n```\n:::\n\n\nNext, we can use this text file in our job array, the content of which we store in `scripts/array.sh`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n\n#SBATCH --job-name=my_array\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --array=1-8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n\n#calculate the index of the current job within the batch\n#in our case the index will store the values 1 to 8\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build array structure via ale file names\nCURRENT_SAMPLE=$(cat samples.txt |  sed -n \"${INDEX}p\")\n\n#print what is actually happening\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\n#run the actual job\nfastqc data/seq_project/*/${CURRENT_SAMPLE} -o results/fastqc  --threads 1\n```\n:::\n\n\nIn the script we use some new SLURM arguments:\n\n- `#SBATCH --array=1-8`: Sets up a job array, specifying the range (1-8). We choose 1-8 because we have exactly 8 fastq.gz files we want to analyse \n- `#SBATCH --output=logs/array_%A_%a.out`: Store the standard output and error. `%A` represents the job ID assigned by Slurm, and `%a` represents the array task ID\n\nThe job does the following:\n\n-   The `INDEX` variable is storing the value of the current `SLURM_ARRAY_TASK_ID`. This represents the ID of the current job within the array. In our case this will be first 1, then 2, ..., and finally 3.\n-   Next, we build the array structure:\n    -    The `CURRENT_SAMPLE` variable is created by reading the `sample_list.txt` file with `cat`. \n    -    We then use a pipe to extract the sample at the calculated index using sed. [Sed](https://scienceparkstudygroup.github.io/software_information/source/cli/cli_file_manipulation.html#sed-manipulating-the-content-of-files) is an extremly powerful way to edit text that we have not yet covered but `-n 1p` is a option that allows us to print one specific line of a file, here the first one when running array 1. So for the first array the actual code run is the following `cat samples.txt |  sed -n \"1p\"`. For the next array, we would run `cat samples.txt |  sed -n \"2p\"` and so forth.\n    -    The output of the pipe is stored in a variable, called `CURRENT_SAMPLE`. For our first sample this will be `Sample-DUMMY1_R1.fastq.gz`\n-  We use echo to record what was executed when to store it in the standard output\n-  We run our actual fastqc job on the file name that is currently stored in the `CURRENT_SAMPLE` variable.\n\nIf we check what is happening right after submitting the job with `squeue` we should see something like this:\n\n![](../img/arrays.png){width=\"50%\" fig-align=\"left\"}\n\nWe see that jobs 1-4 are already running and the other jobs are currently waiting for space.\n\nIf we check the log files we should see: \n\n![](../img/arrays2.png){width=\"50%\" fig-align=\"left\"}\n\nWe see that we get an individual output and error file for each job. In the output we see what value is stored in the `INDEX`, here 1, and the `CURRENT_SAMPLE`, here  `Sample-DUMMY1_R1.fastq.gz`. \n\n:::\n\n\n\n## Installing software\n\nThere might be cases where the software you are interested in is not installed. \n\nIn the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges. Let'go through a very brief example: \n\n\n### Install mamba \n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n```\n:::\n\n\n\n### Setting up an environment\n\nLet's assume we want to install seqkit, a tool that  allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length.\n\nWe can do this as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#check if the tool is installed (should return command not found if the software is not installed)\nseqkit -h\n\n#create an empty environment and name it seqkit\nmamba create -n seqkit\n\n#install some software, i.e. seqkit, into the seqkit environment\nmamba install -n seqkit -c bioconda seqkit\n\n#to run the tool activate the environment first\nmamba activate seqkit\n\n#check if tool is installed\nseqkit -h\n\n#run the tool via srun\nmkdir results/seqkit\nsrun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\nless -S results/seqkit/seqkit_stats.tsv \n\n#leave the environment\nconda deactivate\n```\n:::\n\n\nWhen installing the seqkit package we specify that we want to look for seqkit in the bioconda channel with the option `-c`. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.\n\nUnsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page, which also should inform you whether you need to add a specific channel to install the software.\n\nA full set of mamba/conda commands can be found [here](https://docs.conda.io/projects/conda/en/latest/commands/index.html.)\n\n::: {.callout-caution collapse=\"false\" title=\"Exercise\"}\n\n1. Download and view the file `results/seqkit/seqkit_stats.tsv` on your own computer\n2. Run the seqkit again but this time submit the job via a sbatch script instead of using srun\n\n<details>\n<summary>Click me to see an answer</summary>\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#question 1\nscp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .\n\n#question 2 \nsbatch scripts/seqkit.sh\n```\n:::\n\n\nContent of `scripts/seqkit.sh`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\n#SBATCH --job-name=seqkit_job\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start seqkit\"\n\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\n\necho \"seqkit finished\"\n```\n:::\n\n\n\n</details>\n:::",
    "supporting": [
      "hpc_howto_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}