[
  {
    "objectID": "source/installation.html",
    "href": "source/installation.html",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow you to interact with an operating system. The key difference between the CLI and GUI is that the interaction with the CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. The CLI is also referred to as the shell, terminal, console or prompt.\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows. In this tutorial, we assume that you work with a Bash shell.\n\n\n\n\n\nThe default shell is usually Bash and there is no need to install anything to be able to follow this tutorial. On most versions of Linux, the shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you should be able to switch the shell by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ search function. Search for: Terminal and press Return.\n\nTo ensure that you work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window. The name of the current shell should be printed to the terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again. If you have trouble switching from zsh to bash, do not worry, most steps in the tutorial work with either shell.\n\n\n\nOperating systems, like macOS and Linux, come with a native command-line terminal, making it straightforward to run bash commands. However, Windows users need to install some software first to be able to use bash. Below you find three options:\n\nMobaxterm enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here. This option should be the easiest to setup if you have little experience with the shell.\nGit Bash is another option, for detailed installation instructions please have a look at the carpenties website.\nThe Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option is the most flexible as you have a full Linux system at your command but since you basically install a separate system on your PC you need to have enough memory to run this. Installation instructions can be found here.\n\n\n\n\n\n\n\nNote\n\n\n\nI am myself familiar with Linux, Mac and WSL and the following tutorial is tailored towards the location of things when using WSL. If you use another system that is no problem, however, your folder structure might be slightly different when using Git Bash or Mobaxterm.\nSimilarly, I am mainly familiar with the bash not the zsh shell. For Mac users that have trouble switching to bash this might create some issues when using wildcards but these users should be able to otherwise follow most parts of this tutorial.\nIf parts of the tutorial do not work for you due to issues when working with different operating systems/shells, feel free to contact me and I can adjust the tutorial accordingly.\n\n\n\n\n\n\nAfter you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/installation.html#terminology",
    "href": "source/installation.html#terminology",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow you to interact with an operating system. The key difference between the CLI and GUI is that the interaction with the CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. The CLI is also referred to as the shell, terminal, console or prompt.\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows. In this tutorial, we assume that you work with a Bash shell.",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/installation.html#installation-guides",
    "href": "source/installation.html#installation-guides",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The default shell is usually Bash and there is no need to install anything to be able to follow this tutorial. On most versions of Linux, the shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you should be able to switch the shell by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ search function. Search for: Terminal and press Return.\n\nTo ensure that you work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window. The name of the current shell should be printed to the terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again. If you have trouble switching from zsh to bash, do not worry, most steps in the tutorial work with either shell.\n\n\n\nOperating systems, like macOS and Linux, come with a native command-line terminal, making it straightforward to run bash commands. However, Windows users need to install some software first to be able to use bash. Below you find three options:\n\nMobaxterm enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here. This option should be the easiest to setup if you have little experience with the shell.\nGit Bash is another option, for detailed installation instructions please have a look at the carpenties website.\nThe Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option is the most flexible as you have a full Linux system at your command but since you basically install a separate system on your PC you need to have enough memory to run this. Installation instructions can be found here.\n\n\n\n\n\n\n\nNote\n\n\n\nI am myself familiar with Linux, Mac and WSL and the following tutorial is tailored towards the location of things when using WSL. If you use another system that is no problem, however, your folder structure might be slightly different when using Git Bash or Mobaxterm.\nSimilarly, I am mainly familiar with the bash not the zsh shell. For Mac users that have trouble switching to bash this might create some issues when using wildcards but these users should be able to otherwise follow most parts of this tutorial.\nIf parts of the tutorial do not work for you due to issues when working with different operating systems/shells, feel free to contact me and I can adjust the tutorial accordingly.",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/installation.html#sanity-check",
    "href": "source/installation.html#sanity-check",
    "title": "Setting up a terminal",
    "section": "",
    "text": "After you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/hpc_howto.html",
    "href": "source/hpc_howto.html",
    "title": "Using an HPC",
    "section": "",
    "text": "Now, that you are familiar more familiar with the cli, let’s get used to working on an HPC by first login into Crunchomics, uploading our sequencing data an run some software to check the quality of our reads.\nFor people following this tutorial outside of UvA:\n\nIf you have access to an HPC using SLURM you still follow the tutorial\nIf you do not have access to an HPC but are interested in how to install and run software that can be used to analyse sequencing data, you still might want to check out the steps run below. The dataset is very small and all analyses can be run on a desktop computer\n\n\n\nSSH (Secure Shell) is a network protocol that enables secure remote connections between two systems. The general ssh command that you can use to login into any HPC looks as follows:\n\nssh -X username@server\n\nOptions:\n\n-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. Put simply this option enables us to run graphical applications on a remote server and this for example allows us to view a pdf.\n\nIf you have access to and want to connect to Crunchomics you would edit the command above to look like this:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network.\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, please contact ICT.\n\n\n\n\n\nIgnore this section if you already did this but if this is your first time on Crunchomics, then you want to first run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is pre-installed on the HPC\nSets up a python3 environment and some useful python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following command in the cli:\n\n/zfs/omics/software/script/omics_install_script\n\n\n\n\nscp stands for Secure Copy Protocol and allows you to securely copy files and directories between remote hosts. When transferring data the transfer is always prepared from the terminal of your personal computer and not from the HPCs login node.\nThe basic syntax we use looks like this:\nscp [options] SOURCE DESTINATION\nTo start analysing our data, we want to move the fastq.gz files that we have worked with before from our personal folder, the source, to a folder on Cruncomics, the destination. Let’s start setting up a project folder from which we want to run our analyses by:\n\nMoving from our home directory into our personal directory on the Crunchomics HPC. We move there since we have more space in the personal directory. Notice, for the command below to work, we need to have already executed the omics_install_script script above\nMake a project folder with a descriptive file name, i.e. projectX\n\n\ncd personal/\nmkdir projectX\ncd projectX\n\nNow that we have organized our working directory on the HPC, we next want to move the data folder (which right now is only on your own, personal computer) with the sequencing that you have downloaded before to Crunchomics. We do this by moving the whole data folder from our own computer to the new project folder on the HPC. Therefore, it is important that:\n\nwe run the following command from the cli on our own computer and not from the cli while being logged into the HPC!\nyou exchange the two instances of username in the code below with your username/uvanetid\nIn the example below, I am running the code from inside of the data_analysis folder that we have generated in the previous tutorial since that is where I have the data folder that I want to move. I use the -r option in order to move the whole data folder and not a single file.\n\n\n#run the command below from your local computer, \n#ensure that the data folder is inside the directory from which you run this command\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/projectX\n\n#run ls on crunchomics to check if you moved the data successfully\nll data/seq_project/*/*fastq.gz\n\n\n\n\n\n\n\nTip: Moving data from the HPC to our own computer\n\n\n\n\n\nWe can also move data from the HPC to our own computer. For example, let’s assume we want to move a single sequencing file from crunchomics back to our computer. In this case,\n\nWe do not need -r since we only move a single file\nWe again run this command from a terminal on our computer, not while being logged in the HPC\nWe use . to indicate that we want to move the file into the directory we are currently in. If we want to move the file elsewhere we can use any absolute or relative path as needed\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz .\n\n\n\n\n\n\n\n\n\n\nTip: Moving data from the HPC using wildcards\n\n\n\n\n\nWe can also move several files at once and ignore the whole folder structure by using wildcards when using scp.\n\n#make a random directory in our Crunchomics working directory to move our data into\n#again, we generate this folder on Crunchomics\nmkdir transfer_test\n\n#move files from crunchomics to our local computer\n#again, always run scp from your local computer\nscp data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\n#view the data, see how the folder structure is different compared to our first example? \n#since we moved the data TO Crunchomics, we run ls while being logged into Crunchomics\nll transfer_test/*\n\nNotice for MAC users:\nFor Mac users that work with an zsh shell the command above might not work and you might get an error like “file not found”, “no matches found” or something the like. Without going into details zsh has a slightly different way of handling wildcards and tries to interpret the wildcard literally and thus does not find our files. If you see the error and you are sure the file exists it should work to edit your line of code as follows:\n\n\\scp  data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\nIf that does not work, these are some other things to try (different zsh environments might need slightly different solutions):\n\nnoglob scp data/seq_project/barcode00*/*fastq.gz ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\nscp 'data/seq_project/barcode00*/*fastq.gz' ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\n\n\n\n\n\n\n\n\n\nTip: Moving files using the FileZilla GUI\n\n\n\n\n\nscp takes some getting used to and there are several graphical user interface programs that are available which provide the same functionality. Here, we will look at FileZilla, as it is compatible with Windows, Mac, and Linux.\nTo use FileZilla, do the following:\n\nDownload the FileZilla Client here. The free version is all we need.\nFollow the download instructions\nClick the FileZilla app to start it\nAdd Uva user credentials\n\nEnter omics-h0.science.uva.nl in host\nEnter user_name in user\nEnter psswd chosen when making surf account\nPort should be 22 to open a sftp connection\n\nTrust the host\nYou land in your home directory (on the right hand side)\nYou can copy-paste data by drag and dropping\n\n\n\n\n\n\n\n\n\nNow, that we have our data prepared we want to run a tool to assess the quality of our reads. Before doing that, let’s talk about submitting jobs to an HPC.\nAs a reminder: We do not run big jobs on the login node but need to submit such jobs to the compute nodes via SLURM. Login nodes are for preparing your programs to run and you run your actual jobs by submitting them to the compute nodes using SLURM.\nIn the next few sections you will get to know the basics steps to be able to do this.\nBefore doing this it is, however, useful when getting started on a new HPC to know how to get basic information about what nodes are available on a cluster and how busy an HPC is. This allows us to better know how many resources to request in order to have our job run efficiently but also get started in a timely manner.\nOne way to get a list of available compute nodes is by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, the different columns you see are:\n\npartition: tells us what queues that are available. There are few partitions on Crunchomics and you do not need to define them in our case. However, other systems use partitions to group a subset of nodes with different type of hardwares or a specific maximum wall time. For example, you might have specific partitions for memory-heavy versus time-intensive jobs.\nstate: tells you if a node is busy or not. Here:\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes available for each partition\nNodeList: the names of the compute nodes (i.e. omics-cn001 to omics-cn005)\n\n\n\n\nThe following command gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC, which might look something like this:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\nIf we would have submitted a job, we also should see the job listed there.\n\n\n\n\nsrun is used when you want to run tasks interactively or want to have more control over the execution of a job. You directly issue srun commands in the terminal and you at the same time are able to specify the tasks to be executed and their resource requirements. For example, you might want to run softwareX and request that this job requires 10 CPUs and 5 GB of memory.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback printed to the screen\nYou are testing or debugging your commands before incorporating them into a script\nYou need more control over the execution of tasks\nTypically, you use srun for smaller jobs that do not run for too long, i.e. a few hours\n\nLet’s submit a very simple example for which we would not even need to submit a job, but just to get you started.\n\nsrun echo \"Hello interactively\"\n\nYou should see the output of echo printed to the screen and if you would run squeue you won’t even see your job since everything ran so fast. But congrats, you communicated the first time with the compute node.\nNow assume you want to run a more complex interactive task with srun that might run longer and benefit from using more CPUs. In this case you need to specify the resources your job needs by adding flags, i.e. some of which you see here:\n\nsrun --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=1G echo \"Hello interactively\"\n\nThe different flags mean the following:\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit. In our case this definitely is over-kill to request a full node with 64 CPUs to print a single line of text to the screen.\n--ntasks=1: Defines the number of tasks to run. Here, it’s set to 1 since we want to use echo once\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs\necho \"Hello interactively: The actual command you want to run interactively\n\n\n\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster or recommended by the tool itself. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTest with small datasets: For initial testing and debugging especially when working with large datasets, consider working with a smaller subset of your data. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used and see whether you can optimize a run if you plan to run similar jobs over and over again. An example command would be sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,ncpus. In the report, look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate.\nEnsuer that the job used the resources you requested. For instance, if you would have used --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). You can verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\n\nFastQC is a quality control tool for high throughput sequence data that is already installed on Crunchomics. This will be the actual software that we now want to run to look more closely at the quality of the sequencing data that we just uploaded to Crunchomics.\nBy the way: Whenever running a software for a first time, it is useful to check the manual, for example with fastqc -h.\nLet’s start by setting up a clean folder structure to keep our files organized. I start with making a folder in which I want to store all results generated in this analysis and by using the -p argument I generate a fastqc folder inside the results folder at the same time. Then, we can submit our job using srun.\n\nmkdir -p results/fastqc \n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\nll results/fastqc/*\n\nSince we work with little data this will run extremely fast despite only using 1 CPU (or thread, in our case these two words can be used interchangeably). However, if you would be logged into Crunchomics via a second window and run squeue you should see that your job is actively running (in the example below, the job we submitted is named after the software and got the jobid 746):\n\n\n\n\n\nAdditionally, after the run is completed, you should see that several HTML files were generated in our fastqc folder.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse scp to download the HTML files generated by fastqc to your own computer and view one of the HTML files.\n\n\nClick me to see an answer\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/results/fastqc/*html results/fastqc/\n\nYou could also open a file on Crunchomics with firefox results/fastqc/Sample-DUMMY1_R1_fastqc.html. However, connecting to the internet via a cli on a remote server tends to be rather slow and its often better to view files on your own computer especially if they are large files.\nIf you want to know more about how to to interpret the output, you can visit the fastqc website, which gives some examples for interpreting good and bad reports.\n\n\n\n\n\n\n\n\nOne down-side of srun for long running jobs is that your terminal gets “blocked” as long as the job is running and your job will be aborted if you loose the ssh connection to the HPC. For long running jobs, there are two ways to deal with this:\n\nsubmit a srun job in a screen\nuse sbatch\n\nIn this section, we will cover how to use screen. Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach (go outside of a screen but keep the screen running in the background) from a screen by pressing control+a+d.\nIf you want to run multiple analyses in multiple screens at the same time, then can be useful to give your screens more descriptive names. You can give screens a name using the -S option:\n\nscreen -S run_fastqc\n\nAfter detaching from this screen again with control+a+d you can create a list of all currently running screens with:\n\nscreen -ls\n\nYou can re-connect to an existing screen like this:\n\nscreen -r run_fastqc\n\nNow inside our screen, we can run fastqc same as we did before (but now don’t risk to loose a long-running job):\n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1 \n\nFor long-running jobs we can start multiple screens at once or even if we just have one screen open, close it and leave for the day or simply work on other things on the cli outside of the screen.\nIf you want to completely close and remove a screen, type the following and press enter while being inside of the screen:\n\nexit\n\n\n\n\nsbatch is your go-to command when you have a script (i.e. a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nLet’s start with generating some new folders to keep our project folder organized:\n\nmkdir scripts \nmkdir logs\n\nTo get started, assume we have created a script in the scripts folder named run_fastqc.sh with the content that is shown below. Notice, how in this script I added some additional echo commands? I just use these to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you would do it.\n\n#!/bin/bash\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\nIn the job script above:\n\n#!/bin/bash . This so-called Shebang line tells the shell to interpret and run the Slurm script using the bash shell. This line should always be added at the very top of your SBATCH/Slurm script.\nThe lines that follow and start with # are the lines in which we define the amount of resources required for our job to run. In our case, we request 1 CPU and 5G of memory.\nIf your code needs any dependencies, such as conda environments, you would add these dependencies here. We do not need this for our example here, but you might need to add something like this conda activate my_env if you have installed your own software. We will touch upon conda environments and installing software a bit later.\nThe lines afterwards are the actually commands that we want to run on the compute nodes, We also call this the job steps.\n\nTo prepare the script and run it:\n\nRun nano scripts/run_fastqc.sh to generate an empty jobscript file\nAdd the code from above into the file we just opened\nPress ctrl+x to exit nano\nType Y when prompted if the changes should be saved\nConfirm that the file name is good by pressing enter\n\nAfterwards, you can submit run_fastqc.sh as follows:\n\n#submit job: 754\nsbatch scripts/run_fastqc.sh\n\n#check if job is running correctly\nsqueue\n\nAfter running this, you should see that the job was submitted and something like this printed to the screen Submitted batch job 754. You will also see that a new file is generated that will look something like this slurm-754.out.\nWhen you submit a batch job using sbatch, Slurm redirects the standard output and standard error messages, which you have seen printed to the screen when you used srun, to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here\n\nFeel free to explore the content of the log file, do you see how the echo commands are used as well?\n\n\n\n\n\n\nTip: sbatch and better log files\n\n\n\n\n\nWe have seen that by default sbatch redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important especially if you generate lots of files, you find below an example to:\n\nStore the standard output and error in two separate files\nRedirect the output into another folder, the logs folder\nIn the code below, the %j is replaced with the job allocation number once the log files are generated\n\n\n#!/bin/bash\n#SBATCH --job-name=our_fastqc_job\n#SBATCH --output=logs/fastqc_%j.out\n#SBATCH --error=logs/fastqc_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\n\n\n\n\n\n\n\n\n\nAdvanced tip: sbatch arrays to run multiple files in parallel\n\n\n\n\n\nWith fastqc we are very lucky that the tool can identify all the fastq files in the directory we specify with -o by making use of the wildcard. This is extremely useful for us but by far not all programs work this way.\nFor this section, lets assume that we need to provide each individual file we want to analyse, one by one, and can not provide a folder name. How would we run such a job effectively?\nWhat we want to do is created what is called a job array that allows us to:\n\nRun multiple jobs that have the same job definition, i.e. cpus, memory and software used\nRun these job in the most optimal way. I.e. we do not want to run one job after each other but we also want to run jobs in parallel at the same time to optimize resource usage.\n\nLet’s start with making a list with files we want to work with based on what we have already learned:\n\nls data/seq_project/*/*.gz | cut -f4 -d \"/\" &gt; samples.txt\n\nNext, we can use this text file in our job array, the content of which we store in scripts/array.sh:\n\n#!/bin/bash\n\n#SBATCH --job-name=my_array\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --array=1-8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n\n#calculate the index of the current job within the batch\n#in our case the index will store the values 1 to 8 for our 8 files\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build an array structure that stores the fastq.gz file names\nCURRENT_SAMPLE=$(cat samples.txt |  sed -n \"${INDEX}p\")\n\n#print what is actually happening\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\n#run the actual job\nfastqc data/seq_project/*/${CURRENT_SAMPLE} -o results/fastqc  --threads 1\n\nIn the script we use some new SLURM arguments:\n\n#SBATCH --array=1-8: Sets up a job array, specifying the range (1-8). We choose 1-8 because we have exactly 8 fastq.gz files we want to analyse\n#SBATCH --output=logs/array_%A_%a.out: Store the standard output and error. %A represents the job ID assigned by Slurm, and %a represents the array task ID\nImportant: Are your individual arrays large jobs that use a lot of CPUs/memory? Then you might not want to submit all at the same time but submit them in a step-wise manner. With #SBATCH --array=1-8%2 you would only submit the first two jobs and only after these have finished the next jobs. Remember, try to use not more than ~20% of the crunchomics cluster at the same time.\n\nThe job does the following:\n\nThe INDEX variable is storing the value of the current SLURM_ARRAY_TASK_ID. This represents the ID of the current job within the array. In our case this will be first 1, then 2, …, and finally 8.\nNext, we build the array structure in which the CURRENT_SAMPLE variable is created by:\n\nReading the sample_list.txt file with cat\nUsing a pipe to extract the file name at the calculated index using sed. Sed is an extremely powerful way to edit text that we have not covered here but -n 1p is a option that allows us to print one specific line of a file, in our case the first one when running array 1. So for the first array the actual code run is the following cat samples.txt |  sed -n \"1p\". For the next array, we would run cat samples.txt |  sed -n \"2p\" and so forth.\nThe output of the pipe is stored in a variable, called CURRENT_SAMPLE. For our first sample this will be Sample-DUMMY1_R1.fastq.gz\n\nWe use echo to record what was executed when and store this information in the standard output\nWe run our actual fastqc job on the file name that is currently stored in the CURRENT_SAMPLE variable.\n\nIf we check what is happening right after submitting the job with squeue we should see something like this:\n\n\n\n\n\nWe see that jobs 1-4 are already running and the jobs 5-8 are currently waiting for space. That is one of the useful things using a job manager such as SLURM. It takes care of finding the appropriate resources on all nodes for us as long as we defined the required cpus and memory sensibly.\nIf we check the log files we should see something like this:\n\n\n\n\n\nWe see that we get an individual output and error file for each job. In the output we see what value is stored in the INDEX, here 1, and the CURRENT_SAMPLE, here Sample-DUMMY1_R1.fastq.gz and that the analysis finished successfully.\n\n\n\n\n\n\nThere might be cases where the software you are interested in is not installed on the HPC you are working with (or on your own computer).\nIn the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These systems allow you to find and install packages in their own environment without administrator privileges. Let’s have a look at a very brief example:\n\n\nA lot of systems already come with conda/mamba installed, however, if possible we recommend working with mamba instead of conda. mamba is a replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nIf you have conda installed and do not want to install anything else, that is fine. Just replace all instances of mamba with conda below.\nThis command should work in most cases to setup conda together with mamba:\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nWhen running the bash command, you get asked a few questions:\n\nRead the license and use arrow down to scroll down. Don’t scroll too fast, so that you see the next question\nDecide where conda gets installed. You can say yes to the default location in your home. But don’t forget that for Crunchomics your home only has 25G of space. You could also install the miniforge/mambaforge folder in your personal folder instead.\nSay yes, when you get asked whether conda should be initialized during start up\nRestart the shell (exit the terminal and use ssh to log back in) for the changes to take effect\nCheck if conda is accessible by running conda -h\n\n\n\n\nLet’s assume we want to install seqkit, a tool that allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length per sample.\nWe can install seqkit into a separate environment, which we can give a descriptive name, as follows:\n\n#check if the tool is installed (should return \"command not\" found if the software is not installed)\nseqkit -h\n\n#create an empty environment and name it seqkit and we add the version number to the name\n#this basically sets up seqkit separate from our default working environment\n#this is useful whenever software require complicated dependencies allowing us to have a separate install away from software that could conflict with each other\nmamba create -n seqkit_2.6.1\n\n#install seqkit, into the environment we just created\nmamba install -n seqkit_2.6.1 -c bioconda seqkit=2.6.1\n\n#to run seqkit, we need activate the environment first\nmamba activate seqkit_2.6.1\n\n#check if tool is installed, \n#if installed properly this should return some detailed information on how to run seqkit\nseqkit -h\n\n#run the tool via srun\nmkdir results/seqkit\nsrun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\nless -S results/seqkit/seqkit_stats.tsv \n\n#close the environment\nconda deactivate\n\nWhen installing seqkit, we:\n\nspecify the exact version we want to download with =2.6.1. We could also install the newest version that conda/mamba can find by running mamba install -n seqkit -c bioconda seqkit.\nspecify that we want to look for seqkit in the bioconda channel with the option -c. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.\n\nForgot what conda environments you installed in the past? You can run conda env list to generate a list of all existing environments.\nUnsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page. This page should inform you whether you need to add a specific channel to install the software as well as the version numbers available.\nA full set of mamba/conda commands can be found here.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nDownload and view the file results/seqkit/seqkit_stats.tsv on your own computer\nRun the seqkit again but this time submit the job via a sbatch script instead of using srun. Notice, that you need to tell SLURM how it can activate the conda environment that has seqkit installed. You might need to google how to do that, since this requires some extra line of code that we have not covered yet but see this as a good exercise for how to handle error messages that you see in the log files for your own analyses\n\n\n\nClick me to see an answer\n\n\n#question 1\nscp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .\n\n#question 2 \nsbatch scripts/seqkit.sh\n\nContent of scripts/seqkit.sh:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit_job\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\n#activate dependencies\nsource ~/.bashrc\nmamba activate seqkit_2.6.1\n\n#run seqkit\necho \"Start seqkit\"\n\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\n\necho \"seqkit finished\"\n\nIn the script above, we see that we need to add two lines of code to activate the seqkit conda environment:\nsource ~/.bashrc\nmamba activate seqkit_2.6.1\nWhen you run a script or a command, it operates in its own environment. The source command is like telling the script to look into another file, in this case, ~/.bashrc, and execute the commands in that file as if they were written directly into the script.\nHere, source ~/.bashrc is telling the script to execute the commands in the ~/.bashrc file. This is typically done to set up environment variables, paths, and activate any software or tools that are required for the script to run successfully. In our case this tells Slurm where we have installed conda and thus enables Slurm to use conda itself.\nThis allows slurm to, after executing source ~/.bashrc, activates a Conda environment using mamba activate seqkit_2.6.1. This ensures that the SeqKit tool and its dependencies are available and properly configured for use in the subsequent part of the script.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#ssh-connecting-to-a-sever",
    "href": "source/hpc_howto.html#ssh-connecting-to-a-sever",
    "title": "Using an HPC",
    "section": "ssh: Connecting to a sever",
    "text": "ssh: Connecting to a sever\nSSH (Secure Shell) is a network protocol that enables secure remote connections between two systems. The general ssh command that you can use to login into any HPC looks as follows:\n\nssh -X username@server\n\nOptions:\n\n-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. Put simply this option enables us to run graphical applications on a remote server and this for example allows us to view a pdf.\n\nIf you have access to and want to connect to Crunchomics you would edit the command above to look like this:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network.\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, please contact ICT.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#crunchomics-preparing-your-account",
    "href": "source/hpc_howto.html#crunchomics-preparing-your-account",
    "title": "Using an HPC",
    "section": "Crunchomics: Preparing your account",
    "text": "Crunchomics: Preparing your account\nIgnore this section if you already did this but if this is your first time on Crunchomics, then you want to first run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is pre-installed on the HPC\nSets up a python3 environment and some useful python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following command in the cli:\n\n/zfs/omics/software/script/omics_install_script",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#scp-transferring-data-fromto-a-server",
    "href": "source/hpc_howto.html#scp-transferring-data-fromto-a-server",
    "title": "Using an HPC",
    "section": "scp: Transferring data from/to a server",
    "text": "scp: Transferring data from/to a server\nscp stands for Secure Copy Protocol and allows you to securely copy files and directories between remote hosts. When transferring data the transfer is always prepared from the terminal of your personal computer and not from the HPCs login node.\nThe basic syntax we use looks like this:\nscp [options] SOURCE DESTINATION\nTo start analysing our data, we want to move the fastq.gz files that we have worked with before from our personal folder, the source, to a folder on Cruncomics, the destination. Let’s start setting up a project folder from which we want to run our analyses by:\n\nMoving from our home directory into our personal directory on the Crunchomics HPC. We move there since we have more space in the personal directory. Notice, for the command below to work, we need to have already executed the omics_install_script script above\nMake a project folder with a descriptive file name, i.e. projectX\n\n\ncd personal/\nmkdir projectX\ncd projectX\n\nNow that we have organized our working directory on the HPC, we next want to move the data folder (which right now is only on your own, personal computer) with the sequencing that you have downloaded before to Crunchomics. We do this by moving the whole data folder from our own computer to the new project folder on the HPC. Therefore, it is important that:\n\nwe run the following command from the cli on our own computer and not from the cli while being logged into the HPC!\nyou exchange the two instances of username in the code below with your username/uvanetid\nIn the example below, I am running the code from inside of the data_analysis folder that we have generated in the previous tutorial since that is where I have the data folder that I want to move. I use the -r option in order to move the whole data folder and not a single file.\n\n\n#run the command below from your local computer, \n#ensure that the data folder is inside the directory from which you run this command\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/projectX\n\n#run ls on crunchomics to check if you moved the data successfully\nll data/seq_project/*/*fastq.gz\n\n\n\n\n\n\n\nTip: Moving data from the HPC to our own computer\n\n\n\n\n\nWe can also move data from the HPC to our own computer. For example, let’s assume we want to move a single sequencing file from crunchomics back to our computer. In this case,\n\nWe do not need -r since we only move a single file\nWe again run this command from a terminal on our computer, not while being logged in the HPC\nWe use . to indicate that we want to move the file into the directory we are currently in. If we want to move the file elsewhere we can use any absolute or relative path as needed\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz .\n\n\n\n\n\n\n\n\n\n\nTip: Moving data from the HPC using wildcards\n\n\n\n\n\nWe can also move several files at once and ignore the whole folder structure by using wildcards when using scp.\n\n#make a random directory in our Crunchomics working directory to move our data into\n#again, we generate this folder on Crunchomics\nmkdir transfer_test\n\n#move files from crunchomics to our local computer\n#again, always run scp from your local computer\nscp data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\n#view the data, see how the folder structure is different compared to our first example? \n#since we moved the data TO Crunchomics, we run ls while being logged into Crunchomics\nll transfer_test/*\n\nNotice for MAC users:\nFor Mac users that work with an zsh shell the command above might not work and you might get an error like “file not found”, “no matches found” or something the like. Without going into details zsh has a slightly different way of handling wildcards and tries to interpret the wildcard literally and thus does not find our files. If you see the error and you are sure the file exists it should work to edit your line of code as follows:\n\n\\scp  data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\nIf that does not work, these are some other things to try (different zsh environments might need slightly different solutions):\n\nnoglob scp data/seq_project/barcode00*/*fastq.gz ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\nscp 'data/seq_project/barcode00*/*fastq.gz' ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\n\n\n\n\n\n\n\n\n\nTip: Moving files using the FileZilla GUI\n\n\n\n\n\nscp takes some getting used to and there are several graphical user interface programs that are available which provide the same functionality. Here, we will look at FileZilla, as it is compatible with Windows, Mac, and Linux.\nTo use FileZilla, do the following:\n\nDownload the FileZilla Client here. The free version is all we need.\nFollow the download instructions\nClick the FileZilla app to start it\nAdd Uva user credentials\n\nEnter omics-h0.science.uva.nl in host\nEnter user_name in user\nEnter psswd chosen when making surf account\nPort should be 22 to open a sftp connection\n\nTrust the host\nYou land in your home directory (on the right hand side)\nYou can copy-paste data by drag and dropping",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#slurm-basics",
    "href": "source/hpc_howto.html#slurm-basics",
    "title": "Using an HPC",
    "section": "Slurm basics",
    "text": "Slurm basics\n\nGet information about the cluster\nNow, that we have our data prepared we want to run a tool to assess the quality of our reads. Before doing that, let’s talk about submitting jobs to an HPC.\nAs a reminder: We do not run big jobs on the login node but need to submit such jobs to the compute nodes via SLURM. Login nodes are for preparing your programs to run and you run your actual jobs by submitting them to the compute nodes using SLURM.\nIn the next few sections you will get to know the basics steps to be able to do this.\nBefore doing this it is, however, useful when getting started on a new HPC to know how to get basic information about what nodes are available on a cluster and how busy an HPC is. This allows us to better know how many resources to request in order to have our job run efficiently but also get started in a timely manner.\nOne way to get a list of available compute nodes is by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, the different columns you see are:\n\npartition: tells us what queues that are available. There are few partitions on Crunchomics and you do not need to define them in our case. However, other systems use partitions to group a subset of nodes with different type of hardwares or a specific maximum wall time. For example, you might have specific partitions for memory-heavy versus time-intensive jobs.\nstate: tells you if a node is busy or not. Here:\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes available for each partition\nNodeList: the names of the compute nodes (i.e. omics-cn001 to omics-cn005)\n\n\n\nView info about jobs in the queue\nThe following command gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC, which might look something like this:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\nIf we would have submitted a job, we also should see the job listed there.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#srunsubmitting-a-job-interactively",
    "href": "source/hpc_howto.html#srunsubmitting-a-job-interactively",
    "title": "Using an HPC",
    "section": "srun:Submitting a job interactively",
    "text": "srun:Submitting a job interactively\nsrun is used when you want to run tasks interactively or want to have more control over the execution of a job. You directly issue srun commands in the terminal and you at the same time are able to specify the tasks to be executed and their resource requirements. For example, you might want to run softwareX and request that this job requires 10 CPUs and 5 GB of memory.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback printed to the screen\nYou are testing or debugging your commands before incorporating them into a script\nYou need more control over the execution of tasks\nTypically, you use srun for smaller jobs that do not run for too long, i.e. a few hours\n\nLet’s submit a very simple example for which we would not even need to submit a job, but just to get you started.\n\nsrun echo \"Hello interactively\"\n\nYou should see the output of echo printed to the screen and if you would run squeue you won’t even see your job since everything ran so fast. But congrats, you communicated the first time with the compute node.\nNow assume you want to run a more complex interactive task with srun that might run longer and benefit from using more CPUs. In this case you need to specify the resources your job needs by adding flags, i.e. some of which you see here:\n\nsrun --cpus-per-task=1 --mem=1G --time=00:10:00 echo \"Hello interactively\"\n\nThe different flags mean the following:\n\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs\n--time=00:10:00 Sets the time limit to 10 minutes\necho \"Hello interactively: The actual command you want to run interactively\n\nOther flags:\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit. In our case this definitely is over-kill to request a full node with 64 CPUs to print a single line of text to the screen.\n--ntasks=1: Defines the number of tasks to run. Here, we would set it to 1 since we want to use echo once\n\n\nChoosing the right amount of resources\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster or recommended by the tool itself. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTest with small datasets: For initial testing and debugging especially when working with large datasets, consider working with a smaller subset of your data. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used and see whether you can optimize a run if you plan to run similar jobs over and over again. An example command would be sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,ncpus. In the report, look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate.\nEnsuer that the job used the resources you requested. For instance, if you would have used --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). You can verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\nRun FastQC with srun\nFastQC is a quality control tool for high throughput sequence data that is already installed on Crunchomics. This will be the actual software that we now want to run to look more closely at the quality of the sequencing data that we just uploaded to Crunchomics.\nBy the way: Whenever running a software for a first time, it is useful to check the manual, for example with fastqc -h.\nLet’s start by setting up a clean folder structure to keep our files organized. I start with making a folder in which I want to store all results generated in this analysis and by using the -p argument I generate a fastqc folder inside the results folder at the same time. Then, we can submit our job using srun.\n\nmkdir -p results/fastqc \n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\nll results/fastqc/*\n\nSince we work with little data this will run extremely fast despite only using 1 CPU (or thread, in our case these two words can be used interchangeably). However, if you would be logged into Crunchomics via a second window and run squeue you should see that your job is actively running (in the example below, the job we submitted is named after the software and got the jobid 746):\n\n\n\n\n\nAdditionally, after the run is completed, you should see that several HTML files were generated in our fastqc folder.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse scp to download the HTML files generated by fastqc to your own computer and view one of the HTML files.\n\n\nClick me to see an answer\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/results/fastqc/*html results/fastqc/\n\nYou could also open a file on Crunchomics with firefox results/fastqc/Sample-DUMMY1_R1_fastqc.html. However, connecting to the internet via a cli on a remote server tends to be rather slow and its often better to view files on your own computer especially if they are large files.\nIf you want to know more about how to to interpret the output, you can visit the fastqc website, which gives some examples for interpreting good and bad reports.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#screen-submitting-long-running-jobs-via-srun",
    "href": "source/hpc_howto.html#screen-submitting-long-running-jobs-via-srun",
    "title": "Using an HPC",
    "section": "screen: Submitting long running jobs via srun",
    "text": "screen: Submitting long running jobs via srun\nOne down-side of srun for long running jobs is that your terminal gets “blocked” as long as the job is running and your job will be aborted if you loose the ssh connection to the HPC. For long running jobs, there are two ways to deal with this:\n\nsubmit a srun job in a screen\nuse sbatch\n\nIn this section, we will cover how to use screen. Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach (go outside of a screen but keep the screen running in the background) from a screen by pressing control+a+d.\nIf you want to run multiple analyses in multiple screens at the same time, then can be useful to give your screens more descriptive names. You can give screens a name using the -S option:\n\nscreen -S run_fastqc\n\nAfter detaching from this screen again with control+a+d you can create a list of all currently running screens with:\n\nscreen -ls\n\nYou can re-connect to an existing screen like this:\n\nscreen -r run_fastqc\n\nNow inside our screen, we can run fastqc same as we did before (but now don’t risk to loose a long-running job):\n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1 \n\nFor long-running jobs we can start multiple screens at once or even if we just have one screen open, close it and leave for the day or simply work on other things on the cli outside of the screen.\nIf you want to completely close and remove a screen, type the following and press enter while being inside of the screen:\n\nexit",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#sbatch-submitting-a-long-running-job",
    "href": "source/hpc_howto.html#sbatch-submitting-a-long-running-job",
    "title": "Using an HPC",
    "section": "sbatch: submitting a long-running job",
    "text": "sbatch: submitting a long-running job\nsbatch is your go-to command when you have a script (i.e. a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nLet’s start with generating some new folders to keep our project folder organized:\n\nmkdir scripts \nmkdir logs\n\nTo get started, assume we have created a script in the scripts folder named run_fastqc.sh with the content that is shown below. Notice, how in this script I added some additional echo commands? I just use these to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you would do it.\n\n#!/bin/bash\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G      \n#SBATCH --time=1:00:00\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\nIn the job script above:\n\n#!/bin/bash . This so-called Shebang line tells the shell to interpret and run the Slurm script using the bash shell. This line should always be added at the very top of your SBATCH/Slurm script.\nThe lines that follow and start with # are the lines in which we define the amount of resources required for our job to run. In our case, we request 1 CPU, 5G of memory and 1 hour time limit.\nIf your code needs any dependencies, such as conda environments, you would add these dependencies here. We do not need this for our example here, but you might need to add something like this conda activate my_env if you have installed your own software. We will touch upon conda environments and installing software a bit later.\nThe lines afterwards are the actually commands that we want to run on the compute nodes, We also call this the job steps.\n\nTo prepare the script and run it:\n\nRun nano scripts/run_fastqc.sh to generate an empty jobscript file\nAdd the code from above into the file we just opened\nPress ctrl+x to exit nano\nType Y when prompted if the changes should be saved\nConfirm that the file name is good by pressing enter\n\nAfterwards, you can submit run_fastqc.sh as follows:\n\n#submit job: 754\nsbatch scripts/run_fastqc.sh\n\n#check if job is running correctly\nsqueue\n\nAfter running this, you should see that the job was submitted and something like this printed to the screen Submitted batch job 754. You will also see that a new file is generated that will look something like this slurm-754.out.\nWhen you submit a batch job using sbatch, Slurm redirects the standard output and standard error messages, which you have seen printed to the screen when you used srun, to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here\n\nFeel free to explore the content of the log file, do you see how the echo commands are used as well?\n\n\n\n\n\n\nTip: sbatch and better log files\n\n\n\n\n\nWe have seen that by default sbatch redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important especially if you generate lots of files, you find below an example to:\n\nStore the standard output and error in two separate files\nRedirect the output into another folder, the logs folder\nIn the code below, the %j is replaced with the job allocation number once the log files are generated\n\n\n#!/bin/bash\n#SBATCH --job-name=our_fastqc_job\n#SBATCH --output=logs/fastqc_%j.out\n#SBATCH --error=logs/fastqc_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\n\n\n\n\n\n\n\n\n\nAdvanced tip: sbatch arrays to run multiple files in parallel\n\n\n\n\n\nWith fastqc we are very lucky that the tool can identify all the fastq files in the directory we specify with -o by making use of the wildcard. This is extremely useful for us but by far not all programs work this way.\nFor this section, lets assume that we need to provide each individual file we want to analyse, one by one, and can not provide a folder name. How would we run such a job effectively?\nWhat we want to do is created what is called a job array that allows us to:\n\nRun multiple jobs that have the same job definition, i.e. cpus, memory and software used\nRun these job in the most optimal way. I.e. we do not want to run one job after each other but we also want to run jobs in parallel at the same time to optimize resource usage.\n\nLet’s start with making a list with files we want to work with based on what we have already learned:\n\nls data/seq_project/*/*.gz | cut -f4 -d \"/\" &gt; samples.txt\n\nNext, we can use this text file in our job array, the content of which we store in scripts/array.sh:\n\n#!/bin/bash\n\n#SBATCH --job-name=my_array\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --array=1-8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n\n#calculate the index of the current job within the batch\n#in our case the index will store the values 1 to 8 for our 8 files\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build an array structure that stores the fastq.gz file names\nCURRENT_SAMPLE=$(cat samples.txt |  sed -n \"${INDEX}p\")\n\n#print what is actually happening\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\n#run the actual job\nfastqc data/seq_project/*/${CURRENT_SAMPLE} -o results/fastqc  --threads 1\n\nIn the script we use some new SLURM arguments:\n\n#SBATCH --array=1-8: Sets up a job array, specifying the range (1-8). We choose 1-8 because we have exactly 8 fastq.gz files we want to analyse\n#SBATCH --output=logs/array_%A_%a.out: Store the standard output and error. %A represents the job ID assigned by Slurm, and %a represents the array task ID\nImportant: Are your individual arrays large jobs that use a lot of CPUs/memory? Then you might not want to submit all at the same time but submit them in a step-wise manner. With #SBATCH --array=1-8%2 you would only submit the first two jobs and only after these have finished the next jobs. Remember, try to use not more than ~20% of the crunchomics cluster at the same time.\n\nThe job does the following:\n\nThe INDEX variable is storing the value of the current SLURM_ARRAY_TASK_ID. This represents the ID of the current job within the array. In our case this will be first 1, then 2, …, and finally 8.\nNext, we build the array structure in which the CURRENT_SAMPLE variable is created by:\n\nReading the sample_list.txt file with cat\nUsing a pipe to extract the file name at the calculated index using sed. Sed is an extremely powerful way to edit text that we have not covered here but -n 1p is a option that allows us to print one specific line of a file, in our case the first one when running array 1. So for the first array the actual code run is the following cat samples.txt |  sed -n \"1p\". For the next array, we would run cat samples.txt |  sed -n \"2p\" and so forth.\nThe output of the pipe is stored in a variable, called CURRENT_SAMPLE. For our first sample this will be Sample-DUMMY1_R1.fastq.gz\n\nWe use echo to record what was executed when and store this information in the standard output\nWe run our actual fastqc job on the file name that is currently stored in the CURRENT_SAMPLE variable.\n\nIf we check what is happening right after submitting the job with squeue we should see something like this:\n\n\n\n\n\nWe see that jobs 1-4 are already running and the jobs 5-8 are currently waiting for space. That is one of the useful things using a job manager such as SLURM. It takes care of finding the appropriate resources on all nodes for us as long as we defined the required cpus and memory sensibly.\nIf we check the log files we should see something like this:\n\n\n\n\n\nWe see that we get an individual output and error file for each job. In the output we see what value is stored in the INDEX, here 1, and the CURRENT_SAMPLE, here Sample-DUMMY1_R1.fastq.gz and that the analysis finished successfully.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#installing-software",
    "href": "source/hpc_howto.html#installing-software",
    "title": "Using an HPC",
    "section": "Installing software",
    "text": "Installing software\nThere might be cases where the software you are interested in is not installed on the HPC you are working with (or on your own computer).\nIn the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These systems allow you to find and install packages in their own environment without administrator privileges. Let’s have a look at a very brief example:\n\nInstall conda/mamba\nA lot of systems already come with conda/mamba installed, however, if possible we recommend working with mamba instead of conda. mamba is a replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nIf you have conda installed and do not want to install anything else, that is fine. Just replace all instances of mamba with conda below.\nThis command should work in most cases to setup conda together with mamba:\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nWhen running the bash command, you get asked a few questions:\n\nRead the license and use arrow down to scroll down. Don’t scroll too fast, so that you see the next question\nDecide where conda gets installed. You can say yes to the default location in your home. But don’t forget that for Crunchomics your home only has 25G of space. You could also install the miniforge/mambaforge folder in your personal folder instead.\nSay yes, when you get asked whether conda should be initialized during start up\nRestart the shell (exit the terminal and use ssh to log back in) for the changes to take effect\nCheck if conda is accessible by running conda -h\n\n\n\nSetting up an environment\nLet’s assume we want to install seqkit, a tool that allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length per sample.\nWe can install seqkit into a separate environment, which we can give a descriptive name, as follows:\n\n#check if the tool is installed (should return \"command not\" found if the software is not installed)\nseqkit -h\n\n#create an empty environment and name it seqkit and we add the version number to the name\n#this basically sets up seqkit separate from our default working environment\n#this is useful whenever software require complicated dependencies allowing us to have a separate install away from software that could conflict with each other\nmamba create -n seqkit_2.6.1\n\n#install seqkit, into the environment we just created\nmamba install -n seqkit_2.6.1 -c bioconda seqkit=2.6.1\n\n#to run seqkit, we need activate the environment first\nmamba activate seqkit_2.6.1\n\n#check if tool is installed, \n#if installed properly this should return some detailed information on how to run seqkit\nseqkit -h\n\n#run the tool via srun\nmkdir results/seqkit\nsrun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\nless -S results/seqkit/seqkit_stats.tsv \n\n#close the environment\nconda deactivate\n\nWhen installing seqkit, we:\n\nspecify the exact version we want to download with =2.6.1. We could also install the newest version that conda/mamba can find by running mamba install -n seqkit -c bioconda seqkit.\nspecify that we want to look for seqkit in the bioconda channel with the option -c. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.\n\nForgot what conda environments you installed in the past? You can run conda env list to generate a list of all existing environments.\nUnsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page. This page should inform you whether you need to add a specific channel to install the software as well as the version numbers available.\nA full set of mamba/conda commands can be found here.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nDownload and view the file results/seqkit/seqkit_stats.tsv on your own computer\nRun the seqkit again but this time submit the job via a sbatch script instead of using srun. Notice, that you need to tell SLURM how it can activate the conda environment that has seqkit installed. You might need to google how to do that, since this requires some extra line of code that we have not covered yet but see this as a good exercise for how to handle error messages that you see in the log files for your own analyses\n\n\n\nClick me to see an answer\n\n\n#question 1\nscp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .\n\n#question 2 \nsbatch scripts/seqkit.sh\n\nContent of scripts/seqkit.sh:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit_job\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\n#activate dependencies\nsource ~/.bashrc\nmamba activate seqkit_2.6.1\n\n#run seqkit\necho \"Start seqkit\"\n\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\n\necho \"seqkit finished\"\n\nIn the script above, we see that we need to add two lines of code to activate the seqkit conda environment:\nsource ~/.bashrc\nmamba activate seqkit_2.6.1\nWhen you run a script or a command, it operates in its own environment. The source command is like telling the script to look into another file, in this case, ~/.bashrc, and execute the commands in that file as if they were written directly into the script.\nHere, source ~/.bashrc is telling the script to execute the commands in the ~/.bashrc file. This is typically done to set up environment variables, paths, and activate any software or tools that are required for the script to run successfully. In our case this tells Slurm where we have installed conda and thus enables Slurm to use conda itself.\nThis allows slurm to, after executing source ~/.bashrc, activates a Conda environment using mamba activate seqkit_2.6.1. This ensures that the SeqKit tool and its dependencies are available and properly configured for use in the subsequent part of the script.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/code_documentation.html",
    "href": "source/code_documentation.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might want to work with your code. You want to document your code with as much detail as you would fill out a lab book as your documentation will help others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R but are general enough to apply to other computational languages (and scientific disciplines).\n\n\n\n\n\n\nNote\n\n\n\nThe information in this section is not part of the actual tutorial but was added to give you a starting point for how to document your code.\nIf you follow the in-person tutorial you might want to start by recording your notes using a plain text editor but feel free to explore the more advanced options after the tutorial.\n\n\n\n\n\n\n\nWhen documenting code, its best to avoid visual editors, such as Word, as these editors are not designed for writing code and easily destroy the formatting by for example changing ` to ’, which when writing code is quite a big difference.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as separating the code from comments, adding headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown (more on Markdown in a second) that allows you to integrate R code directly into your documentation.\nIf you have not installed R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nAfter you are done writing your documentation you can knit the document into an HTML, PDF or word document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio that can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations. If you do not have R and Rstudio installed, follow these instructions.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different computational languages. VSCode might take a bit longer to setup than RStudio but offers more flexibility due to various extensions that users can install.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.\n\n\n\n\n\n\nMarkdown is a lightweight markup language that is easy to read and write. It allows you to add formatting elements, such as headers, to plain text documents.\nHeaders:\nUse # to add a header and separate different sections of your documentation. The more # symbols you use after each other, the smaller the header will be. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers do not have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks followed by the computational language, i.e. bash or python, used.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nYou can easily add links to external resources or within your documentation as follows:\n[Link Text](https://www.example.com)\nEmphasis:\nYou can use * or _ to write italic and ** or __ for bold text.\n*italic*\n**bold**\nPictures\nYou can also add images to your documentation as follows:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/code_documentation.html#documenting-code",
    "href": "source/code_documentation.html#documenting-code",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might want to work with your code. You want to document your code with as much detail as you would fill out a lab book as your documentation will help others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R but are general enough to apply to other computational languages (and scientific disciplines).\n\n\n\n\n\n\nNote\n\n\n\nThe information in this section is not part of the actual tutorial but was added to give you a starting point for how to document your code.\nIf you follow the in-person tutorial you might want to start by recording your notes using a plain text editor but feel free to explore the more advanced options after the tutorial.",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/code_documentation.html#choose-your-editor",
    "href": "source/code_documentation.html#choose-your-editor",
    "title": "Introduction to the cli",
    "section": "",
    "text": "When documenting code, its best to avoid visual editors, such as Word, as these editors are not designed for writing code and easily destroy the formatting by for example changing ` to ’, which when writing code is quite a big difference.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as separating the code from comments, adding headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown (more on Markdown in a second) that allows you to integrate R code directly into your documentation.\nIf you have not installed R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nAfter you are done writing your documentation you can knit the document into an HTML, PDF or word document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio that can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations. If you do not have R and Rstudio installed, follow these instructions.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different computational languages. VSCode might take a bit longer to setup than RStudio but offers more flexibility due to various extensions that users can install.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/code_documentation.html#markdown-for-documentation",
    "href": "source/code_documentation.html#markdown-for-documentation",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Markdown is a lightweight markup language that is easy to read and write. It allows you to add formatting elements, such as headers, to plain text documents.\nHeaders:\nUse # to add a header and separate different sections of your documentation. The more # symbols you use after each other, the smaller the header will be. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers do not have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks followed by the computational language, i.e. bash or python, used.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nYou can easily add links to external resources or within your documentation as follows:\n[Link Text](https://www.example.com)\nEmphasis:\nYou can use * or _ to write italic and ** or __ for bold text.\n*italic*\n**bold**\nPictures\nYou can also add images to your documentation as follows:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will explain how use bash and connect and work with an HPC. To follow this tutorial please visit this website.\nYou first will learn how to set up a terminal and document your code. Afterwards, the actual tutorial will begin, in which you will assume the role of a researcher who just received data from a sequencing center and wants to starts analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, you will learn how to organize, view and extract information from your data using bash and the command line interface\nThen you will learn how to work with this data on an HPC. You will learn how to connect to an HPC, upload your sequencing data and submit a job in order to assess the quality of your sequencing data\n\nThis tutorial was written for researchers at the Institute for Biodiversity and Ecosystem Dynamics (IBED) at the University of Amsterdam who can get access to Crunchomics, the Genomics Compute Environment for SILS and IBED. For researchers that do not have access to Crunchomics or another slurm-based HPC, then you still can follow most sections of this tutorial on your own computer if you are interested in learning about bash."
  },
  {
    "objectID": "readme.html#welcome-page",
    "href": "readme.html#welcome-page",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will explain how use bash and connect and work with an HPC. To follow this tutorial please visit this website.\nYou first will learn how to set up a terminal and document your code. Afterwards, the actual tutorial will begin, in which you will assume the role of a researcher who just received data from a sequencing center and wants to starts analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, you will learn how to organize, view and extract information from your data using bash and the command line interface\nThen you will learn how to work with this data on an HPC. You will learn how to connect to an HPC, upload your sequencing data and submit a job in order to assess the quality of your sequencing data\n\nThis tutorial was written for researchers at the Institute for Biodiversity and Ecosystem Dynamics (IBED) at the University of Amsterdam who can get access to Crunchomics, the Genomics Compute Environment for SILS and IBED. For researchers that do not have access to Crunchomics or another slurm-based HPC, then you still can follow most sections of this tutorial on your own computer if you are interested in learning about bash."
  },
  {
    "objectID": "example_doc/example_notebook.html",
    "href": "example_doc/example_notebook.html",
    "title": "My report for analysis X (project title)",
    "section": "",
    "text": "Nina Dombrowski\n\n2023-12-18"
  },
  {
    "objectID": "example_doc/example_notebook.html#description",
    "href": "example_doc/example_notebook.html#description",
    "title": "My report for analysis X (project title)",
    "section": "Description",
    "text": "Description\nThe goal of this workflow is to analyse sequencing data received on 23.01.2022 from Data Science Sequencing Centre. The sequencing data consists of 8 samples, 4 control samples and 4 sulfur-treatments. The code below documents the cleaning of the sequences and the analyses run in QIIME."
  },
  {
    "objectID": "example_doc/example_notebook.html#pre-requisites",
    "href": "example_doc/example_notebook.html#pre-requisites",
    "title": "My report for analysis X (project title)",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nAll analyses were run on an HPC with an CentOS 7 operating system.\nDependencies:\n\nFastQC v0.11.9 (pre-installed)\nseqkit v2.6.1\n…\n\nInstallation of software that was not pre-installed:\n\nmamba create -n seqkit_2.6.1\nmamba install -n seqkit_2.6.1 -c bioconda seqkit=2.6.1"
  },
  {
    "objectID": "example_doc/example_notebook.html#analysis-workflow",
    "href": "example_doc/example_notebook.html#analysis-workflow",
    "title": "My report for analysis X (project title)",
    "section": "Analysis workflow",
    "text": "Analysis workflow\n\nSetup working directory\n\n#path to working directory\ncd /path_to_folder_you_analyse_the_data\n\n#activate conda environment \nmamba activate seqkit_2.6.1\n\n#custom variables \ndownload_link=\"https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\"\n\n\n\nDownload the sequencing data\n\nmkdir data\nmkdir scripts \nmkdir logs \n\n#download data using link provided by the sequencing center on 23.01.2023\nwget -P data $download_link\ntar -xvf data/seq_project.tar.gz -C data\nrm data/*.tar.gz\n\n#generate a list for the files we work with \nls data/seq_project/*/*.gz | cut -f4 -d \"/\" &gt; samples.txt\n\n\n\nPerform sanity checks\n\n#we work with 8 files\nwc -l samples.txt\n\n#we have as many R1 as we have R2 files\nls data/seq_project/*/*R1* | wc -l\nls data/seq_project/*/*R2* | wc -l\n\n#the sequence header looks as expected\nzcat data/seq_project/*/*gz | head\n\n#R1 and R2 reads have the same number of reads \nfor i in `cat samples.txt`; do \n    echo \"$i: $(( $(zcat data/seq_project/*/$i | wc -l) /4 ))\"\ndone &gt; read_counts_per_sample.txt \n\nWe work with so many reads per sample:\nSample-DUMMY1_R1.fastq.gz: 44846\nSample-DUMMY1_R2.fastq.gz: 44846\nSample-DUMMY2_R1.fastq.gz: 19733\nSample-DUMMY2_R2.fastq.gz: 19733\nSample-DUMMY3_R1.fastq.gz: 19784\nSample-DUMMY3_R2.fastq.gz: 19784\nSample-DUMMY4_R1.fastq.gz: 24380\nSample-DUMMY4_R2.fastq.gz: 24380\n\n\nGet read statistics\n\nRun FastQC\n\nmkdir -p results/fastqc \n\n#Submitted batch job 8100\nsbatch scripts/run_fastqc.sh \n\nll results/fastqc/*\n\nComments on the visual inspection of the HTML reports generated by FastQC:\n\nThe sequence quality looks good, most reads have a phred score between 28-40 when looking at the per base quality\nThe reads are typically 250 bp long (as expected)\nIn the over-represented sequences no hits to Illumina adapters where found\n\n\n\nRun seqkit\n\nmkdir -p results/seqkit \n\n#Submitted batch job 8114\nsbatch scripts/seqkit.sh\n\nComments on the content:\n\nSample Sample-DUMMY1_R2.fastq.gz had reads of only 35 length, check with the sequencing center of something went wrong or if they did some quality cleaning as before sending the sequences\nOther parameters look good\n\n…."
  },
  {
    "objectID": "example_doc/example_notebook.html#citation",
    "href": "example_doc/example_notebook.html#citation",
    "title": "My report for analysis X (project title)",
    "section": "Citation",
    "text": "Citation\nIf your work gets published and you provide the code for example on Github its useful to add information about how to cite your work"
  },
  {
    "objectID": "example_doc/example_notebook.html#contact",
    "href": "example_doc/example_notebook.html#contact",
    "title": "My report for analysis X (project title)",
    "section": "Contact",
    "text": "Contact\nFor questions, please contact nd@sds.net"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will explain how to use bash and connect and work with an HPC.\nOn the first pages you will get some introduction into setting up a terminal and documenting your code. Afterwards, the actual tutorial will begin, in which you will assume the role of a researcher who received data from a sequencing center and wants to start analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, you will learn how to organize, view and extract information from your data using bash and the command line interface\nIn the second section, you will learn how to work with data on an HPC. Therefore, you will learn how to connect to an HPC, upload your sequencing data and submit a job in order to assess the quality of your sequencing data\n\nThis tutorial was written for researchers at the Institute for Biodiversity and Ecosystem Dynamics (IBED) at the University of Amsterdam who can get access to Crunchomics, the Genomics Compute Environment for SILS and IBED. For researchers that do not have access to Crunchomics, or another slurm-based HPC, then you still can follow most sections of this tutorial on your own computer if you are interested in learning about bash.\nFor information not covered in this tutorial, check out IBED’s bioinformatics guidance page.",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "index.html#welcome-page",
    "href": "index.html#welcome-page",
    "title": "Introduction to the cli",
    "section": "Welcome page",
    "text": "Welcome page\nOn this website you will find a short tutorial that will explain how to use bash and connect and work with an HPC.\nOn the first pages you will get some introduction into setting up a terminal and documenting your code. Afterwards, the actual tutorial will begin, in which you will assume the role of a researcher who received data from a sequencing center and wants to start analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, you will learn how to organize, view and extract information from your data using bash and the command line interface\nIn the second section, you will learn how to work with data on an HPC. Therefore, you will learn how to connect to an HPC, upload your sequencing data and submit a job in order to assess the quality of your sequencing data\n\nThis tutorial was written for researchers at the Institute for Biodiversity and Ecosystem Dynamics (IBED) at the University of Amsterdam who can get access to Crunchomics, the Genomics Compute Environment for SILS and IBED. For researchers that do not have access to Crunchomics, or another slurm-based HPC, then you still can follow most sections of this tutorial on your own computer if you are interested in learning about bash.\nFor information not covered in this tutorial, check out IBED’s bioinformatics guidance page.",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "source/bash_intro.html",
    "href": "source/bash_intro.html",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing and starting the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\n\npwd\n\npwd prints the location of the current working directory and tells you where exactly you are in the file system. When we start our terminal we typically start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but the path might be slightly different depending on your operating system. Below you find some hints to orient yourself better for different terminal interfaces/operating systems:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder that gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn General set Persistent home directory to a folder of your choice\n\nTo access the current folder in the file explorer you can type explorer.exe .\nThe path to the desktop should something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the current directory in the file explorer you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nAccessing the Uva OneDrive folder:\nTo access the OneDrive Uva folder, you need to add quotes between the absolute file path since bash does otherwise not know what to do with the space in the file name, i.e. cd \"/mnt/c/Users/UserName/OneDrive - Uva\". This is also a good example for why you normally do NOT want to add spaces in your filepath.\n\n\n\n\n\n\nNow that we know where we are, let’s find out what files and folders exist in our home directory. For this you can use the ls command, which allows us to list all the contents of the directory you are currently in:\n\nls\n\nIn my case this returns something like this (and this will look different for your current directory):\n\n\n\nThe colors will look different depending on the CLI you use but in the example above you see a list of files (in bold text) and folders (green-highlighted text) that can be found in my current directory.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands by looking at the general structure of a command.\n\n\n\nA command generally consists of three elements, the command itself and some optional options and arguments:\n\n\n\nUnderstanding better what a command looks like, let’s use the ls command together with the option -l. This option results in ls printing the content of a folder in what is called a long listing format.\n\nls -l\n\nAfter running this, we see our files and folders again but in a long format, which gives more detailed information and structures our output a bit better. In the example below we can see that we now print additional information about who owns the files (i.e. access modes), how large the files are, when they were last modified and of course the name:\n\n\n\n\n\n\nIf you want to know what options are available for a command (or software tool) it is always a good idea to check out the manual. In most of the cases you can do this with:\n\nman ls\n\nYou exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways to access the manual. The most common ways are:\n\nman ls\nls --help\nls -h\n\n\n\n\nMost of the time you do not want to perform your analyses in the home directory but in a dedicated folder for your project. To get started, we will learn about the cd command that allows us to move around the file system.\nThe file system is a hierarchical system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to, for example, reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use what is called a relative path.\nTo start moving around the file system, let’s begin by moving relative to our working directory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory. If you run this, choose any folder you see after running the ls command:\n\ncd source/\n\nIf you use pwd after moving around directories, you should see that a different location is printed to the screen.\nWe can also move back to our original directory using cd ... This command will move us back one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, ~, which can be used as a shortcut to move directly into our home directory from wherever you are on the file system:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd.\n\n\n\n\n\n\nNow that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and then create and move into a new folder called data_analysis as follows:\n\n#go into the home directory\ncd ~\n\n#in the home directory make a new folder and name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated correctly\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: Commenting your code\n\n\n\n\n\nNotice, how in the example below I added the commands to run as well as some explanation about what I did?\nHere, I used a specific character # in front of a line of text to denote the beginning of a single-line comment. Anything coming after the character is considered a comment and won’t be executed by the shell.\nIn the above example I definitely commented the code too much as my comments basically duplicate the code and that should be avoided, however, it is useful to add comments in code to ensure that the purpose of the code is clear to others.\nYou could, for example, add a comment above functions to describe what the function does and you can also add comments to “tricky” code where it is not immediately obvious what you are trying to do. Basically, you want to add comments to any section where you think that the future you might get confused a month later.\nHere you find some examples for python and R but the same logic applies when writing code for bash, some examples for that can be found here.\n\n\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name.\nWhile we talk about being lazy: If you press the arrow-up key on your keyboard you can cycle through commands you already ran and this often times can be useful to recycle code snippets (i.e. if you are trying certain settings and want to quickly change a small section of your code).\n\n\n\n\n\n\nNext, let’s download our sequencing data, called seq_project.tar.gz using wget. In the command below we add the option -P to specify into which folder we want to download the data.\n\n#download data using link provided by the sequencing center on 23.01.2023\nwget https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\nIn this example, you can also see how I documented my code here in order to add some more specifics about where the link came from to help future me in case I, for example, need to look for some e-mails about the data later on.\n\n\n\n\n\n\nWget for Mac users\n\n\n\n\n\nFor Mac users wget is not installed by default. If you do not want to install it, run the following:\n\ncurl -LO https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l\n\n#question 2:\nls -lh\n\n\n\n\n\n\n\n\nWhen working with many files a good folder structure is essential to keep track of our files. To keep our project folder organized, we will copy the seq_project.tar.gz file we just downloaded into a special folder that we will call data.\n\n#make a folder for our raw data\nmkdir data\n\n#move the tar.gz file into the data folder \ncp seq_project.tar.gz data/\n\n#check what happened\nls -l\nls -l data\n\nWhen running the two ls commands, we see that we now have two copies of seq_project.tar.gz, one file is in our working directory and the other one is in our data folder. Copying large amounts of data is not ideal since we will use unneccassary space. However, we can use another command to move the tar folder into our data folder.\n\n\n\nmv is the command we can use to move data across locations without generating a copy. We run it as follows:\n\n#move the tar.gz file into the data fodler \nmv seq_project.tar.gz data/\n\n#check what happened\nls -l\nls -l data\n\nNotice that mv will move the tar file and, without asking, overwrite the existing tar file we had in the data folder when we ran cp. This means for you that if you run move its best to make sure that you do not overwrite files by mistake.\n\n\n\nThe data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is commonly used when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use with the tar command are:\n\nx tells tar to extract files from the archive\nv displays verbose information while creating the tarball\nf specifies the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content of the folder we just extracted. Ensure that you explore the content of potential sub-directories.\nFind the path for at least one sequence file that ends on fastq.gz. Hint: to make your life easier, check the Tip: Command-line completion above to not have to type every single word.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project\n\n#question 2:\nls -l data/seq_project/barcode001_User1_ITS_1_L001/\n\n\n\n\n\n\n\n\nTo remove files and folders, we use the rm command. We want to remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!\n\n\n\n\n\nAfter we have explored the folder with our sequencing data with ls, we have seen that data/seq_project contains 4 folders, 2 folders for two different users who each generated two samples.\nYou also might have also noticed that it gets tedious to figure out how many files are in each folder because we would need to run ls on each single folder and view its contents individually.\nLuckily, the shell provides special characters to rapidly specify groups of filenames or patterns. Wild-cards are characters that can be used as a substitute for any class of characters in a search.\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. It allows us to list all files in the seq_project folder as follows:\n\nls data/seq_project/*\n\n\nYou should have seen before that the sequencing files all end with .fastq.gz we can make this command a bit more specific and at the same time make the output a bit more readable:\n\nls data/seq_project/*/*.gz\n\n\nIf we wanted to print the absolute and not the relative path, we could do the following (beware that the absolute path will be slightly different depending in you system):\n\nls /home/User_name/data/seq_project/*/*.gz\n\nNow we can easily see for each folder how many files we have.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a wildcard to list files for barcode001 only\nUse a wildcard to only list R1 (i.e. forward) files\nUse a wildcard to copy all R1 files into a new folder, call this new folder R1_files. Use ls to confirm if you copied the right files\nTo avoid duplicates, remove the R1_files folder from question 3 after you have ensured the command you used worked\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls data/seq_project/barcode001*/*gz\n\n#question 2:\nls data/seq_project/*/*R1*\n\n#question 3:\nmkdir R1_files\ncp data/seq_project/*/*R1* R1_files\nls data/seq_project/*/*.gz\nls -l R1_files\n\n#question 4:\nrm -r R1_files\n\nAfter solving question 2 you should see:\n\nAfter solving question 3, we should see that all files still exist in its original location but are now also inside of the R1_files folder. Notice, how we can use wildcards together with other commands to easily reorganize our data?\n\n\n\n\n\n\n\n\n\n\nTip: More wildcards\n\n\n\n\n\n* is not the only wildcard we can use and a full list can be found here.\nDifferent wildcards can be useful in different contexts, not only only when listing files but also finding patterns in files. To make such searches as specific as possible there are different wildcards available. A few examples are:\nThe [0-9] wildcard = matches any number exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[0-9]*/*.gz\n\nThe [012] wildcard = matches 1 or 2 exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[12]*/*.gz\n\n[A-Z] matches any letter in capitals occurring once\n[a-z]* matches any letter in non-capital letters occurring many times\n\nls data/seq_project/barcode001_User1_ITS_1_[A-Z]001/*.gz\n\n\n\n\n\n\n\nWe have seen that by default our commands directly print the standard output to the terminal. For example, when we use ls the list of files and folders is printed to your terminal. However, we can also redirect the standard output to a file by using the &gt; character.\nFor example, we might want to generate a list with all fastq files and can do this as follows:\n\nls data/seq_project/*/* &gt; fastq_paths.txt\n\n\n\n\nNext, let’s view the content of the list we just generated. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nhead can be used to check the first 10 rows of our files:\n\nhead fastq_paths.txt\n\n\n\n\nIf you want to check the last 10 rows use tail:\n\ntail fastq_paths.txt\n\n\n\n\nless is a program that let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S fastq_paths.txt\n\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n\n\n\n\nTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano fastq_paths.txt\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter\n\n\n\n\n\n\n\n\nAnother useful tool is the wc (= wordcount) command that allows us to count the number of lines via -l in a file. It is an useful tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files.\nWe could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nOne down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command.\n\n\n\nPipes are a powerful utility to connect multiple commands together. Pipes allow us to feed the standard output of one command, such as ls as input into another command, such as wc -l, and as such combine multiple commands together.\nTherefore, let`s use ls to first list all fastq files and then pipe the output from ls into the wc command in order to count with how many files we work with:\n\nls data/seq_project/*/* | wc -l\n\nWe should see again that we work with 8 files, but now we did not have to generate an intermediate text file and have a more condensed command.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse ls and wc to count how many R1 files we have\nCount how many R2 files we have\nCount how many files were generated for User1\n\n\n\nClick me to see an answer\n\n\n#question 1: We see 4 files\nls data/seq_project/*/*R1*gz | wc -l \n\n#question 2: We see 4 files\nls data/seq_project/*/*R2*gz | wc -l \n\n#question 3: We see 4 files\nls data/seq_project/*_User1_*/*gz | wc -l \n\nChecking whether we have the same number of R1 and R2 files is a good sanity check, to ensure that we have received all the files from the sequencing center whenever we generate paired-end sequencing data.\n\n\n\n\n\n\n\nRemember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt)?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\nhead fastq_paths.txt\n\n#extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#do the same as above but this time save the output in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:\n\n\n\n\nsort: sorts lines in a file from A-Z\nuniq: removes or finds duplicates . For this command to work you need to provide it with a sorted file\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse cut on fastq_paths.txt to print the folder names in which the fastq.gz files reside (i.e. barcode001_User1_ITS_1_L001)\nPrint the folder names as in 1 and then extract the user names from this list\nEnsure that when running the code from 2 that we print a unique list of user names\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncut -f3 -d \"/\" fastq_paths.txt\n\n#question 2:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#question 3:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\n\ncat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files. For concatenation we can provide cat with multiple file listed after each other or use a wildcard.\n\nTo view files we do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncat data/seq_project/*/*R1.fastq.gz &gt; combined_R1.fastq.gz\n\n#question 2:\nls -lh data/seq_project/*/*R1.fastq.gz\nls -lh combined_R1.fastq.gz\n\nIf you compare the two numbers generated for question 2, you should see that the combined.fastq.gz file is roughly the sum of the individual files.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nView the first few rows of any one of the fastq.gz sequence files\nDoes this look like a normal sequence file to you?\n\n\n\nClick me to see an answer\n\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, so what is going on?\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloaded folder, you have seen that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. gzip is a tool used to (de)-compress the size of files.\nIn order to view the content of compressed files, we sometimes need to de-compress them first. We can do this using the gzip command together with the decompress -d option:\n\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows of information for each sequence as shown below:\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running the gzip command above, make a list of each sequence file with ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project/*/*\n\n#question 2:\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with sequencing files the data is usually much larger and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nls data/seq_project/*/*\n\n\n\nWe have seen that to view the content of a compressed file and make sense of the content we had to first decompress the file. However, sequence files tend to get rather large and we might not want to decompress our files to not clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen called zcat. When running zcat on a file then the original compressed file is kept as is. We combine the zcat command with the head command, since we do not want to print millions of sequences to the screen but only want to explore the first few rows:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head\n\n\n\n\n\n\n\nZcat for Mac users\n\n\n\n\n\nFor Mac users using the zsh shell, zcat might not work as expected. Try using gzcat instead:\n\ngzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head\n\n\n\n\n\n\n\n\n\nThe grep command is used to search for words, or patterns, in any text file. This command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c to count how often the pattern we search for occurs.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also use grep with wildcards, for example, if we want to look only at samples from User1 we could do the following:\n\ngrep \"User1_*\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing the pattern “ITS_9” in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files (advanced)\nCount how often “AAGACG” occurs in any in any of the fastq.gz files (advanced)\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if sequencing adapters or primers are still part of your sequence.\n\n\n\n\n\n\n\nWe have seen before that wc is a command that allows us to count the number of lines in a file and we can easily use it on fastq files to get an idea about how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We need to divide the number we see on the screen by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: Better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $(( $(wc -l) /4))\n\nIn the command above we have some new syntax:\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( $(wc -l) /4))\n$(...) indicates a command substitution. It allows the output of the enclosed command (wc -l) to be used as part of another command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”.\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line. In this case, it calculates the result of dividing the number of lines in the fastq file by 4.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences.\n\n\n\n\n\n\n\nImagine we want to count the lines not only from one but all files. Could we do something like the code below?\n\nzcat data/seq_project/*/*.gz | wc -l\n\nWhen running this, we see that the command prints a single number, 869 944, but not the counts for each file, so something did not work right.\nThe problem with this command is that it prints the text from all 8 fastq files and only afterwards performs the counting. We basically concatenated all files and then calculated the sum of all 8 files. However, what we want to do is to repeat the same operation over and over again:\n\nDecompress a first file\nCount the lines in the first file\nDecompress a second file\nCount the lines in the second file\n…\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nLet’s start with a simple example but before that let’s introduce a simple command echo. echo is used to print information, such as Hello to the terminal:\n\necho \"Welcome 1 time!\"\n\nWe can use for-loops to print something to the screen not only one, but two, three, four … times as follows:\n\nfor i in 1 2 3; do \n    echo \"Welcome $i times\"\ndone\n\nAn alternative and more condensed way of writing this that you might encounter in the wild is the following:\n\nfor i in 1 2 3; do echo \"Welcome $i times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nLet’s try to do the same but now for our sequencing files by storing the individual files found in data/seq_project/*/*.gz in the variable i and print i to the screen in a for-loop.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"I work with: File $i\"\ndone\n\n\nfor i in data/seq_project/*/*.gz; do: This part initializes a loop that iterates over all files matching the pattern data/seq_project/*/*.gz. The variable i is assigned to each file in succession.\n\nWe can then use these variables stored in i to perform more useful operations, for example for each file, step-by-step, count the number of lines in each file by using some of the tools we have seen before:\n\nfor i in data/seq_project/*/*.gz; do \n    zcat $i | wc -l\ndone\n\n\nzcat $i | wc -l: This is the action performed inside the loop. zcat is used to concatenate and display the content of compressed files (*.gz). The | (pipe) symbol redirects this output to wc -l, which counts the number of lines in the uncompressed content.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a for loop and count how often grep finds the pattern “TAAGA” in each individual fastq.gz file\n\n\n\nClick me to see an answer\n\n\nfor i in data/seq_project/*/*.gz; do \n    zcat $i | grep -c \"TAAGA\"\ndone\n\n\n\n\n\nSometimes it is useful to not store the full path in i, especially when we want to store the output of our loop in a new file. Luckily, we can use the list of files we stored in fastq_files.txt to rewrite this command a bit and make use of the fact that we can use cat to print something in a text file to the screen:\n\nfor i in `cat fastq_files.txt`; do  \n    zcat data/seq_project/*/$i | wc -l\ndone\n\n\nfor i in `cat fastq_files.txt`; do: This initiates a loop that iterates over each item in the file fastq_files.txt. The backticks ` are used to execute the cat command within the backticks to read the content of the text file and assign its output to the variable i.\nzcat data/seq_project/*/$i | wc -l: Like before, this line uncompresses (zcat) and counts (wc) the number of lines in the specified file. However, in this case, the file is determined by the content of fastq_files.txt, which contains a list of file names.\n\nAfter this modification of the code, we can very easily store the output in a file instead of printing the results to the screen.\n\n#Count the number of lines in each sequencing file\n#Store the output in a new folder\nmkdir counts \n\nfor i in `cat fastq_files.txt`; do \n    zcat data/seq_project/*/$i | wc -l &gt; counts/$i.txt\ndone\n\nls -l counts/*txt \n\nhead counts/Sample-DUMMY1_R1.fastq.gz.txt\n\n\n\n\n\n\n\nAvanced Tip: Better counting in for loops\n\n\n\n\n\nLet’s get a bit more advanced to show you some powerful features of bash. For this imagine that you would do this for 100 files. In this case it would be useful to see the file names next to the counts. We can achieve this by using what we have learned in the Better counting tip where we have learned about echo and command substitution.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(zcat $i | wc -l)\"\ndone\n\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on here \"$i: $(zcat $i | wc -l)\"\n$(...): These parentheses are used for command substitution. It means that the command zcat $i | wc -l is executed, and its output (the line count of the uncompressed content) is substituted in that position.\nThe double quotes (““) perform what is called a string concatenation. It concatenates the filename ($i), a colon (:), a space, and the line count obtained from the command substitution. The entire string is then passed as a single argument to the echo command.\n\nAlmost perfect, now we only want to divide this by 4:\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(( $(zcat $i | wc -l) /4 ))\"\ndone\n\nNotice, how we use both single brackets and double brackets?\n\n$((...)) in contrast to $(...) is an arithmetic expansion in Bash. It allows you to perform arithmetic operations and substitute the result into the command line.\n\n\n\n\n\n\n\n\n\n\nAvanced Tip: Using bash to make sample mapping files\n\n\n\n\n\nFor a lot of mapping files for amplicon analyses, you need to prepare a table that lists your files, often with the R1 and R2 files in separate columns. How could we do this using bash?\nThis code is more advanced and uses a few concepts we have not talked about but should give you an idea about the different data transformations you can do with bash.\n\nfor file in data/seq_project/*/*.gz; do\n    if [[ $file == *_R1.fastq.gz ]]; then\n        r1_file=$file\n        r2_file=$(echo $file | sed 's/_R1/_R2/')\n        echo \"$r1_file,$r2_file\"\n    fi\ndone\n\n\nfor file in data/seq_project/*/*.gz: We loop through all fastq.gz files\nif [[ $file == *_R1.fastq.gz ]]; then: This line checks if the current file’s name matches the pattern *_R1.fastq.gz. The [[ ... ]] is a conditional construct in bash, and the == is used for string comparison. If the condition is true (i.e., the file is an R1 file), then the code inside the if block will be executed. The block starts with ifand finishes at the fi.\nr1_file=$file: This line involves creating a variable named r1_file and assigning it the value of the current file ($file). A variable is a symbolic name or identifier associated with a value or data. It allows you to store and retrieve information for later use. Here, r1_file is used to store the name of the current R1 file.\nr2_file=$(echo $file | sed 's/_R1/_R2/'): This line sets the variable r2_file by replacing _R1 with _R2 in the current file name. This is done using the sed command, which is a stream editor for filtering and transforming text.\necho \"$r1_file,$r2_file\": This line prints the R1 and R2 file names separated by a comma. This will be part of the output in the format R1,R2.\n\nWe can do even more and add a column with the sample name with what we have learned before:\n\nfor file in data/seq_project/*/*.gz; do\n    if [[ $file == *_R1.fastq.gz ]]; then\n        sample_name=$(echo $file | cut -f3 -d \"/\" )\n        r1_file=$file\n        r2_file=$(echo $file | sed 's/_R1/_R2/')\n        echo \"$sample_name,$r1_file,$r2_file\"\n    fi\ndone\n\nYou can easily combine this with some of the things we learned, such as using cut to extract specific parts of the sample_name as well. I.e. lets assume you only want to list the index:\n\nfor file in data/seq_project/*/*.gz; do\n    if [[ $file == *_R1.fastq.gz ]]; then\n        sample_name=$(echo $file | cut -f3 -d \"/\"  |cut -f1 -d \"_\" ) \n        r1_file=$file\n        r2_file=$(echo $file | sed 's/_R1/_R2/')\n        echo \"$sample_name,$r1_file,$r2_file\"\n    fi\ndone",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#pwd-find-out-where-we-are",
    "href": "source/bash_intro.html#pwd-find-out-where-we-are",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing and starting the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\n\npwd\n\npwd prints the location of the current working directory and tells you where exactly you are in the file system. When we start our terminal we typically start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but the path might be slightly different depending on your operating system. Below you find some hints to orient yourself better for different terminal interfaces/operating systems:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder that gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn General set Persistent home directory to a folder of your choice\n\nTo access the current folder in the file explorer you can type explorer.exe .\nThe path to the desktop should something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the current directory in the file explorer you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nAccessing the Uva OneDrive folder:\nTo access the OneDrive Uva folder, you need to add quotes between the absolute file path since bash does otherwise not know what to do with the space in the file name, i.e. cd \"/mnt/c/Users/UserName/OneDrive - Uva\". This is also a good example for why you normally do NOT want to add spaces in your filepath.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#ls-list-the-contents-of-a-directory",
    "href": "source/bash_intro.html#ls-list-the-contents-of-a-directory",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know where we are, let’s find out what files and folders exist in our home directory. For this you can use the ls command, which allows us to list all the contents of the directory you are currently in:\n\nls\n\nIn my case this returns something like this (and this will look different for your current directory):\n\n\n\nThe colors will look different depending on the CLI you use but in the example above you see a list of files (in bold text) and folders (green-highlighted text) that can be found in my current directory.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands by looking at the general structure of a command.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#the-structure-of-a-command",
    "href": "source/bash_intro.html#the-structure-of-a-command",
    "title": "Introduction to Bash",
    "section": "",
    "text": "A command generally consists of three elements, the command itself and some optional options and arguments:\n\n\n\nUnderstanding better what a command looks like, let’s use the ls command together with the option -l. This option results in ls printing the content of a folder in what is called a long listing format.\n\nls -l\n\nAfter running this, we see our files and folders again but in a long format, which gives more detailed information and structures our output a bit better. In the example below we can see that we now print additional information about who owns the files (i.e. access modes), how large the files are, when they were last modified and of course the name:",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#getting-help",
    "href": "source/bash_intro.html#getting-help",
    "title": "Introduction to Bash",
    "section": "",
    "text": "If you want to know what options are available for a command (or software tool) it is always a good idea to check out the manual. In most of the cases you can do this with:\n\nman ls\n\nYou exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways to access the manual. The most common ways are:\n\nman ls\nls --help\nls -h",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cd-move-around-folders",
    "href": "source/bash_intro.html#cd-move-around-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Most of the time you do not want to perform your analyses in the home directory but in a dedicated folder for your project. To get started, we will learn about the cd command that allows us to move around the file system.\nThe file system is a hierarchical system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to, for example, reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use what is called a relative path.\nTo start moving around the file system, let’s begin by moving relative to our working directory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory. If you run this, choose any folder you see after running the ls command:\n\ncd source/\n\nIf you use pwd after moving around directories, you should see that a different location is printed to the screen.\nWe can also move back to our original directory using cd ... This command will move us back one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, ~, which can be used as a shortcut to move directly into our home directory from wherever you are on the file system:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#mkdir-make-new-folders",
    "href": "source/bash_intro.html#mkdir-make-new-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and then create and move into a new folder called data_analysis as follows:\n\n#go into the home directory\ncd ~\n\n#in the home directory make a new folder and name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated correctly\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: Commenting your code\n\n\n\n\n\nNotice, how in the example below I added the commands to run as well as some explanation about what I did?\nHere, I used a specific character # in front of a line of text to denote the beginning of a single-line comment. Anything coming after the character is considered a comment and won’t be executed by the shell.\nIn the above example I definitely commented the code too much as my comments basically duplicate the code and that should be avoided, however, it is useful to add comments in code to ensure that the purpose of the code is clear to others.\nYou could, for example, add a comment above functions to describe what the function does and you can also add comments to “tricky” code where it is not immediately obvious what you are trying to do. Basically, you want to add comments to any section where you think that the future you might get confused a month later.\nHere you find some examples for python and R but the same logic applies when writing code for bash, some examples for that can be found here.\n\n\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name.\nWhile we talk about being lazy: If you press the arrow-up key on your keyboard you can cycle through commands you already ran and this often times can be useful to recycle code snippets (i.e. if you are trying certain settings and want to quickly change a small section of your code).",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wget-download-data",
    "href": "source/bash_intro.html#wget-download-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s download our sequencing data, called seq_project.tar.gz using wget. In the command below we add the option -P to specify into which folder we want to download the data.\n\n#download data using link provided by the sequencing center on 23.01.2023\nwget https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\nIn this example, you can also see how I documented my code here in order to add some more specifics about where the link came from to help future me in case I, for example, need to look for some e-mails about the data later on.\n\n\n\n\n\n\nWget for Mac users\n\n\n\n\n\nFor Mac users wget is not installed by default. If you do not want to install it, run the following:\n\ncurl -LO https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l\n\n#question 2:\nls -lh",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cp-copying-data",
    "href": "source/bash_intro.html#cp-copying-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "When working with many files a good folder structure is essential to keep track of our files. To keep our project folder organized, we will copy the seq_project.tar.gz file we just downloaded into a special folder that we will call data.\n\n#make a folder for our raw data\nmkdir data\n\n#move the tar.gz file into the data folder \ncp seq_project.tar.gz data/\n\n#check what happened\nls -l\nls -l data\n\nWhen running the two ls commands, we see that we now have two copies of seq_project.tar.gz, one file is in our working directory and the other one is in our data folder. Copying large amounts of data is not ideal since we will use unneccassary space. However, we can use another command to move the tar folder into our data folder.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#mv-moving-data",
    "href": "source/bash_intro.html#mv-moving-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "mv is the command we can use to move data across locations without generating a copy. We run it as follows:\n\n#move the tar.gz file into the data fodler \nmv seq_project.tar.gz data/\n\n#check what happened\nls -l\nls -l data\n\nNotice that mv will move the tar file and, without asking, overwrite the existing tar file we had in the data folder when we ran cp. This means for you that if you run move its best to make sure that you do not overwrite files by mistake.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#tar-work-with-tar-files",
    "href": "source/bash_intro.html#tar-work-with-tar-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is commonly used when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use with the tar command are:\n\nx tells tar to extract files from the archive\nv displays verbose information while creating the tarball\nf specifies the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content of the folder we just extracted. Ensure that you explore the content of potential sub-directories.\nFind the path for at least one sequence file that ends on fastq.gz. Hint: to make your life easier, check the Tip: Command-line completion above to not have to type every single word.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project\n\n#question 2:\nls -l data/seq_project/barcode001_User1_ITS_1_L001/",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#rm-remove-files-and-folders",
    "href": "source/bash_intro.html#rm-remove-files-and-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "To remove files and folders, we use the rm command. We want to remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wildcards",
    "href": "source/bash_intro.html#wildcards",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After we have explored the folder with our sequencing data with ls, we have seen that data/seq_project contains 4 folders, 2 folders for two different users who each generated two samples.\nYou also might have also noticed that it gets tedious to figure out how many files are in each folder because we would need to run ls on each single folder and view its contents individually.\nLuckily, the shell provides special characters to rapidly specify groups of filenames or patterns. Wild-cards are characters that can be used as a substitute for any class of characters in a search.\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. It allows us to list all files in the seq_project folder as follows:\n\nls data/seq_project/*\n\n\nYou should have seen before that the sequencing files all end with .fastq.gz we can make this command a bit more specific and at the same time make the output a bit more readable:\n\nls data/seq_project/*/*.gz\n\n\nIf we wanted to print the absolute and not the relative path, we could do the following (beware that the absolute path will be slightly different depending in you system):\n\nls /home/User_name/data/seq_project/*/*.gz\n\nNow we can easily see for each folder how many files we have.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a wildcard to list files for barcode001 only\nUse a wildcard to only list R1 (i.e. forward) files\nUse a wildcard to copy all R1 files into a new folder, call this new folder R1_files. Use ls to confirm if you copied the right files\nTo avoid duplicates, remove the R1_files folder from question 3 after you have ensured the command you used worked\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls data/seq_project/barcode001*/*gz\n\n#question 2:\nls data/seq_project/*/*R1*\n\n#question 3:\nmkdir R1_files\ncp data/seq_project/*/*R1* R1_files\nls data/seq_project/*/*.gz\nls -l R1_files\n\n#question 4:\nrm -r R1_files\n\nAfter solving question 2 you should see:\n\nAfter solving question 3, we should see that all files still exist in its original location but are now also inside of the R1_files folder. Notice, how we can use wildcards together with other commands to easily reorganize our data?\n\n\n\n\n\n\n\n\n\n\nTip: More wildcards\n\n\n\n\n\n* is not the only wildcard we can use and a full list can be found here.\nDifferent wildcards can be useful in different contexts, not only only when listing files but also finding patterns in files. To make such searches as specific as possible there are different wildcards available. A few examples are:\nThe [0-9] wildcard = matches any number exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[0-9]*/*.gz\n\nThe [012] wildcard = matches 1 or 2 exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[12]*/*.gz\n\n[A-Z] matches any letter in capitals occurring once\n[a-z]* matches any letter in non-capital letters occurring many times\n\nls data/seq_project/barcode001_User1_ITS_1_[A-Z]001/*.gz",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#io-redirection-to-new-files",
    "href": "source/bash_intro.html#io-redirection-to-new-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "We have seen that by default our commands directly print the standard output to the terminal. For example, when we use ls the list of files and folders is printed to your terminal. However, we can also redirect the standard output to a file by using the &gt; character.\nFor example, we might want to generate a list with all fastq files and can do this as follows:\n\nls data/seq_project/*/* &gt; fastq_paths.txt",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-text-files",
    "href": "source/bash_intro.html#exploring-the-content-of-text-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s view the content of the list we just generated. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nhead can be used to check the first 10 rows of our files:\n\nhead fastq_paths.txt\n\n\n\n\nIf you want to check the last 10 rows use tail:\n\ntail fastq_paths.txt\n\n\n\n\nless is a program that let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S fastq_paths.txt\n\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n\n\n\n\nTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano fastq_paths.txt\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wc-count-things",
    "href": "source/bash_intro.html#wc-count-things",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command that allows us to count the number of lines via -l in a file. It is an useful tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files.\nWe could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nOne down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#pipes",
    "href": "source/bash_intro.html#pipes",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Pipes are a powerful utility to connect multiple commands together. Pipes allow us to feed the standard output of one command, such as ls as input into another command, such as wc -l, and as such combine multiple commands together.\nTherefore, let`s use ls to first list all fastq files and then pipe the output from ls into the wc command in order to count with how many files we work with:\n\nls data/seq_project/*/* | wc -l\n\nWe should see again that we work with 8 files, but now we did not have to generate an intermediate text file and have a more condensed command.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse ls and wc to count how many R1 files we have\nCount how many R2 files we have\nCount how many files were generated for User1\n\n\n\nClick me to see an answer\n\n\n#question 1: We see 4 files\nls data/seq_project/*/*R1*gz | wc -l \n\n#question 2: We see 4 files\nls data/seq_project/*/*R2*gz | wc -l \n\n#question 3: We see 4 files\nls data/seq_project/*_User1_*/*gz | wc -l \n\nChecking whether we have the same number of R1 and R2 files is a good sanity check, to ensure that we have received all the files from the sequencing center whenever we generate paired-end sequencing data.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cut-extract-elements-from-strings",
    "href": "source/bash_intro.html#cut-extract-elements-from-strings",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Remember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt)?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\nhead fastq_paths.txt\n\n#extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#do the same as above but this time save the output in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#sort-and-uniq-create-unique-lists",
    "href": "source/bash_intro.html#sort-and-uniq-create-unique-lists",
    "title": "Introduction to Bash",
    "section": "",
    "text": "sort: sorts lines in a file from A-Z\nuniq: removes or finds duplicates . For this command to work you need to provide it with a sorted file\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse cut on fastq_paths.txt to print the folder names in which the fastq.gz files reside (i.e. barcode001_User1_ITS_1_L001)\nPrint the folder names as in 1 and then extract the user names from this list\nEnsure that when running the code from 2 that we print a unique list of user names\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncut -f3 -d \"/\" fastq_paths.txt\n\n#question 2:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#question 3:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cat-read-and-combine-files",
    "href": "source/bash_intro.html#cat-read-and-combine-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "cat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files. For concatenation we can provide cat with multiple file listed after each other or use a wildcard.\n\nTo view files we do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncat data/seq_project/*/*R1.fastq.gz &gt; combined_R1.fastq.gz\n\n#question 2:\nls -lh data/seq_project/*/*R1.fastq.gz\nls -lh combined_R1.fastq.gz\n\nIf you compare the two numbers generated for question 2, you should see that the combined.fastq.gz file is roughly the sum of the individual files.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-compressed-fastq-files",
    "href": "source/bash_intro.html#exploring-the-content-of-compressed-fastq-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Exercise\n\n\n\n\n\n\nView the first few rows of any one of the fastq.gz sequence files\nDoes this look like a normal sequence file to you?\n\n\n\nClick me to see an answer\n\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, so what is going on?\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloaded folder, you have seen that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. gzip is a tool used to (de)-compress the size of files.\nIn order to view the content of compressed files, we sometimes need to de-compress them first. We can do this using the gzip command together with the decompress -d option:\n\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows of information for each sequence as shown below:\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running the gzip command above, make a list of each sequence file with ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project/*/*\n\n#question 2:\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with sequencing files the data is usually much larger and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nls data/seq_project/*/*\n\n\n\nWe have seen that to view the content of a compressed file and make sense of the content we had to first decompress the file. However, sequence files tend to get rather large and we might not want to decompress our files to not clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen called zcat. When running zcat on a file then the original compressed file is kept as is. We combine the zcat command with the head command, since we do not want to print millions of sequences to the screen but only want to explore the first few rows:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head\n\n\n\n\n\n\n\nZcat for Mac users\n\n\n\n\n\nFor Mac users using the zsh shell, zcat might not work as expected. Try using gzcat instead:\n\ngzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#grep-find-patterns-in-data",
    "href": "source/bash_intro.html#grep-find-patterns-in-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The grep command is used to search for words, or patterns, in any text file. This command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c to count how often the pattern we search for occurs.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also use grep with wildcards, for example, if we want to look only at samples from User1 we could do the following:\n\ngrep \"User1_*\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing the pattern “ITS_9” in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files (advanced)\nCount how often “AAGACG” occurs in any in any of the fastq.gz files (advanced)\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if sequencing adapters or primers are still part of your sequence.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wc-count-things-ii",
    "href": "source/bash_intro.html#wc-count-things-ii",
    "title": "Introduction to Bash",
    "section": "",
    "text": "We have seen before that wc is a command that allows us to count the number of lines in a file and we can easily use it on fastq files to get an idea about how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We need to divide the number we see on the screen by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: Better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $(( $(wc -l) /4))\n\nIn the command above we have some new syntax:\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( $(wc -l) /4))\n$(...) indicates a command substitution. It allows the output of the enclosed command (wc -l) to be used as part of another command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”.\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line. In this case, it calculates the result of dividing the number of lines in the fastq file by 4.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#for-loops",
    "href": "source/bash_intro.html#for-loops",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Imagine we want to count the lines not only from one but all files. Could we do something like the code below?\n\nzcat data/seq_project/*/*.gz | wc -l\n\nWhen running this, we see that the command prints a single number, 869 944, but not the counts for each file, so something did not work right.\nThe problem with this command is that it prints the text from all 8 fastq files and only afterwards performs the counting. We basically concatenated all files and then calculated the sum of all 8 files. However, what we want to do is to repeat the same operation over and over again:\n\nDecompress a first file\nCount the lines in the first file\nDecompress a second file\nCount the lines in the second file\n…\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nLet’s start with a simple example but before that let’s introduce a simple command echo. echo is used to print information, such as Hello to the terminal:\n\necho \"Welcome 1 time!\"\n\nWe can use for-loops to print something to the screen not only one, but two, three, four … times as follows:\n\nfor i in 1 2 3; do \n    echo \"Welcome $i times\"\ndone\n\nAn alternative and more condensed way of writing this that you might encounter in the wild is the following:\n\nfor i in 1 2 3; do echo \"Welcome $i times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nLet’s try to do the same but now for our sequencing files by storing the individual files found in data/seq_project/*/*.gz in the variable i and print i to the screen in a for-loop.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"I work with: File $i\"\ndone\n\n\nfor i in data/seq_project/*/*.gz; do: This part initializes a loop that iterates over all files matching the pattern data/seq_project/*/*.gz. The variable i is assigned to each file in succession.\n\nWe can then use these variables stored in i to perform more useful operations, for example for each file, step-by-step, count the number of lines in each file by using some of the tools we have seen before:\n\nfor i in data/seq_project/*/*.gz; do \n    zcat $i | wc -l\ndone\n\n\nzcat $i | wc -l: This is the action performed inside the loop. zcat is used to concatenate and display the content of compressed files (*.gz). The | (pipe) symbol redirects this output to wc -l, which counts the number of lines in the uncompressed content.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a for loop and count how often grep finds the pattern “TAAGA” in each individual fastq.gz file\n\n\n\nClick me to see an answer\n\n\nfor i in data/seq_project/*/*.gz; do \n    zcat $i | grep -c \"TAAGA\"\ndone\n\n\n\n\n\nSometimes it is useful to not store the full path in i, especially when we want to store the output of our loop in a new file. Luckily, we can use the list of files we stored in fastq_files.txt to rewrite this command a bit and make use of the fact that we can use cat to print something in a text file to the screen:\n\nfor i in `cat fastq_files.txt`; do  \n    zcat data/seq_project/*/$i | wc -l\ndone\n\n\nfor i in `cat fastq_files.txt`; do: This initiates a loop that iterates over each item in the file fastq_files.txt. The backticks ` are used to execute the cat command within the backticks to read the content of the text file and assign its output to the variable i.\nzcat data/seq_project/*/$i | wc -l: Like before, this line uncompresses (zcat) and counts (wc) the number of lines in the specified file. However, in this case, the file is determined by the content of fastq_files.txt, which contains a list of file names.\n\nAfter this modification of the code, we can very easily store the output in a file instead of printing the results to the screen.\n\n#Count the number of lines in each sequencing file\n#Store the output in a new folder\nmkdir counts \n\nfor i in `cat fastq_files.txt`; do \n    zcat data/seq_project/*/$i | wc -l &gt; counts/$i.txt\ndone\n\nls -l counts/*txt \n\nhead counts/Sample-DUMMY1_R1.fastq.gz.txt\n\n\n\n\n\n\n\nAvanced Tip: Better counting in for loops\n\n\n\n\n\nLet’s get a bit more advanced to show you some powerful features of bash. For this imagine that you would do this for 100 files. In this case it would be useful to see the file names next to the counts. We can achieve this by using what we have learned in the Better counting tip where we have learned about echo and command substitution.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(zcat $i | wc -l)\"\ndone\n\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on here \"$i: $(zcat $i | wc -l)\"\n$(...): These parentheses are used for command substitution. It means that the command zcat $i | wc -l is executed, and its output (the line count of the uncompressed content) is substituted in that position.\nThe double quotes (““) perform what is called a string concatenation. It concatenates the filename ($i), a colon (:), a space, and the line count obtained from the command substitution. The entire string is then passed as a single argument to the echo command.\n\nAlmost perfect, now we only want to divide this by 4:\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(( $(zcat $i | wc -l) /4 ))\"\ndone\n\nNotice, how we use both single brackets and double brackets?\n\n$((...)) in contrast to $(...) is an arithmetic expansion in Bash. It allows you to perform arithmetic operations and substitute the result into the command line.\n\n\n\n\n\n\n\n\n\n\nAvanced Tip: Using bash to make sample mapping files\n\n\n\n\n\nFor a lot of mapping files for amplicon analyses, you need to prepare a table that lists your files, often with the R1 and R2 files in separate columns. How could we do this using bash?\nThis code is more advanced and uses a few concepts we have not talked about but should give you an idea about the different data transformations you can do with bash.\n\nfor file in data/seq_project/*/*.gz; do\n    if [[ $file == *_R1.fastq.gz ]]; then\n        r1_file=$file\n        r2_file=$(echo $file | sed 's/_R1/_R2/')\n        echo \"$r1_file,$r2_file\"\n    fi\ndone\n\n\nfor file in data/seq_project/*/*.gz: We loop through all fastq.gz files\nif [[ $file == *_R1.fastq.gz ]]; then: This line checks if the current file’s name matches the pattern *_R1.fastq.gz. The [[ ... ]] is a conditional construct in bash, and the == is used for string comparison. If the condition is true (i.e., the file is an R1 file), then the code inside the if block will be executed. The block starts with ifand finishes at the fi.\nr1_file=$file: This line involves creating a variable named r1_file and assigning it the value of the current file ($file). A variable is a symbolic name or identifier associated with a value or data. It allows you to store and retrieve information for later use. Here, r1_file is used to store the name of the current R1 file.\nr2_file=$(echo $file | sed 's/_R1/_R2/'): This line sets the variable r2_file by replacing _R1 with _R2 in the current file name. This is done using the sed command, which is a stream editor for filtering and transforming text.\necho \"$r1_file,$r2_file\": This line prints the R1 and R2 file names separated by a comma. This will be part of the output in the format R1,R2.\n\nWe can do even more and add a column with the sample name with what we have learned before:\n\nfor file in data/seq_project/*/*.gz; do\n    if [[ $file == *_R1.fastq.gz ]]; then\n        sample_name=$(echo $file | cut -f3 -d \"/\" )\n        r1_file=$file\n        r2_file=$(echo $file | sed 's/_R1/_R2/')\n        echo \"$sample_name,$r1_file,$r2_file\"\n    fi\ndone\n\nYou can easily combine this with some of the things we learned, such as using cut to extract specific parts of the sample_name as well. I.e. lets assume you only want to list the index:\n\nfor file in data/seq_project/*/*.gz; do\n    if [[ $file == *_R1.fastq.gz ]]; then\n        sample_name=$(echo $file | cut -f3 -d \"/\"  |cut -f1 -d \"_\" ) \n        r1_file=$file\n        r2_file=$(echo $file | sed 's/_R1/_R2/')\n        echo \"$sample_name,$r1_file,$r2_file\"\n    fi\ndone",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/example_doc.html",
    "href": "source/example_doc.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you want to see an example for documented code, check out a notebook I wrote based on what we are doing in this tutorial.\nThe link above leads you to an example for:\n\nHow could I use github to make my code available to others\nHow a “code book” could lool like. The example you see in the folder is provided as a Quarto markdown file here and for convenience the report was also rendered as a html to make it easier to read for potential collaborators. To view the HTML report, you need to download it first.\n\nThe example is based on the data you analyse throughout this tutorial, so the actual code might only make sense after you have finished the tutorial. Feel free to revisit this page after you have written some code yourself.\nPlease note that this is just an example to get you started and such a report might look different depending on your needs.\nBelow you find some general hints for what to put into different sections that you can see in the qmd file.\n\n\n\n\n\n\n\nIn one of the first sections of my notebools I add everything that a user that wants to use my workflow has to change and I try to standardize the code below, i.e. I try to write it in a way so that it does not need to be changed. This ensures that another user could just run my code as is without having to edit anything other than this first section. Typically things to add are:\n\nFrom where to start the analyses, i.e. the location of the working directory or project folder\nCustom paths, to for example for databases that need to be downloaded from the web or repositories\nCustom variables: In the example above I store the link to the data in a variable called download_link, I then use the variable in the code below to download the data. By doing it this way I have one location in the code another person needs to change the code when for example the path to the data changes. The code below stays the same since I use the download_link variable and not the actual path any more. When writing code try to think ahead and minimize the number of instances where things need to be changed if for example the location to your data changes.\n\nI tend to NOT add the information on how I use scp to log into an HPC to keep my user name and login information private.\n\n\n\n\nWhen I first start working with new data, I try to add as many sanity checks as possible to ensure that my data looks good. That way I avoid that I don’t notice an issue and run into trouble further down the line. I at the same time understand my data more and learn with how many samples and how much data I am working with.\nI also add such sanity checks whenever I modify my data. For example, when I merge individual files into a large file I might count the lines for the individual files and the combined file simply to ensure that I used my wildcards correctly.\nRemember: The computer is only as smart as the person using it and will blindly run your commands. Because of that the computer can do unexpected things and you need to account for that.\n\n\n\n\nIn the example above I use markdown to not only document my code but also add some comments whenever I might need it. In the example above, I added some notes about what the results from FastQC actually told me.\nThis kind of documentation can be useful for:\n\nJustifying decisions further down the line. In this example, I might decide on how to clean the sequencing data. For example, if I would have found a lot of low quality reads or the adapter still being part of the sequence then I would have specifically cleaned my sequences to deal with that\nFuture you. If you read the report a month, or a year, later you have the key information in your report and don’t have to open any files or tables that are located elsewhere\n\n\n\n\n\nThis part might make more sense after you have worked through the part of the tutorial about using an HPC. But what you see here is how I have written down code that simply says that I submitted a script to a HPC but it does not actually say how I ran the FastQC software. The actual code is “hidden” inside of the run_fastqc.sh script. This also means that a person reading your workflow does not have the code right away. You can deal with this in two ways.\n\nInstead of the sbatch command, you can add the actual line of code that was run on the compute node.\nWhen publishing your code with your manuscript, add the whole scripts folder to where you publish the main code, i.e. on github or zenodo\n\nI tend to prefer number 2 because I like to record the code in my notebooks exactly as I ran it but you can do it differently as long as all the code you ran is recorded and accessible to others once you publish your data.",
    "crumbs": [
      "An example notebook"
    ]
  },
  {
    "objectID": "source/example_doc.html#an-example-notebook",
    "href": "source/example_doc.html#an-example-notebook",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you want to see an example for documented code, check out a notebook I wrote based on what we are doing in this tutorial.\nThe link above leads you to an example for:\n\nHow could I use github to make my code available to others\nHow a “code book” could lool like. The example you see in the folder is provided as a Quarto markdown file here and for convenience the report was also rendered as a html to make it easier to read for potential collaborators. To view the HTML report, you need to download it first.\n\nThe example is based on the data you analyse throughout this tutorial, so the actual code might only make sense after you have finished the tutorial. Feel free to revisit this page after you have written some code yourself.\nPlease note that this is just an example to get you started and such a report might look different depending on your needs.\nBelow you find some general hints for what to put into different sections that you can see in the qmd file.\n\n\n\n\n\n\n\nIn one of the first sections of my notebools I add everything that a user that wants to use my workflow has to change and I try to standardize the code below, i.e. I try to write it in a way so that it does not need to be changed. This ensures that another user could just run my code as is without having to edit anything other than this first section. Typically things to add are:\n\nFrom where to start the analyses, i.e. the location of the working directory or project folder\nCustom paths, to for example for databases that need to be downloaded from the web or repositories\nCustom variables: In the example above I store the link to the data in a variable called download_link, I then use the variable in the code below to download the data. By doing it this way I have one location in the code another person needs to change the code when for example the path to the data changes. The code below stays the same since I use the download_link variable and not the actual path any more. When writing code try to think ahead and minimize the number of instances where things need to be changed if for example the location to your data changes.\n\nI tend to NOT add the information on how I use scp to log into an HPC to keep my user name and login information private.\n\n\n\n\nWhen I first start working with new data, I try to add as many sanity checks as possible to ensure that my data looks good. That way I avoid that I don’t notice an issue and run into trouble further down the line. I at the same time understand my data more and learn with how many samples and how much data I am working with.\nI also add such sanity checks whenever I modify my data. For example, when I merge individual files into a large file I might count the lines for the individual files and the combined file simply to ensure that I used my wildcards correctly.\nRemember: The computer is only as smart as the person using it and will blindly run your commands. Because of that the computer can do unexpected things and you need to account for that.\n\n\n\n\nIn the example above I use markdown to not only document my code but also add some comments whenever I might need it. In the example above, I added some notes about what the results from FastQC actually told me.\nThis kind of documentation can be useful for:\n\nJustifying decisions further down the line. In this example, I might decide on how to clean the sequencing data. For example, if I would have found a lot of low quality reads or the adapter still being part of the sequence then I would have specifically cleaned my sequences to deal with that\nFuture you. If you read the report a month, or a year, later you have the key information in your report and don’t have to open any files or tables that are located elsewhere\n\n\n\n\n\nThis part might make more sense after you have worked through the part of the tutorial about using an HPC. But what you see here is how I have written down code that simply says that I submitted a script to a HPC but it does not actually say how I ran the FastQC software. The actual code is “hidden” inside of the run_fastqc.sh script. This also means that a person reading your workflow does not have the code right away. You can deal with this in two ways.\n\nInstead of the sbatch command, you can add the actual line of code that was run on the compute node.\nWhen publishing your code with your manuscript, add the whole scripts folder to where you publish the main code, i.e. on github or zenodo\n\nI tend to prefer number 2 because I like to record the code in my notebooks exactly as I ran it but you can do it differently as long as all the code you ran is recorded and accessible to others once you publish your data.",
    "crumbs": [
      "An example notebook"
    ]
  },
  {
    "objectID": "source/hpc_intro.html",
    "href": "source/hpc_intro.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, you can log into an HPC onto a login node. The purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files as well as compiling and preparing a job script). You then submit a job script from the head to the compute nodes via Slurm. The compute nodes are used to actually run a program and Slurm is an open-source workload manager/scheduler that is used on many big HPCs. Slurm has three key functions:\n\nprovide the users access to the resources on the compute nodes for a certain amount of time to perform any computation\nprovide a framework to start, execute, and check the work on the set of allocated compute nodes\nmange the queue of submitted jobs based on the availability of resources\n\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind that you are sharing the system with other users. Some rules of thumb:\n\nDo NOT run jobs that request many CPUs and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDo NOT allocate more than 20% (CPU or memory) of the cluster for more than a day. If you have large jobs, contact Wim de Leeuw\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nAre granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nIn your home directory, /home/$USER , you have 25 G quotum\nIn your personal directory, /zfs/omics/personal/$USER , you can store up to 500 GB data\nFor larger, collaborative projects you can contact the Crunchomics team and ask for a shared folder to which several team members can have access\nYou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nA manual with more information and documentation about the cluster can be found here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal. This also means that even if you remove files to make space, these files will still count towards your quota for two weeks\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM.",
    "crumbs": [
      "Using an HPC",
      "HPC introduction"
    ]
  },
  {
    "objectID": "source/hpc_intro.html#hpc-introduction",
    "href": "source/hpc_intro.html#hpc-introduction",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, you can log into an HPC onto a login node. The purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files as well as compiling and preparing a job script). You then submit a job script from the head to the compute nodes via Slurm. The compute nodes are used to actually run a program and Slurm is an open-source workload manager/scheduler that is used on many big HPCs. Slurm has three key functions:\n\nprovide the users access to the resources on the compute nodes for a certain amount of time to perform any computation\nprovide a framework to start, execute, and check the work on the set of allocated compute nodes\nmange the queue of submitted jobs based on the availability of resources\n\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind that you are sharing the system with other users. Some rules of thumb:\n\nDo NOT run jobs that request many CPUs and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDo NOT allocate more than 20% (CPU or memory) of the cluster for more than a day. If you have large jobs, contact Wim de Leeuw\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nAre granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nIn your home directory, /home/$USER , you have 25 G quotum\nIn your personal directory, /zfs/omics/personal/$USER , you can store up to 500 GB data\nFor larger, collaborative projects you can contact the Crunchomics team and ask for a shared folder to which several team members can have access\nYou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nA manual with more information and documentation about the cluster can be found here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal. This also means that even if you remove files to make space, these files will still count towards your quota for two weeks\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM.",
    "crumbs": [
      "Using an HPC",
      "HPC introduction"
    ]
  }
]