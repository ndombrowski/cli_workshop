[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will provide guidance in how to use bash and the command line interface.\nThe first few pages will introduce you how to set up a terminal and document your code.\nAfterwards, the actual tutorial will begin, in which we will assume the role of a researcher who just received amplicon sequencing data from a sequencing center and wants to starts analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, we will learn how to organize and how to view and exploring the data on the filesystem. We will start with some basic operations to assess with how much data we work with and extract information from our files.\nAfter becoming familiar with the command line, we will learn how to analyze our data on an HPC (since often analysing large datasets on a personal computer can become quite resource intensive). We will learn how to connect to an HPC, download some software to assess the quality of our sequencing data and submit jobs to computational noted. If you do not have access to an HPC you can follow most parts on your own computer as the dataset we provide is a small one."
  },
  {
    "objectID": "index.html#welcome-page",
    "href": "index.html#welcome-page",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will provide guidance in how to use bash and the command line interface.\nThe first few pages will introduce you how to set up a terminal and document your code.\nAfterwards, the actual tutorial will begin, in which we will assume the role of a researcher who just received amplicon sequencing data from a sequencing center and wants to starts analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, we will learn how to organize and how to view and exploring the data on the filesystem. We will start with some basic operations to assess with how much data we work with and extract information from our files.\nAfter becoming familiar with the command line, we will learn how to analyze our data on an HPC (since often analysing large datasets on a personal computer can become quite resource intensive). We will learn how to connect to an HPC, download some software to assess the quality of our sequencing data and submit jobs to computational noted. If you do not have access to an HPC you can follow most parts on your own computer as the dataset we provide is a small one."
  },
  {
    "objectID": "source/installation.html",
    "href": "source/installation.html",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow a user to interact with an operating system. The key difference between the CLI and GUI is that the interaction with CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. CLI is often also referred to as the shell, terminal, console, prompt or various other names\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows.\n\n\n\n\n\nThe default shell is usually Bash and there is usually no need to install anything to be able to follow this tutorial. On most versions of Linux, th shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you can run it by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ computer search function. Search for: Terminal and press Return.\n\nTo ensure that we work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again.\n\n\n\nOperating systems like macOS and Linux come with a native command-line terminal, making it straightforward to run bash commands. However, for Windows users you need to install some software first to be able to use bash, below you find three options:\nOne option to access the bash shell commands is using Git Bash, for detailed installation instructions please have a look at the carpenties website.\nA second option is Mobaxterm, which enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here.\nA final option is to use Windows and Linux at the same time on a Windows machine. The Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option allows you to use all the tools available but since you more or less are installing a separating system on your PC needs to have enough memory to run this. Installation instructions can be found here.\n\n\n\n\n\n\nNote\n\n\n\nI am myself mostly familiar with WSL and the following tutorial is tailored towards the location of things when using WSL and Linux and your folder structure might be slightly different when using Git Bash or Mobaxterm.\nSimilarly, I am mainly familiar with the bash not the zsh shell. For Mac users that have a newer MAC and have trouble switching to bash this might create some issues when using wildcards.\nFor both issues: If parts of the tutorial do not work for you due to that, feel free to contact me and I can adjust the tutorial accordingly.\n\n\n\n\n\n\nAfter you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:"
  },
  {
    "objectID": "source/installation.html#terminology",
    "href": "source/installation.html#terminology",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow a user to interact with an operating system. The key difference between the CLI and GUI is that the interaction with CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. CLI is often also referred to as the shell, terminal, console, prompt or various other names\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows."
  },
  {
    "objectID": "source/installation.html#installation-guides",
    "href": "source/installation.html#installation-guides",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The default shell is usually Bash and there is usually no need to install anything to be able to follow this tutorial. On most versions of Linux, th shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you can run it by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ computer search function. Search for: Terminal and press Return.\n\nTo ensure that we work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again.\n\n\n\nOperating systems like macOS and Linux come with a native command-line terminal, making it straightforward to run bash commands. However, for Windows users you need to install some software first to be able to use bash, below you find three options:\nOne option to access the bash shell commands is using Git Bash, for detailed installation instructions please have a look at the carpenties website.\nA second option is Mobaxterm, which enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here.\nA final option is to use Windows and Linux at the same time on a Windows machine. The Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option allows you to use all the tools available but since you more or less are installing a separating system on your PC needs to have enough memory to run this. Installation instructions can be found here.\n\n\n\n\n\n\nNote\n\n\n\nI am myself mostly familiar with WSL and the following tutorial is tailored towards the location of things when using WSL and Linux and your folder structure might be slightly different when using Git Bash or Mobaxterm.\nSimilarly, I am mainly familiar with the bash not the zsh shell. For Mac users that have a newer MAC and have trouble switching to bash this might create some issues when using wildcards.\nFor both issues: If parts of the tutorial do not work for you due to that, feel free to contact me and I can adjust the tutorial accordingly."
  },
  {
    "objectID": "source/installation.html#sanity-check",
    "href": "source/installation.html#sanity-check",
    "title": "Setting up a terminal",
    "section": "",
    "text": "After you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "tba"
  },
  {
    "objectID": "readme.html#title",
    "href": "readme.html#title",
    "title": "Introduction to the cli",
    "section": "",
    "text": "tba"
  },
  {
    "objectID": "source/code_documentation.html",
    "href": "source/code_documentation.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might work with your code. Documentation serves as a roadmap for your code. It helps others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R code but most are general enough to apply to other computational langauges (and scientific disciplines).\nIf you want to see an example for documented code, check out [this example file]tba. The example file is a markdown file (qmd) generated with VSCode. Some examples for text editors to use can be found below.\n\n\n\n\n\n\nNote\n\n\n\nThe information in this section is not part of the actual tutorial but was added to give you a starting point for how to document your code.\nIf you follow the in-person tutorial it is best to record your notes using a plain text editor but feel free to explore the more advanced options after the tutorial.\n\n\n\n\n\n\n\nWhen documenting code, its best to avoid visual editors, such as word, as they are not designed for writing code and easily destroy the formating by for example changing ` to ’, which when writing code is quite a big difference.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as adding headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown that allows you to integrate R code directly into your documentation.\nIf you have not install R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nKnit the Document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio but can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different compuational languages.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.\n\n\n\n\n\n\nMarkdown is a lightweight markup language that’s easy to read and write. It allows you to add formatting elements to plain text documents.\nHeaders:\nUse # for headers. The more # symbols, the smaller the header. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers don’t have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nCreate links to external resources or within your documentation.\n[Link Text](https://www.example.com)\nEmphasis:\nUse * or _ for italic and ** or __ for bold.\n*italic*\n**bold**\nPictures\nYou can easily add images to your documentation as well:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/ifrom spamage.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|"
  },
  {
    "objectID": "source/code_documentation.html#documenting-code",
    "href": "source/code_documentation.html#documenting-code",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might work with your code. Documentation serves as a roadmap for your code. It helps others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R code but most are general enough to apply to other computational langauges (and scientific disciplines).\nIf you want to see an example for documented code, check out [this example file]tba. The example file is a markdown file (qmd) generated with VSCode. Some examples for text editors to use can be found below.\n\n\n\n\n\n\nNote\n\n\n\nThe information in this section is not part of the actual tutorial but was added to give you a starting point for how to document your code.\nIf you follow the in-person tutorial it is best to record your notes using a plain text editor but feel free to explore the more advanced options after the tutorial."
  },
  {
    "objectID": "source/code_documentation.html#choose-your-editor",
    "href": "source/code_documentation.html#choose-your-editor",
    "title": "Introduction to the cli",
    "section": "",
    "text": "When documenting code, its best to avoid visual editors, such as word, as they are not designed for writing code and easily destroy the formating by for example changing ` to ’, which when writing code is quite a big difference.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as adding headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown that allows you to integrate R code directly into your documentation.\nIf you have not install R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nKnit the Document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio but can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different compuational languages.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features."
  },
  {
    "objectID": "source/code_documentation.html#markdown-for-documentation",
    "href": "source/code_documentation.html#markdown-for-documentation",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Markdown is a lightweight markup language that’s easy to read and write. It allows you to add formatting elements to plain text documents.\nHeaders:\nUse # for headers. The more # symbols, the smaller the header. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers don’t have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nCreate links to external resources or within your documentation.\n[Link Text](https://www.example.com)\nEmphasis:\nUse * or _ for italic and ** or __ for bold.\n*italic*\n**bold**\nPictures\nYou can easily add images to your documentation as well:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/ifrom spamage.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|"
  },
  {
    "objectID": "source/bash_intro.html",
    "href": "source/bash_intro.html",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing and starting the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\n\npwd\n\npwd prints the location of the current working directory and tells you where exactly you are in the file system. When we login we typically start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but the path might be slightly different depending on your operating system. Below you find some help to orient yourself better for different terminal interfaces:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder, which gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn there set Persistent home directory to a folder of your choice\n\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\n\n\n\n\n\n\nNow that we know where we are, let’s find out what files and folders exist in our home directory. For this we can use the ls command, which allows us to list directory contents:\n\nls\n\nIn my case this returns something like this:\n\n\n\nThe colors will look different depending on the cli use but in my case I see a list of files (in bold text) and folders (green-highlighted text) in my home directory.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands.\n\n\n\nLet´s start with looking at the general structure of a command, which generally consists of three elements, the command itself and some optional options and arguments:\n\n\n\nUnderstanding better what a command looks like, let’s use the ls command together with the option -l. This option results in ls printing the content of a folder in a long listing format.\n\nls -l\n\nAfter running this, we should see our files and folders again but in a long format, which gives more detailed information and structures our output a bit better:\n\n\n\n\n\n\nIf you want to know what options are available for a command it is always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways to access the manual. These most common ways are:\n\nman ls\nls --help\nls -h\n\n\n\n\nMost of the time you do not want to perform your analyses in the home directory but in a dedicated folder for your project. To get started, we will learn about the cd command that allows us to move around the file system.\nThe file system is a hierarchical system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use a relative path.\nTo start moving around, let’s begin by moving relative to our working directory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory:\n\ncd source/\n\nIf you use pwd after moving around directories, you should see that a different location is printed to the screen.\nWe can also move back to our original directory using cd .., which will move us back one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, ~, which can be used as a shortcut to move directly into our home directory from wherever you are on the file system:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd.\n\n\n\n\n\n\nNow that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and then create and move into a new folder called data_analysis as follows:\n\n#go into the home directory\ncd ~\n\n#in the home directory make a new folder and name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated correctly\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: commenting your code\n\n\n\n\n\nNotice, how in the example below I added the commands to run as well as some explanation about what I did?\nHere, I used a specific character # in front of a line of text to denote the beginning of a single-line comment. Anything coming after the character is considered a commend and won’t be executed by the shell.\nIn the above example I definitely commented the code too much as my comments basically duplicate the code and that should be avoided, however, it is useful to add comments in code to ensure that the purpose of the code is clear to others.\nYou could, for example, add a comment above functions to describe what the function does and you can also add comments to “tricky” code where it is not immediately obvious what you are trying to do. Basically, you want to add comments to any section where you think that the future you might get confused a month later.\nHere you find some examples for python and R but the same logic applies when writing code for bash, some examples for that can be found here.\n\n\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name.\n\n\n\n\n\n\nNext, let’s download our sequencing data into a new data folder. One way to do this is using wget. In the command below we add the option -P to specify into which folder we want to download the data. You an also see how I documented my code here in order to add some more specifics about where the link came from to help future me in case I, for example, need to look for some e-mails about the data later on.\n\nmkdir data\n\n#download data using link provided by the sequencing center on 23.01.2023\nwget -P data https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/\n\n#question 2:\nls -lh data/seq_project.tar.gz\n\n\n\n\n\n\n\n\nThe data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is commonly used when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use with the tar command are:\n\nx tells it to extract files from the archive\nv display verbose information and provide detailed information while creating the tarball\nf specify the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content of the folder we just extracted. Ensure that you explore the content of potential sub-directories.\nFind the path for at least one sequence file that ends on fastq.gz. Hint, to make your life easier, check the Tip: Command-line completion above to not have to type every single word.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project\n\n#question 2:\nls -l data/seq_project/barcode001_User1_ITS_1_L001/\n\n\n\n\n\n\n\n\nTo remove files and folders, we use the rm command. We want to remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!\n\n\n\n\n\nAfter we have explored the folder with our sequencing data with ls, we have seen that data/seq_project contains 4 folders, 2 folders for two different users who each generated two replicates.\nYou also might have also noticed that it gets tedious to figure out how many files are in each folder because we would need to run ls on each single folder and view its content individually.\nLuckily, the shell provides special characters to rapidly specify groups of filenames or patterns. Wild-cards are characters that can be used as a substitute for any class of characters in a search.\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. It allows us to list all files in the seq_project folder as follows:\n\nls data/seq_project/*\n\nYou should have seen before that the sequencing files all end with .fastq.gz we can make this command a bit more specific and at the same time make the output a bit more readable:\n\nls data/seq_project/*/*.gz\n\nIf we wanted to print the absolute and not the relative path, we could do the following (beware that the absolute path will be slightly different depending in you system):\n\nls /home/User_name/data/seq_project/*/*.gz\n\nNow we can easily see for each folder how many files we have.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a wildcard to list files for barcode001 only\nUse a wildcard to list files only for user1\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls data/seq_project/barcode001*/*gz\n\n#question 2:\nls data/seq_project/*_User1_*/*gz\n\n\n\n\n\n\n\n\n\n\n\nTip: More wildcards\n\n\n\n\n\n* is not the only wildcard we can use and a full list can be found here.\nDifferent wildcards can be useful in different contexts, not only only when listing files but also finding patterns in files. To make such searches as specific as possible there are different wildcards available. A few examples are:\nThe [0-9] wildcard = matches any number exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[0-9]*/*.gz\n\nThe [012] wildcard = matches 1 or 2 exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[12]*/*.gz\n\n[A-Z] matches any letter in capitals occurring once\n[a-z]* matches any letter in non-capital letters occurring many times\n\nls data/seq_project/barcode001_User1_ITS_1_[A-Z]001/*.gz\n\n\n\n\n\n\n\nWe have seen by now that by default our commands direct print the standard output to the terminal. For example, when we use ls the list of files and folders is printed. However, we can also redirect the standard output to a file by using the &gt; character.\nFor example, we might want to generate a list with all fastq files and can do this as follows:\n\nls data/seq_project/*/* &gt; fastq_paths.txt\n\n\n\n\nNext, let’s view the content of the list we just generated. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nhead can be used to check the first 10 rows of our files:\n\nhead fastq_paths.txt\n\n\n\n\nIf you want to check the last 10 rows use tail:\n\ntail fastq_paths.txt\n\n\n\n\nless is a program that let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S fastq_paths.txt\n\nOnce opened, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n\n\n\n\nTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano fastq_paths.txt\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter\n\n\n\n\n\n\n\n\nAnother useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an useful tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files.\nWe could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nOne down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command.\n\n\n\nPipes are a powerful utility to connect multiple commands together. Pipes allow us to feed the standard output of one command, such as ls as input into another command such as wc -l and as such combine multiple commands together.\nTherefore, lets use ls to first list all fastq files and then pipe the output from ls into the wc command in order to count with how many files we work with:\n\nls data/seq_project/*/* | wc -l\n\nWe should see again that we work with 8 files, but now we did not have to generate an intermediate text file and have a more condensed command.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse ls and wc to count how many R1 files we have\nCount how many R2 files we have\nCount how many files were generated for User1?\n\n\n\nClick me to see an answer\n\n\n#question 1: We see 4 files\nls data/seq_project/*/*R1*/*gz | wc -l \n\n#question 2: We see 4 files\nls data/seq_project/*/*R2*/*gz | wc -l \n\n#question 3: We see 4 files\nls data/seq_project/*_User1_*/*gz | wc -l \n\nChecking whether we have the same number of R1 and R2 files is a good sanity check, to ensure that we have received all the files from the sequencing center whenever we generate paired sequencing data.\n\n\n\n\n\n\n\nRemember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\nhead fastq_paths.txt\n\n#extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#do the same as above but this time save the output in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as a list with all the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:\n\n\n\n\nsort: sort lines in a file from A-Z and is useful for file organization.\nuniq: remove or find duplicates . For this command to work you need to provide it with a sorted file\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse cut to print the folder names that lead to the fastq.gz files\nPrint the folder names as in 1 but then also extract the User names\nEnsure that when running the code from 2 that we print a unique list of User names\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncut -f3 -d \"/\" fastq_paths.txt\n\n#question 2:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#question 3:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\n\ncat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files\n\nTo view files we do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncat data/seq_project/*/*R1.fastq.gz &gt; combined.fastq.gz\n\n#question 2:\nls -lh data/seq_project/*/*R1.fastq.gz\n\n#question 3: \nls -lh combined.fastq.gz\n\nIf you compare the numbers from part 2 and 3, you should see that the combined.fastq.gz file is roughly the sum of the individual files.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nView the first few rows of any one of the fastq.gz sequence files\nDoes this look like a normal sequence file to you?\n\n\n\nClick me to see an answer\n\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, so what is going on?\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloads folder, you have seen that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to (de)-compress the size of files.\nIn order to view the content of such files, we sometimes need to de-compress them first. We can do this using the gzip command together with the decompress -d option:\n\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows with information for each sequence:\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running the gzip command above, make a list of each sequence file with ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project/*/*\n\n#question 2:\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with sequencing files the data is usually much large and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nls data/seq_project/*/*\n\n\n\nWe have seen that to view the content of a compressed file and make sense of the content we had to first decompress the file. However, sequence files tend to get rather large and we might not want to decompress our files to not clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen called zcat, the original compressed file is kept while doing this. We combine this command with the head command, since do not want to print millions of sequences to the screen but only want to explore the first few rows:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head\n\n\n\n\n\n\nThe grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c for counting.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also combine this with wildcards, for example, if we look for all samples from User1 we could do the following:\n\ngrep \"User1_ITS_*_\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing the pattern “ITS_9” in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files\nCount how often “AAGACG” occurs in any in any of the fastq.gz files\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if if the adapters or primers are still part of your sequence.\n\n\n\n\n\n\n\nAnother useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an easy way to perform sanity checks and in our case allows us to count how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We need to divide the number we see on the screen by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $((`wc -l`/4))\n\nIn the command above we have some new syntax:\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( `wc -l` / 4))\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line.\nThe backticks (``) around wc -l indicate command substitution. This means that the wc -l command is executed, and its output (the number of lines counted) is used in the overall command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”. Command substitution allows you to use the actual result of the command.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences.\n\n\n\n\n\n\n\nImagine we want to count the lines not only from one but all files. Could we do something like the code below?\n\nzcat data/seq_project/*/*.gz | wc -l\n\nWhen running this, we see that the command prints a single number, 869 944, but not the counts for each file, so something did not work right.\nThe problem with this command is that it prints the text from all 8 fastq files and only afterwards performs the counting. We basically concatenated all files and then calculated the sum of all 8 files. However, what we want to do is to repeat the same operation over and over again:\n\nDecompress a first file\nCount the lines in the first file\nDecompress a second file\nCount the lines in the second file\n…\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nLet’s start with a simple example but before that let’s introduce a simple command echo. echo is used to print information, such as Hello to the terminal:\n\necho \"Welcome 1 time!\"\n\nWe can use for-loops to print something to the screen not only one, but two, three, four … times as follows:\n\nfor i in 1 2 3; do \n    echo \"Welcome ${i} times\"\ndone\n\nAn alternative and more condensed way of writing this that you might encounter in the wild is the following:\n\nfor i in 1 2 3; do echo \"Welcome ${i} times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nLet’s try to do the same but for our files by storing the individual files found in data/seq_project/*/*.gz in the variable i and print i to the screen in a for-loop.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"I work with: File ${i}\"\ndone\n\n\nfor i in data/seq_project/*/*.gz; do: This part initializes a loop that iterates over all files matching the pattern data/seq_project/*/*.gz. The variable i is assigned each file in succession.\n\nWe can then use these variables stored in i to perform more useful operations, for example for each file, step-by-step, count the number of lines in each file by using some of the tools we have seen before:\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | wc -l\ndone\n\n\nzcat ${i} | wc -l: This is the action performed inside the loop. zcat is used to concatenate and display the content of compressed files (*.gz). The | (pipe) symbol redirects this output to wc -l, which counts the number of lines in the uncompressed content.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a for loop and count how often grep finds the pattern “TAAGA”\n\n\n\nClick me to see an answer\n\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | grep -c \"TAAGA\"\ndone\n\n\n\n\n\nSometimes it is useful to not store the full path in i, especially when we want to store the output of our loop in a new file. Luckily, we can use the list of files we stored in fastq_files.txt to rewrite this command a bit and make use of the fact that we can use cat to print something in a text file to the screen:\n\nfor i in `cat fastq_files.txt`; do  \n    zcat data/seq_project/*/${i} | wc -l\ndone\n\n\nfor i in `cat fastq_files.txt`; do: This initiates a loop that iterates over each item in the file fastq_files.txt. The backticks ` are used to execute the cat command within the backticks to read the content of the text file and assign its output to the variable i.\nzcat data/seq_project/*/${i} | wc -l: Like before, this line uncompresses (zcat) and counts (wc) the number of lines in the specified file. However, in this case, the file is determined by the content of fastq_files.txt, which contains a list of file names.\n\nAfter this modification of the code, we can very easily store the output in a file instead of printing the results to the screen.\n\n#Count the number of lines in each sequencing file\n#Store the output in a new folder\nmkdir counts \n\nfor i in `cat fastq_files.txt`; do \n    zcat data/seq_project/*/$i | wc -l &gt; counts/${i}.txt\ndone\n\nls counts/*txt \n\nhead counts/Sample-DUMMY1_R1.fastq.gz.txt\n\n\n\n\n\n\n\nTip: Avanced: better counting in for loops\n\n\n\n\n\nLet’s get a bit more advanced to show you some powerful features of bash. For this imagine that you would do this for 100 files. In this case it would be useful to see the file names next to the counts. We can achieve this by using what we have learned in the Better counting tip where we have learned about echo and command substitution.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(zcat $i | wc -l)\"\ndone\n\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on here \"$i: $(zcat $i | wc -l)\"\n$(...): These parentheses are used for command substitution. It means that the command zcat $i | wc -l is executed, and its output (the line count of the uncompressed content) is substituted in that position.\nThe double quotes (““) perform what is called a string concatenation. It concatenates the filename ($i), a colon (:), a space, and the line count obtained from the command substitution. The entire string is then passed as a single argument to the echo command.\n\nAlmost perfect, now we only want to divide this by 4:\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(( $(zcat $i | wc -l) /4 ))\"\ndone\n\nNotice, how we use both single brackets and double brackets?\n\n$((...)) in contrast to $(...) is an arithmetic expansion in Bash. It allows you to perform arithmetic operations and substitute the result into the command line."
  },
  {
    "objectID": "source/bash_intro.html#introduction-to-bash",
    "href": "source/bash_intro.html#introduction-to-bash",
    "title": "Introduction to the cli",
    "section": "",
    "text": "After installing the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter. pwd prints the location of the current working directory and basically tells you where exactly you are.\nWhen we login we start from what is called our home directory.\n\npwd\n\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but might be slightly different depending on your operating system. Below you find some help to orient yourself better for different terminal interfaces:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in the Temp folder, which gets deleted evertime you exit Mobaxterm, To give this a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn there set Persistent home directory to a folder of your choice\n\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the file explorer and get used to where you are you can type explorer.exe .\nYou see that the Ubuntu environment is launched separated from your Windows environment\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\n\n\n\n\n\n\nNow that we know where we are, let’s see how to move around by first seeing what files and folders exist in our home directory. For this we can use the ls command, which stands for list directory contents:\n\nls\n\nIn my case this returns something like this:\n\n\n\nThis might look a bit different for your system in terms of color for file/folder names but what we basically see are the files (in bold text) and folders (green-highlighted text).\n\n\n\nSince this can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands.\nLet´s start with looking at the general structure of a command:\n\n\n\nNow, let’s look a bit closer into the ls command and use it with an option -l, an option we can use that makes ls use a long listing format.\n\nls -l\n\nAfter running this, we should see our files and folders but in what is called the long format (which gives more detailed information and structures our output a bit better):\n\n\n\nIf you are unsure what options come with a program its always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways how to do this. These most common ways are:\n\nman ls\nls --help\nls -h\n\n\n\n\nMost of the time you do not want to perform your analyses in the home directory but elsewhere. We can use the cd command to move around the file system.\nThe Unix file system is a hierarchical file system to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/) character. All other files are “descendants” of root. To move from the root, we can go via the descendants to reach the john folder as follows: /users/john\n\n\n\nFor our analyses, we will move into any of the folders that we see listed after we have used ls -l. In my case I want to move into the source directory:\n\ncd source/\n\nIf you use pwd afterwards, then you should see that we moved into another directory.\nWe can also move back to our original directory using cd .., which will move the user up one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nExercise\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tips in the section introducing pwd"
  },
  {
    "objectID": "source/bash_intro.html#pwd-finding-out-where-we-are",
    "href": "source/bash_intro.html#pwd-finding-out-where-we-are",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\npwd prints the location of the current working directory and basically tells you where exactly you are.\n\npwd\n\nWhen we login we start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but might be slightly different depending on your operating system. Below you find some help to orient yourself better for different terminal interfaces:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder, which gets deleted evertime you exit Mobaxterm, To give this a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn there set Persistent home directory to a folder of your choice\n\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the file explorer and get used to where you are you can type explorer.exe .\nYou see that the Ubuntu environment is launched separated from your Windows environment\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop"
  },
  {
    "objectID": "source/bash_intro.html#ls-list-the-content-of-a-directory",
    "href": "source/bash_intro.html#ls-list-the-content-of-a-directory",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know where we are, let’s see how to move around by first seeing what files and folders exist in our home directory. For this we can use the ls command, which stands for list directory contents:\n\nls\n\nIn my case this returns something like this:\n\n\n\nThis might look a bit different for your system in terms of color for file and folder names but what you see are all files (in bold text) and folders (green-highlighted text) in the directory you currently are in.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands."
  },
  {
    "objectID": "source/bash_intro.html#the-structure-of-a-command",
    "href": "source/bash_intro.html#the-structure-of-a-command",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Let´s start with looking at the general structure of a command, which generally consists of three elements, the command itself and some optional options and arguments:\n\n\n\nUnderstanding better what a command looks like, let’s use the ls command together with the option -l. This option results in ls printing the content of a folder in a long listing format.\n\nls -l\n\nAfter running this, we should see our files and folders again but in a long format, which gives more detailed information and structures our output a bit better:"
  },
  {
    "objectID": "source/bash_intro.html#cd-moving-around-folders",
    "href": "source/bash_intro.html#cd-moving-around-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Most of the time you do not want to perform your analyses in the home directory but elsewhere. We can use the cd command to move around the file system.\nThe Unix file system is a hierarchical file system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use a relative path.\nTo start moving around, let’s begin by moving relative to our working direcory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory and do this as follows:\n\ncd source/\n\nIf you use pwd afterwards, then you should see that we moved into another directory.\nWe can also move back to our original directory using cd .., which will move the user up one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, i.e. ~, which can be used as a shortcut to move directly into our home directory:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd."
  },
  {
    "objectID": "source/bash_intro.html#mkdir-making-new-folders",
    "href": "source/bash_intro.html#mkdir-making-new-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and create and move into a new folder called data_analysis as follows:\n\n#go into the folder from which you want to work (here we go to the home directory)\ncd ~\n\n#make a new folder in the directory we currently are in, name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name."
  },
  {
    "objectID": "source/bash_intro.html#wget-downloading-data",
    "href": "source/bash_intro.html#wget-downloading-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s download our sequencing data into a new data folder. One way to do this is using wget. In the command below we add the option -P to specify into which folder we want to download the data:\n\n#make a folder for our downloads\nmkdir data\n\n#download some data using wget\nwget -P data https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#check if file was downloaded\nls -l data/\n\n#check file size\nls -lh data/seq_project.tar.gz"
  },
  {
    "objectID": "source/bash_intro.html#tar-working-with-tar-files",
    "href": "source/bash_intro.html#tar-working-with-tar-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is common in Unix and Unix-like systems when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use are:\n\nx tells it to extract files from the archive\nv display verbose information and provide detailed information while creating the tarball\nf specify the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundeling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content we just downloaded\nFind the path to reach at least one sequence file. Hint, to make your life easier, check the Tip: Command-line completion above.\n\n\n\nClick me to see an answer\n\n\n#check the content of the dowloaded folder\nls -l data/seq_project\n\n#find a sequence file\nls -l data/seq_project/barcode001_User1_ITS_1_L001/"
  },
  {
    "objectID": "source/bash_intro.html#wildcards",
    "href": "source/bash_intro.html#wildcards",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After we have explored the folder with our sequencing data with ls, we have seen that data/seq_project contains 4 folders, 2 folders for two different users who each generated two replicates.\nYou also might have also noticed that it gets tedious to figure out how many files are in each folder because we would need to run ls on each single folder and view its content individually.\nLuckily, the shell provides special characters to rapidly specify groups of filenames or patterns. Wild-cards are characters that can be used as a substitute for any class of characters in a search.\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. It allows us to list all files in the seq_project folder as follows:\n\nls data/seq_project/*\n\nYou should have seen before that the sequencing files all end with .fastq.gz we can make this command a bit more specific and at the same time make the output a bit more readable:\n\nls data/seq_project/*/*.gz\n\nIf we wanted to print the absolute and not the relative path, we could do the following (beware that the absolute path will be slightly different depending in you system):\n\nls /home/User_name/data/seq_project/*/*.gz\n\nNow we can easily see for each folder how many files we have.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a wildcard to list files for barcode001 only\nUse a wildcard to list files only for user1\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls data/seq_project/barcode001*/*gz\n\n#question 2:\nls data/seq_project/*_User1_*/*gz\n\n\n\n\n\n\n\n\n\n\n\nTip: More wildcards\n\n\n\n\n\n* is not the only wildcard we can use and a full list can be found here.\nDifferent wildcards can be useful in different contexts, not only only when listing files but also finding patterns in files. To make such searches as specific as possible there are different wildcards available. A few examples are:\nThe [0-9] wildcard = matches any number exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[0-9]*/*.gz\n\nThe [012] wildcard = matches 1 or 2 exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[12]*/*.gz\n\n[A-Z] matches any letter in capitals occurring once\n[a-z]* matches any letter in non-capital letters occurring many times\n\nls data/seq_project/barcode001_User1_ITS_1_[A-Z]001/*.gz"
  },
  {
    "objectID": "source/bash_intro.html#gzip-decompressing-files",
    "href": "source/bash_intro.html#gzip-decompressing-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After downloading and exploring the content of the downloads folder, you see that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to (de)-compress the size of files.\nIn order to work with files, we sometimes need to de-compress them first. We can do this as follows (by using the decompress, -d argument):\n\n#decompress gz data (-d = decompress)\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running gzip, check if we now have one uncompressed file by using ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#check the content of the dowloaded folder\nls -l data/seq_project/*/*\n\n#compare the size of the different files\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with 16S amplicon sequencing files the data is usually much large and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep have our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\n#compress\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\n#check if that worked\nls data/seq_project/*/*"
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-files",
    "href": "source/bash_intro.html#exploring-the-content-of-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s view the content of our fastq files. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nOne very quick way to check the first 10 rows of any file is head. Let’s try this on one of our sequence files:\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, what is going on?\n\n\n\nTo view the content of a compressed file and make sense of the content we need to first uncompress the file. However, sequence files tend to get rather large and we do not want to clutter the files.\nLuckily, there is one useful tool in bash to uncompress the file and print the content to the screen called zcat,\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nWhen running this we see the content of the whole file printed to the screen. This is a bit better but imagine doing this for a file with millions of sequences. In such a case we would not want to print the whole file to the screen but maybe only the first view rows.\n\n\n\nPipes are a powerful utility to connect multiple commands together. They allow us to feed the standard output of one command, such as zcat as input into another command such as head and as such combine multiple commands together:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows with information for each sequence:\n\n\n\n::: {.callout-caution collapse=“false” title=“Exercise”} 1. View the content of a file ending on R2 (i.e. one of the reverse reads) 2. Do you see a way how you could distinguish forward from reverse reads?\n\n\nClick me to see an answer\n\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | head\n\nR1 and R2 files have a different header, i.e. R1: 1:N:0:TGCTCGTAGT R2: 2:N:0:TGCTCGTAGT\nHowever, except this small distinction the headers from the two files should have exactly the same header.\n\n\n\n\nless is a program that lets you view a file’s contents one screen at a time. This is useful when dealing with a large text file because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | less -S\n\nOnce opened, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q"
  },
  {
    "objectID": "source/bash_intro.html#wc-with-how-much-data-do-we-work",
    "href": "source/bash_intro.html#wc-with-how-much-data-do-we-work",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an easy way to perform sanity checks and in our case allows us to count how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We divide by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $((`wc -l`/4))\n\nIn the command above we have some new syntax echo $((wc -l/4))\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( `wc -l` / 4))\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line.\nThe backticks (``) around wc -l indicate command substitution. This means that the wc -l command is executed, and its output (the number of lines counted) is used in the overall command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”. Command substitution allows you to use the actual result of the command.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences."
  },
  {
    "objectID": "source/bash_intro.html#for-loops",
    "href": "source/bash_intro.html#for-loops",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Imagine we want to count the lines not only from one but all files. Could we do something like the code below?\n\nzcat data/seq_project/*/*.gz | wc -l\n\nWhen running this, we see that the command prints a single number, 869 944, but not the counts for each file, so something did not work right.\nThe problem with this command is that it prints the text from all 8 fastq files and only afterwards performs the counting. We basically concatenated all files and then calculated the sum of all 8 files. However, what we want to do is to repeat the same operation over and over again:\n\nDecompress a first file\nCount the lines in the first file\nDecompress a second file\nCount the lines in the second file\n…\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nLet’s start with a simple example but before that let’s introduce a simple command echo. echo is used to print information, such as Hello to the terminal:\n\necho \"Welcome 1 time!\"\n\nWe can use for-loops to print something to the screen not only one, but two, three, four … times as follows:\n\nfor i in 1 2 3; do \n    echo \"Welcome ${i} times\"\ndone\n\nAn alternative and more condensed way of writing this that you might encounter in the wild is the following:\n\nfor i in 1 2 3; do echo \"Welcome ${i} times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nLet’s try to do the same but for our files by storing the individual files found in data/seq_project/*/*.gz in the variable i and print i to the screen in a for-loop.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"I work with: File ${i}\"\ndone\n\n\nfor i in data/seq_project/*/*.gz; do: This part initializes a loop that iterates over all files matching the pattern data/seq_project/*/*.gz. The variable i is assigned each file in succession.\n\nWe can then use these variables stored in i to perform more useful operations, for example for each file, step-by-step, count the number of lines in each file by using some of the tools we have seen before:\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | wc -l\ndone\n\n\nzcat ${i} | wc -l: This is the action performed inside the loop. zcat is used to concatenate and display the content of compressed files (*.gz). The | (pipe) symbol redirects this output to wc -l, which counts the number of lines in the uncompressed content.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a for loop and count how often grep finds the pattern “TAAGA”\n\n\n\nClick me to see an answer\n\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | grep -c \"TAAGA\"\ndone\n\n\n\n\n\nSometimes it is useful to not store the full path in i, especially when we want to store the output of our loop in a new file. Luckily, we can use the list of files we stored in fastq_files.txt to rewrite this command a bit and make use of the fact that we can use cat to print something in a text file to the screen:\n\nfor i in `cat fastq_files.txt`; do  \n    zcat data/seq_project/*/${i} | wc -l\ndone\n\n\nfor i in `cat fastq_files.txt`; do: This initiates a loop that iterates over each item in the file fastq_files.txt. The backticks ` are used to execute the cat command within the backticks to read the content of the text file and assign its output to the variable i.\nzcat data/seq_project/*/${i} | wc -l: Like before, this line uncompresses (zcat) and counts (wc) the number of lines in the specified file. However, in this case, the file is determined by the content of fastq_files.txt, which contains a list of file names.\n\nAfter this modification of the code, we can very easily store the output in a file instead of printing the results to the screen.\n\n#Count the number of lines in each sequencing file\n#Store the output in a new folder\nmkdir counts \n\nfor i in `cat fastq_files.txt`; do \n    zcat data/seq_project/*/$i | wc -l &gt; counts/${i}.txt\ndone\n\nls counts/*txt \n\nhead counts/Sample-DUMMY1_R1.fastq.gz.txt\n\n\n\n\n\n\n\nTip: Avanced: better counting in for loops\n\n\n\n\n\nLet’s get a bit more advanced to show you some powerful features of bash. For this imagine that you would do this for 100 files. In this case it would be useful to see the file names next to the counts. We can achieve this by using what we have learned in the Better counting tip where we have learned about echo and command substitution.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(zcat $i | wc -l)\"\ndone\n\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on here \"$i: $(zcat $i | wc -l)\"\n$(...): These parentheses are used for command substitution. It means that the command zcat $i | wc -l is executed, and its output (the line count of the uncompressed content) is substituted in that position.\nThe double quotes (““) perform what is called a string concatenation. It concatenates the filename ($i), a colon (:), a space, and the line count obtained from the command substitution. The entire string is then passed as a single argument to the echo command.\n\nAlmost perfect, now we only want to divide this by 4:\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(( $(zcat $i | wc -l) /4 ))\"\ndone\n\nNotice, how we use both single brackets and double brackets?\n\n$((...)) in contrast to $(...) is an arithmetic expansion in Bash. It allows you to perform arithmetic operations and substitute the result into the command line."
  },
  {
    "objectID": "source/bash_intro.html#io-redirection-to-new-files",
    "href": "source/bash_intro.html#io-redirection-to-new-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "We have seen by now that by default our commands direct print the standard output to the terminal. For example, when we use ls the list of files and folders is printed. However, we can also redirect the standard output to a file by using the &gt; character.\nFor example, we might want to generate a list with all fastq files and can do this as follows:\n\nls data/seq_project/*/* &gt; fastq_paths.txt"
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-text-files",
    "href": "source/bash_intro.html#exploring-the-content-of-text-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s view the content of the list we just generated. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nhead can be used to check the first 10 rows of our files:\n\nhead fastq_paths.txt\n\n\n\n\nIf you want to check the last 10 rows use tail:\n\ntail fastq_paths.txt\n\n\n\n\nless is a program that let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S fastq_paths.txt\n\nOnce opened, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n\n\n\n\nTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano fastq_paths.txt\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter"
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-compressed-fastq-files",
    "href": "source/bash_intro.html#exploring-the-content-of-compressed-fastq-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Exercise\n\n\n\n\n\n\nView the first few rows of any one of the fastq.gz sequence files\nDoes this look like a normal sequence file to you?\n\n\n\nClick me to see an answer\n\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, so what is going on?\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloads folder, you have seen that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to (de)-compress the size of files.\nIn order to view the content of such files, we sometimes need to de-compress them first. We can do this using the gzip command together with the decompress -d option:\n\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows with information for each sequence:\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running the gzip command above, make a list of each sequence file with ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project/*/*\n\n#question 2:\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with sequencing files the data is usually much large and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nls data/seq_project/*/*\n\n\n\nWe have seen that to view the content of a compressed file and make sense of the content we had to first decompress the file. However, sequence files tend to get rather large and we might not want to decompress our files to not clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen called zcat, the original compressed file is kept while doing this. We combine this command with the head command, since do not want to print millions of sequences to the screen but only want to explore the first few rows:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head"
  },
  {
    "objectID": "source/bash_intro.html#cut-extracting-sections-from-tables",
    "href": "source/bash_intro.html#cut-extracting-sections-from-tables",
    "title": "Introduction to Bash",
    "section": "",
    "text": "We have seen that in fastq_paths.txt we see the list of files and the path leading to these files (relative to our home directory).\nHowever, for some things we might only want to have a list of the file names and not the paths, how would we do that?\nThere are different ways to do this, one is to use cut that allows us to separate columns using any delimiter, for example the / we use in the file structure. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourt element of each separated field.\n\n#view content of the file we work with \nhead fastq_paths.txt\n\n#only extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#store this in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name barcode002_User1_ITS_9_L001 and from that we want to extract some other information, such as a list with all the barcode IDs. We can easily do this as follows:\n\n#extract first the folder name, use a pipe and extract the barcode ID\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:"
  },
  {
    "objectID": "source/bash_intro.html#grep-finding-patterns-in-our-data",
    "href": "source/bash_intro.html#grep-finding-patterns-in-our-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in  fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c for counting.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also combine this with wildcards, for example, if we look for all samples from user1 we could do the following:\n\ngrep \"User1_ITS_*_\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing ITS_9 in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files\nCount how often “AAGACG” occurs in any in any of the fastq.gz files\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#3=2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if if the adaptors or primers are still part of your sequence."
  },
  {
    "objectID": "source/bash_intro.html#getting-help",
    "href": "source/bash_intro.html#getting-help",
    "title": "Introduction to Bash",
    "section": "",
    "text": "If you want to know what options are available for a command it is always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways to access the manual. These most common ways are:\n\nman ls\nls --help\nls -h"
  },
  {
    "objectID": "source/bash_intro.html#using-cat-to-read-and-combine",
    "href": "source/bash_intro.html#using-cat-to-read-and-combine",
    "title": "Introduction to Bash",
    "section": "",
    "text": "cat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files\n\nTo view files we can do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in this case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#combine files\ncat data/seq_project/*/*R1.fastq.gz &gt; combined.fastq.gz\n\n#view size individual files\nls -lh data/seq_project/*/*R1.fastq.gz\n\n#view size new file \nls -lh combined.fastq.gz"
  },
  {
    "objectID": "source/bash_intro.html#wc-counting-things",
    "href": "source/bash_intro.html#wc-counting-things",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command and allow us to count the number of lines in a file. As such it is an easy tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files. We could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nHowever, one down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command."
  },
  {
    "objectID": "source/bash_intro.html#pipes",
    "href": "source/bash_intro.html#pipes",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Pipes are a powerful utility to connect multiple commands together. Pipes allow us to feed the standard output of one command, such as ls as input into another command such as wc -l and as such combine multiple commands together.\nTherefore, lets use ls to first list all fastq files and then pipe the output from ls into the wc command in order to count with how many files we work with:\n\nls data/seq_project/*/* | wc -l\n\nWe should see again that we work with 8 files, but now we did not have to generate an intermediate text file and have a more condensed command.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse ls and wc to count how many R1 files we have\nCount how many R2 files we have\nCount how many files were generated for User1?\n\n\n\nClick me to see an answer\n\n\n#question 1: We see 4 files\nls data/seq_project/*/*R1*/*gz | wc -l \n\n#question 2: We see 4 files\nls data/seq_project/*/*R2*/*gz | wc -l \n\n#question 3: We see 4 files\nls data/seq_project/*_User1_*/*gz | wc -l \n\nChecking whether we have the same number of R1 and R2 files is a good sanity check, to ensure that we have received all the files from the sequencing center whenever we generate paired sequencing data."
  },
  {
    "objectID": "source/bash_intro.html#sort-and-uniq-to-create-unique-lists",
    "href": "source/bash_intro.html#sort-and-uniq-to-create-unique-lists",
    "title": "Introduction to Bash",
    "section": "",
    "text": "sort sort lines in a file from A-Z and is useful for file organization.\nuniqcan be used to remove or find duplicates . However, for this to work the file first needs to be sorted\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nExtract the folder names from path leading to the fastq.gz files\nFrom the folder names, extract the User names\nEnsure that we have a unique list of User names\n\n\n\nClick me to see an answer\n\n\n#extract the folder names\ncut -f3 -d \"/\" fastq_paths.txt\n\n#get user names \ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#get unique user names \ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq"
  },
  {
    "objectID": "source/bash_intro.html#rm-removing-files-and-folders",
    "href": "source/bash_intro.html#rm-removing-files-and-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "To remove files and folders, we use the rm command. Let’s first remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an argument. To do this, we use the -r argument (to remove directories and their contents recursively).\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone.\nSo use rm with care and check what you wrote twice before pressing enter!"
  },
  {
    "objectID": "source/bash_intro.html#pwd-find-out-where-we-are",
    "href": "source/bash_intro.html#pwd-find-out-where-we-are",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing and starting the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\n\npwd\n\npwd prints the location of the current working directory and tells you where exactly you are in the file system. When we login we typically start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but the path might be slightly different depending on your operating system. Below you find some help to orient yourself better for different terminal interfaces:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder, which gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn there set Persistent home directory to a folder of your choice\n\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop"
  },
  {
    "objectID": "source/bash_intro.html#cut-extracting-elements-from-strings",
    "href": "source/bash_intro.html#cut-extracting-elements-from-strings",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Remember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\n#view content of the file we work with \nhead fastq_paths.txt\n\n#only extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#store this in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as a list with all the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:"
  },
  {
    "objectID": "source/bash_intro.html#using-cat-to-read-and-combine-files",
    "href": "source/bash_intro.html#using-cat-to-read-and-combine-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "cat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files\n\nTo view files we can do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#combine files\ncat data/seq_project/*/*R1.fastq.gz &gt; combined.fastq.gz\n\n#view size individual files\nls -lh data/seq_project/*/*R1.fastq.gz\n\n#view size new file \nls -lh combined.fastq.gz"
  },
  {
    "objectID": "source/bash_intro.html#cd-move-around-folders",
    "href": "source/bash_intro.html#cd-move-around-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Most of the time you do not want to perform your analyses in the home directory but in a dedicated folder for your project. To get started, we will learn about the cd command that allows us to move around the file system.\nThe file system is a hierarchical system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use a relative path.\nTo start moving around, let’s begin by moving relative to our working directory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory:\n\ncd source/\n\nIf you use pwd after moving around directories, you should see that a different location is printed to the screen.\nWe can also move back to our original directory using cd .., which will move us back one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, ~, which can be used as a shortcut to move directly into our home directory from wherever you are on the file system:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd."
  },
  {
    "objectID": "source/bash_intro.html#mkdir-make-new-folders",
    "href": "source/bash_intro.html#mkdir-make-new-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and then create and move into a new folder called data_analysis as follows:\n\n#go into the home directory\ncd ~\n\n#in the home directory make a new folder and name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated correctly\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: commenting your code\n\n\n\n\n\nNotice, how in the example below I added the commands to run as well as some explanation about what I did?\nHere, I used a specific character # in front of a line of text to denote the beginning of a single-line comment. Anything coming after the character is considered a commend and won’t be executed by the shell.\nIn the above example I definitely commented the code too much as my comments basically duplicate the code and that should be avoided, however, it is useful to add comments in code to ensure that the purpose of the code is clear to others.\nYou could, for example, add a comment above functions to describe what the function does and you can also add comments to “tricky” code where it is not immediately obvious what you are trying to do. Basically, you want to add comments to any section where you think that the future you might get confused a month later.\nHere you find some examples for python and R but the same logic applies when writing code for bash, some examples for that can be found here.\n\n\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name."
  },
  {
    "objectID": "source/bash_intro.html#wget-download-data",
    "href": "source/bash_intro.html#wget-download-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s download our sequencing data into a new data folder. One way to do this is using wget. In the command below we add the option -P to specify into which folder we want to download the data. You an also see how I documented my code here in order to add some more specifics about where the link came from to help future me in case I, for example, need to look for some e-mails about the data later on.\n\nmkdir data\n\n#download data using link provided by the sequencing center on 23.01.2023\nwget -P data https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/\n\n#question 2:\nls -lh data/seq_project.tar.gz"
  },
  {
    "objectID": "source/bash_intro.html#tar-work-with-tar-files",
    "href": "source/bash_intro.html#tar-work-with-tar-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is commonly used when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use with the tar command are:\n\nx tells it to extract files from the archive\nv display verbose information and provide detailed information while creating the tarball\nf specify the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content of the folder we just extracted. Ensure that you explore the content of potential sub-directories.\nFind the path for at least one sequence file that ends on fastq.gz. Hint, to make your life easier, check the Tip: Command-line completion above to not have to type every single word.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project\n\n#question 2:\nls -l data/seq_project/barcode001_User1_ITS_1_L001/"
  },
  {
    "objectID": "source/bash_intro.html#rm-remove-files-and-folders",
    "href": "source/bash_intro.html#rm-remove-files-and-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "To remove files and folders, we use the rm command. We want to remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!"
  },
  {
    "objectID": "source/bash_intro.html#wc-count-things",
    "href": "source/bash_intro.html#wc-count-things",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an useful tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files.\nWe could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nOne down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command."
  },
  {
    "objectID": "source/bash_intro.html#sort-and-uniq-create-unique-lists",
    "href": "source/bash_intro.html#sort-and-uniq-create-unique-lists",
    "title": "Introduction to Bash",
    "section": "",
    "text": "sort: sort lines in a file from A-Z and is useful for file organization.\nuniq: remove or find duplicates . For this command to work you need to provide it with a sorted file\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse cut to print the folder names that lead to the fastq.gz files\nPrint the folder names as in 1 but then also extract the User names\nEnsure that when running the code from 2 that we print a unique list of User names\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncut -f3 -d \"/\" fastq_paths.txt\n\n#question 2:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#question 3:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq"
  },
  {
    "objectID": "source/bash_intro.html#cat-read-and-combine-files",
    "href": "source/bash_intro.html#cat-read-and-combine-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "cat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files\n\nTo view files we do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncat data/seq_project/*/*R1.fastq.gz &gt; combined.fastq.gz\n\n#question 2:\nls -lh data/seq_project/*/*R1.fastq.gz\n\n#question 3: \nls -lh combined.fastq.gz\n\nIf you compare the numbers from part 2 and 3, you should see that the combined.fastq.gz file is roughly the sum of the individual files."
  },
  {
    "objectID": "source/bash_intro.html#grep-find-patterns-in-data",
    "href": "source/bash_intro.html#grep-find-patterns-in-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c for counting.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also combine this with wildcards, for example, if we look for all samples from User1 we could do the following:\n\ngrep \"User1_ITS_*_\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing the pattern “ITS_9” in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files\nCount how often “AAGACG” occurs in any in any of the fastq.gz files\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if if the adapters or primers are still part of your sequence."
  },
  {
    "objectID": "source/bash_intro.html#ls-list-the-contents-of-a-directory",
    "href": "source/bash_intro.html#ls-list-the-contents-of-a-directory",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know where we are, let’s find out what files and folders exist in our home directory. For this we can use the ls command, which allows us to list directory contents:\n\nls\n\nIn my case this returns something like this:\n\n\n\nThe colors will look different depending on the cli use but in my case I see a list of files (in bold text) and folders (green-highlighted text) in my home directory.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands."
  },
  {
    "objectID": "source/bash_intro.html#cut-extract-elements-from-strings",
    "href": "source/bash_intro.html#cut-extract-elements-from-strings",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Remember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\nhead fastq_paths.txt\n\n#extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#do the same as above but this time save the output in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as a list with all the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:"
  },
  {
    "objectID": "source/bash_intro.html#wc-count-things-1",
    "href": "source/bash_intro.html#wc-count-things-1",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an easy way to perform sanity checks and in our case allows us to count how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We need to divide the number we see on the screen by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $((`wc -l`/4))\n\nIn the command above we have some new syntax:\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( `wc -l` / 4))\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line.\nThe backticks (``) around wc -l indicate command substitution. This means that the wc -l command is executed, and its output (the number of lines counted) is used in the overall command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”. Command substitution allows you to use the actual result of the command.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences."
  },
  {
    "objectID": "source/hpc_intro.html",
    "href": "source/hpc_intro.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes are used to actually run a program.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run multicore programs on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005).\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day.\nDo not leave allocations unused and set reasonable time limits on you jobs.\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM."
  },
  {
    "objectID": "source/hpc_intro.html#introduction",
    "href": "source/hpc_intro.html#introduction",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes are used to actually run a program.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run multicore programs on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005).\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day.\nDo not leave allocations unused and set reasonable time limits on you jobs.\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM."
  },
  {
    "objectID": "source/hpc_intro.html#hpc-introduction",
    "href": "source/hpc_intro.html#hpc-introduction",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes are used to actually run a program.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run multicore programs on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005).\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day.\nDo not leave allocations unused and set reasonable time limits on you jobs.\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM."
  }
]