[
  {
    "objectID": "source/installation.html",
    "href": "source/installation.html",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow a user to interact with an operating system. The key difference between the CLI and GUI is that the interaction with CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. CLI is often also referred to as the shell, terminal, console, prompt or various other names\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows.\n\n\n\n\n\nThe default shell is usually Bash and there is usually no need to install anything to be able to follow this tutorial. On most versions of Linux, th shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you can run it by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ computer search function. Search for: Terminal and press Return.\n\nTo ensure that we work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again.\n\n\n\nOperating systems like macOS and Linux come with a native command-line terminal, making it straightforward to run bash commands. However, for Windows users you need to install some software first to be able to use bash, below you find three options:\nOne option to access the bash shell commands is using Git Bash, for detailed installation instructions please have a look at the carpenties website.\nA second option is Mobaxterm, which enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here.\nA final option is to use Windows and Linux at the same time on a Windows machine. The Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option allows you to use all the tools available but since you more or less are installing a separating system on your PC needs to have enough memory to run this. Installation instructions can be found here.\n\n\n\n\n\n\nNote\n\n\n\nI am myself mostly familiar with WSL and the following tutorial is tailored towards the location of things when using WSL and Linux and your folder structure might be slightly different when using Git Bash or Mobaxterm.\nSimilarly, I am mainly familiar with the bash not the zsh shell. For Mac users that have a newer MAC and have trouble switching to bash this might create some issues when using wildcards.\nFor both issues: If parts of the tutorial do not work for you due to that, feel free to contact me and I can adjust the tutorial accordingly.\n\n\n\n\n\n\nAfter you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/installation.html#terminology",
    "href": "source/installation.html#terminology",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow a user to interact with an operating system. The key difference between the CLI and GUI is that the interaction with CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. CLI is often also referred to as the shell, terminal, console, prompt or various other names\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows.",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/installation.html#installation-guides",
    "href": "source/installation.html#installation-guides",
    "title": "Setting up a terminal",
    "section": "",
    "text": "The default shell is usually Bash and there is usually no need to install anything to be able to follow this tutorial. On most versions of Linux, th shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you can run it by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ computer search function. Search for: Terminal and press Return.\n\nTo ensure that we work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again.\n\n\n\nOperating systems like macOS and Linux come with a native command-line terminal, making it straightforward to run bash commands. However, for Windows users you need to install some software first to be able to use bash, below you find three options:\nOne option to access the bash shell commands is using Git Bash, for detailed installation instructions please have a look at the carpenties website.\nA second option is Mobaxterm, which enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here.\nA final option is to use Windows and Linux at the same time on a Windows machine. The Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option allows you to use all the tools available but since you more or less are installing a separating system on your PC needs to have enough memory to run this. Installation instructions can be found here.\n\n\n\n\n\n\nNote\n\n\n\nI am myself mostly familiar with WSL and the following tutorial is tailored towards the location of things when using WSL and Linux and your folder structure might be slightly different when using Git Bash or Mobaxterm.\nSimilarly, I am mainly familiar with the bash not the zsh shell. For Mac users that have a newer MAC and have trouble switching to bash this might create some issues when using wildcards.\nFor both issues: If parts of the tutorial do not work for you due to that, feel free to contact me and I can adjust the tutorial accordingly.",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/installation.html#sanity-check",
    "href": "source/installation.html#sanity-check",
    "title": "Setting up a terminal",
    "section": "",
    "text": "After you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:",
    "crumbs": [
      "Cli basics",
      "Setting up a terminal"
    ]
  },
  {
    "objectID": "source/hpc_howto.html",
    "href": "source/hpc_howto.html",
    "title": "Using an HPC",
    "section": "",
    "text": "Now, that we are familiar with using the cli, let’s upload our sequencing data to an HPC an run some software to check the quality of our reads.\n\n\nSSH (Secure Shell) is a network protocol that enables secure remote connections between two systems. The basic command to login looks like this:\n\n#connect to a server\nssh -X username@server\n\nOptions:\n\n-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. We use it since it enables users to run graphical applications on a remote server\n\nIf you have access to and want to connect to crunchomics you would connect like this:\n\nssh -X uvanetid1@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, contact ICT.\n\n\n\n\n\nIf you have not used Crunchomics before, then you want to run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is already installed\nSets up a python3 environment and some usefull python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following in the cli:\n\n/zfs/omics/software/script/omics_install_script\n\n\n\n\nscp stands for Secure Copy Protocol and allows us to securely copy files and directories between remote hosts. When transferring data the transfer is prepared from the terminal of your local computer and not from the HPCs login node.\nThe basic syntax is:\nscp [options] SOURCE DESTINATION\nTo start analysing our data, we want to move our fastq.gz files from our local folder, the source, to a folder on Cruncomics, the destination. Let’s start by:\n\nMoving from our home directory into our personal directory. We move there since we have more space in the personal directory. Notice, for bash to find the personal directory, we need to have run the omics_install_script script above first\nMake a project folder with a descriptive file name\n\n\ncd personal/\nmkdir projectX\ncd projectX\n\nInto our project directory, we the want to move the data we have before downloaded with our sequencing data. We do this by moving the data folder we generated before from our local computer to the HPC. Therefore, it is important that:\n\nwe run the following command from the cli on our own computer and not from the cli while being logged into the HPC!\nyou exchange the two instances of username in the code below with your username\n\nI am running the code from the data_analysis folder that we have generated in the previous tutorial and I use the -r option since we are moving a folder, not a file.\n\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/projectX\n\n#check if that worked \nll data/seq_project/*/*fastq.gz\n\n\n\n\n\n\n\nTip: moving data from the HPC to our own computer\n\n\n\n\n\nWe can also move data from the HPC to our own computer. For example, let’s assume we want to move a single sequencing file. In this case,\n\nWe do not need -r since we only move a single file.\nWe again run this command from a terminal on our computer, not while being logged in the HPC\nWe use . to indicate that we want to move the file into the directory we are currently in. If we want to move the file elsewhere we can use any absolute or relative path as needed\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz .\n\n\n\n\n\n\n\n\n\n\nTip: moving data from the HPC using wildcards\n\n\n\n\n\nWe can also move several files at once and ignore the whole folder structure by using wildcards when using scp.\n\n#make a random directory to move our data into for testing purposes\nmkdir transfer_test\n\nscp data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\n#view the data, see how the folder structure is different compared to our first example? \nll transfer_test/*\n\nNotice for MAC users:\nFor Mac users that work with an zsh shell this might not work and they might get an error like “file not found”, “no matches found” or something the like. Without going into details zsh has a slightly different way of handling wildcards and tries to interpret the wildcard literally and tries not to extend the wildcard. If you see the error and you are sure the file exists it should work to edit your line of code as follows:\n\n\\scp  data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\nIf that does not work, these are some other things to try (different zsh environments might need slightly different solutions):\n\nnoglob scp  data/seq_project/barcode00*/*fastq.gz ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\nscp  'data/seq_project/barcode00*/*fastq.gz' ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\n\n\n\n\n\n\n\n\nNow, that we have our data prepared we want to run a tool to assess the quality of our reads. Before doing that, let’s talk about submitting jobs to an HPC.\nWhen getting started on a new HPC it is good to know how to get basic information about what nodes are avaiable on a cluster by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, you see information about the:\n\npartition: the queues that are available\nstate: if a node is busy or not\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes\nNodeList: the names of the nodes omics-cn001 to omics-cn005\n\n\n\n\nThe following command gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\nIf we would have submitted a job, we also should see it running.\n\n\n\n\nsrun is used when you want to run tasks interactively or have more control over the execution. You directly issue srun commands in the terminal, specifying the tasks to be executed and their resource requirements.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback.\nYou are testing or debugging your commands before incorporating them into a script.\nYou need more control over the execution of tasks.\n\nLet’s submit a very simple example for which we would not even need to submit a job, but just to get you started.\n\nsrun echo \"Hello interactively\"\n\nYou should see the output of echo printed to the screen and if you would run squeue you won’t even see your job since everything ran so fast. Now assume you want to run a more complex interactive task with srun then it is good to specify the resources your job needs by adding flags, i.e.\n\nsrun --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=1G echo \"Hello interactively\"\n\nHere, each flag means the following:\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit.\n--ntasks=1: Defines the number of tasks to run. Here, it’s set to 1 since we want to use echo once\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs\necho \"Hello interactively: The actual command you want to run interactively\n\n\n\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTesting with Small Datasets: For initial testing and debugging, consider working with a smaller subset of your dataset. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used. Look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate. An example command would be sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,nnodes,ncpus,nodelist\nFor instance, if you would have used --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). Verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\n\nFastQC is a quality control tool for high throughput sequence data that is already installed on crunchomics. Let’s use this to run an actually software on our data.\n\nmkdir -p results/fastqc \n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\nll results/fastqc /*\n\nSince we work with little data this will run extremely fast, however, if you would be logged into Crunchomics via a second window and run squeue you should see that your job is actively running:\n\n\n\n\n\nAdditionally, after the run is completed, you should see that several html files were generated in our fastqc folder.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse scp to download the data to your own computer and view one of the html files.\n\n\nClick me to see an answer\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/results/fastqc/*html results/fastqc/\n\nYou could also open a file on Crunchomics with firefox results/fastqc/Sample-DUMMY1_R1_fastqc.html. However, connecting to the internet via a cli on a remote server tends to be rather slow and its often better to view files to your own computer especially if they are large files.\nIf you want to know more about how to to interprete the output, you can visit the fastqc website, which gives some examples for interpreting good and bad reports.\n\n\n\n\n\n\n\n\nOne down-side of srun for long running jobs is that your terminal gets “blocked” as long as the job is running and that your job is lost if you loose the ssh connection to the HPC. For long running jobs, there are two ways to deal with this:\n\nsubmit a srun job in a screen\nuse sbatch\n\nIn this section, we will cover how to use Screen. Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach (go outside of a screen but keep the screen running in the background) from a screen with control+a+d.\nIf you run multiple things, it can be useful to give your screens more descriptive names. You can do this as follows:\n\n#start a screen and give it a name\nscreen -S run_fastqc\n\nAfter detaching from a screen you can list all currently running screens with:\n\nscreen -ls\n\nYou can restart a screen like this:\n\n#restart an existing screen\nscreen -r run_fastqc\n\nNow inside our screen, we can run fastqc same as we did before:\n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1 \n\nAnd for long-running jobs we can jump inside and outside of the job, while it is running and at the same time do other things from the cli.\nIf you want to completely close and remove a screen, type the following while being inside of the screen:\n\nexit\n\n\n\n\nsbatch is your go-to command when you have a script (a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks.\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nLet’s start with having some good folder organization to keep our project folder organized:\n\nmkdir scripts \nmkdir logs\n\nTo get started, assume we have created a script named run_fastqc.sh with the following content in which we want to run fastqc. Notice, how in this script I added some additional commands. Here, I just use this to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you could do it.\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\nTo prepare the script:\n\nrun nano scripts/run_fastqc.sh.\nCopy and paste the content you see above.\nPress ctrl+x to exit nano\nType Y when prompted if the changes should be saved.\nConfirm the name by pressing enter\n\nThe we can submit run_fastqc.sh with:\n\n#submit job: 754\nsbatch sbatch scripts/run_fastqc.sh\n\n#check if job is running correctly\nsqueue\n\nAfter running this, you should see that the job was submitted and something like this printed to the screen Submitted batch job 754.\nYou will also see that a new file is generated that will look something like this slurm-425707.out. When you submit a batch job using sbatch, Slurm redirects the standard output and standard error streams of your job to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here.\n\nFeel free to explore the content of the log file, do you see how the echo commands are used as well?\n\n\n\n\n\n\nTip: sbatch and better log files\n\n\n\n\n\nWe have seen that by default sbatch redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important, you find below an example to:\n\nStore the standard output and error in two separate files\nRedirect the output into another folder, the logs folder\nIn the code below, the %j is replaced with the job allocation number\n\n\n#!/bin/bash\n#SBATCH --job-name=our_fastqc_job\n#SBATCH --output=logs/fastqc_%j.out\n#SBATCH --error=logs/fastqc_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\n\n\n\n\n\n\n\n\n\nAdvanced tip: sbatch and multiple files\n\n\n\n\n\nWith fastqc we are very lucky that it can identify all the fastq files in the directory we specify with -o and use a wildcard. This is extremely useful for us but by far not all programs work this way.\nFor this section, lets assume that we need to provide each individual file we want to analyse, one by one. How would we run such a job effectively?\nWhat we want to do is created what is called a job array that allows us to:\n\nRun multiple jobs that have the same job definition, i.e. cpus, memory and software used\nRun these job in the most optimal way. I.e. we do not want to run one job after each other but we also want to run jobs in parallel at the same time to optimize resource usage.\n\nLet’s start with making a list with files we want to work with based on what we have already learned:\n\nls data/seq_project/*/*.gz | cut -f4 -d \"/\" &gt; samples.txt\n\nNext, we can use this text file in our job array, the content of which we store in scripts/array.sh:\n\n#!/bin/bash\n\n#SBATCH --job-name=my_array\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --array=1-8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n\n#calculate the index of the current job within the batch\n#in our case the index will store the values 1 to 8\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build array structure via ale file names\nCURRENT_SAMPLE=$(cat samples.txt |  sed -n \"${INDEX}p\")\n\n#print what is actually happening\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\n#run the actual job\nfastqc data/seq_project/*/${CURRENT_SAMPLE} -o results/fastqc  --threads 1\n\nIn the script we use some new SLURM arguments:\n\n#SBATCH --array=1-8: Sets up a job array, specifying the range (1-8). We choose 1-8 because we have exactly 8 fastq.gz files we want to analyse\n#SBATCH --output=logs/array_%A_%a.out: Store the standard output and error. %A represents the job ID assigned by Slurm, and %a represents the array task ID\n\nThe job does the following:\n\nThe INDEX variable is storing the value of the current SLURM_ARRAY_TASK_ID. This represents the ID of the current job within the array. In our case this will be first 1, then 2, …, and finally 3.\nNext, we build the array structure:\n\nThe CURRENT_SAMPLE variable is created by reading the sample_list.txt file with cat.\nWe then use a pipe to extract the sample at the calculated index using sed. Sed is an extremly powerful way to edit text that we have not yet covered but -n 1p is a option that allows us to print one specific line of a file, here the first one when running array 1. So for the first array the actual code run is the following cat samples.txt |  sed -n \"1p\". For the next array, we would run cat samples.txt |  sed -n \"2p\" and so forth.\nThe output of the pipe is stored in a variable, called CURRENT_SAMPLE. For our first sample this will be Sample-DUMMY1_R1.fastq.gz\n\nWe use echo to record what was executed when to store it in the standard output\nWe run our actual fastqc job on the file name that is currently stored in the CURRENT_SAMPLE variable.\n\nIf we check what is happening right after submitting the job with squeue we should see something like this:\n\n\n\n\n\nWe see that jobs 1-4 are already running and the other jobs are currently waiting for space.\nIf we check the log files we should see:\n\n\n\n\n\nWe see that we get an individual output and error file for each job. In the output we see what value is stored in the INDEX, here 1, and the CURRENT_SAMPLE, here Sample-DUMMY1_R1.fastq.gz.\n\n\n\n\n\n\nThere might be cases where the software you are interested in is not installed.\nIn the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges. Let’go through a very brief example:\n\n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\n\nLet’s assume we want to install seqkit, a tool that allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length.\nWe can do this as follows:\n\n#check if the tool is installed (should return command not found if the software is not installed)\nseqkit -h\n\n#create an empty environment and name it seqkit\nmamba create -n seqkit\n\n#install some software, i.e. seqkit, into the seqkit environment\nmamba install -n seqkit -c bioconda seqkit\n\n#to run the tool activate the environment first\nmamba activate seqkit\n\n#check if tool is installed\nseqkit -h\n\n#run the tool via srun\nmkdir results/seqkit\nsrun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\nless -S results/seqkit/seqkit_stats.tsv \n\n#leave the environment\nconda deactivate\n\nWhen installing the seqkit package we specify that we want to look for seqkit in the bioconda channel with the option -c. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.\nUnsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page, which also should inform you whether you need to add a specific channel to install the software.\nA full set of mamba/conda commands can be found here\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nDownload and view the file results/seqkit/seqkit_stats.tsv on your own computer\nRun the seqkit again but this time submit the job via a sbatch script instead of using srun\n\n\n\nClick me to see an answer\n\n\n#question 1\nscp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .\n\n#question 2 \nsbatch scripts/seqkit.sh\n\nContent of scripts/seqkit.sh:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit_job\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start seqkit\"\n\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\n\necho \"seqkit finished\"",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#ssh-connecting-to-a-sever",
    "href": "source/hpc_howto.html#ssh-connecting-to-a-sever",
    "title": "Using an HPC",
    "section": "",
    "text": "SSH (Secure Shell) is a network protocol that enables secure remote connections between two systems. The basic command to login looks like this:\n\n#connect to a server\nssh -X username@server\n\nOptions:\n\n-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. We use it since it enables users to run graphical applications on a remote server\n\nIf you have access to and want to connect to crunchomics you would connect like this:\n\nssh -X uvanetid1@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, contact ICT.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#crunchomics-preparing-your-account",
    "href": "source/hpc_howto.html#crunchomics-preparing-your-account",
    "title": "Using an HPC",
    "section": "",
    "text": "If you have not used Crunchomics before, then you want to run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is already installed\nSets up a python3 environment and some usefull python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following in the cli:\n\n/zfs/omics/software/script/omics_install_script",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#scp-transferring-data-fromto-a-server",
    "href": "source/hpc_howto.html#scp-transferring-data-fromto-a-server",
    "title": "Using an HPC",
    "section": "",
    "text": "scp stands for Secure Copy Protocol and allows us to securely copy files and directories between remote hosts. When transferring data the transfer is prepared from the terminal of your local computer and not from the HPCs login node.\nThe basic syntax is:\nscp [options] SOURCE DESTINATION\nTo start analysing our data, we want to move our fastq.gz files from our local folder, the source, to a folder on Cruncomics, the destination. Let’s start by:\n\nMoving from our home directory into our personal directory. We move there since we have more space in the personal directory. Notice, for bash to find the personal directory, we need to have run the omics_install_script script above first\nMake a project folder with a descriptive file name\n\n\ncd personal/\nmkdir projectX\ncd projectX\n\nInto our project directory, we the want to move the data we have before downloaded with our sequencing data. We do this by moving the data folder we generated before from our local computer to the HPC. Therefore, it is important that:\n\nwe run the following command from the cli on our own computer and not from the cli while being logged into the HPC!\nyou exchange the two instances of username in the code below with your username\n\nI am running the code from the data_analysis folder that we have generated in the previous tutorial and I use the -r option since we are moving a folder, not a file.\n\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/projectX\n\n#check if that worked \nll data/seq_project/*/*fastq.gz\n\n\n\n\n\n\n\nTip: moving data from the HPC to our own computer\n\n\n\n\n\nWe can also move data from the HPC to our own computer. For example, let’s assume we want to move a single sequencing file. In this case,\n\nWe do not need -r since we only move a single file.\nWe again run this command from a terminal on our computer, not while being logged in the HPC\nWe use . to indicate that we want to move the file into the directory we are currently in. If we want to move the file elsewhere we can use any absolute or relative path as needed\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz .\n\n\n\n\n\n\n\n\n\n\nTip: moving data from the HPC using wildcards\n\n\n\n\n\nWe can also move several files at once and ignore the whole folder structure by using wildcards when using scp.\n\n#make a random directory to move our data into for testing purposes\nmkdir transfer_test\n\nscp data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\n#view the data, see how the folder structure is different compared to our first example? \nll transfer_test/*\n\nNotice for MAC users:\nFor Mac users that work with an zsh shell this might not work and they might get an error like “file not found”, “no matches found” or something the like. Without going into details zsh has a slightly different way of handling wildcards and tries to interpret the wildcard literally and tries not to extend the wildcard. If you see the error and you are sure the file exists it should work to edit your line of code as follows:\n\n\\scp  data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\nIf that does not work, these are some other things to try (different zsh environments might need slightly different solutions):\n\nnoglob scp  data/seq_project/barcode00*/*fastq.gz ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\nscp  'data/seq_project/barcode00*/*fastq.gz' ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#slurm-basics",
    "href": "source/hpc_howto.html#slurm-basics",
    "title": "Using an HPC",
    "section": "",
    "text": "Now, that we have our data prepared we want to run a tool to assess the quality of our reads. Before doing that, let’s talk about submitting jobs to an HPC.\nWhen getting started on a new HPC it is good to know how to get basic information about what nodes are avaiable on a cluster by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, you see information about the:\n\npartition: the queues that are available\nstate: if a node is busy or not\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes\nNodeList: the names of the nodes omics-cn001 to omics-cn005\n\n\n\n\nThe following command gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\nIf we would have submitted a job, we also should see it running.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#srun-submitting-a-job-interactively",
    "href": "source/hpc_howto.html#srun-submitting-a-job-interactively",
    "title": "Using an HPC",
    "section": "",
    "text": "srun is used when you want to run tasks interactively or have more control over the execution. You directly issue srun commands in the terminal, specifying the tasks to be executed and their resource requirements.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback.\nYou are testing or debugging your commands before incorporating them into a script.\nYou need more control over the execution of tasks.\n\nLet’s submit a very simple example for which we would not even need to submit a job, but just to get you started.\n\nsrun echo \"Hello interactively\"\n\nYou should see the output of echo printed to the screen and if you would run squeue you won’t even see your job since everything ran so fast. Now assume you want to run a more complex interactive task with srun then it is good to specify the resources your job needs by adding flags, i.e.\n\nsrun --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=1G echo \"Hello interactively\"\n\nHere, each flag means the following:\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit.\n--ntasks=1: Defines the number of tasks to run. Here, it’s set to 1 since we want to use echo once\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs\necho \"Hello interactively: The actual command you want to run interactively\n\n\n\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTesting with Small Datasets: For initial testing and debugging, consider working with a smaller subset of your dataset. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used. Look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate. An example command would be sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,nnodes,ncpus,nodelist\nFor instance, if you would have used --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). Verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\n\nFastQC is a quality control tool for high throughput sequence data that is already installed on crunchomics. Let’s use this to run an actually software on our data.\n\nmkdir -p results/fastqc \n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\nll results/fastqc /*\n\nSince we work with little data this will run extremely fast, however, if you would be logged into Crunchomics via a second window and run squeue you should see that your job is actively running:\n\n\n\n\n\nAdditionally, after the run is completed, you should see that several html files were generated in our fastqc folder.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse scp to download the data to your own computer and view one of the html files.\n\n\nClick me to see an answer\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/results/fastqc/*html results/fastqc/\n\nYou could also open a file on Crunchomics with firefox results/fastqc/Sample-DUMMY1_R1_fastqc.html. However, connecting to the internet via a cli on a remote server tends to be rather slow and its often better to view files to your own computer especially if they are large files.\nIf you want to know more about how to to interprete the output, you can visit the fastqc website, which gives some examples for interpreting good and bad reports.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#screen-submitting-long-running-jobs-via-srun",
    "href": "source/hpc_howto.html#screen-submitting-long-running-jobs-via-srun",
    "title": "Using an HPC",
    "section": "",
    "text": "One down-side of srun for long running jobs is that your terminal gets “blocked” as long as the job is running and that your job is lost if you loose the ssh connection to the HPC. For long running jobs, there are two ways to deal with this:\n\nsubmit a srun job in a screen\nuse sbatch\n\nIn this section, we will cover how to use Screen. Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach (go outside of a screen but keep the screen running in the background) from a screen with control+a+d.\nIf you run multiple things, it can be useful to give your screens more descriptive names. You can do this as follows:\n\n#start a screen and give it a name\nscreen -S run_fastqc\n\nAfter detaching from a screen you can list all currently running screens with:\n\nscreen -ls\n\nYou can restart a screen like this:\n\n#restart an existing screen\nscreen -r run_fastqc\n\nNow inside our screen, we can run fastqc same as we did before:\n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1 \n\nAnd for long-running jobs we can jump inside and outside of the job, while it is running and at the same time do other things from the cli.\nIf you want to completely close and remove a screen, type the following while being inside of the screen:\n\nexit",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#sbatch-submitting-a-job",
    "href": "source/hpc_howto.html#sbatch-submitting-a-job",
    "title": "Using an HPC",
    "section": "",
    "text": "sbatch is your go-to command when you have a script (a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks.\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nLet’s start with having some good folder organization to keep our project folder organized:\n\nmkdir scripts \nmkdir logs\n\nTo get started, assume we have created a script named run_fastqc.sh with the following content in which we want to run fastqc. Notice, how in this script I added some additional commands. Here, I just use this to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you could do it.\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\nTo prepare the script:\n\nrun nano scripts/run_fastqc.sh.\nCopy and paste the content you see above.\nPress ctrl+x to exit nano\nType Y when prompted if the changes should be saved.\nConfirm the name by pressing enter\n\nThe we can submit run_fastqc.sh with:\n\n#submit job: 754\nsbatch sbatch scripts/run_fastqc.sh\n\n#check if job is running correctly\nsqueue\n\nAfter running this, you should see that the job was submitted and something like this printed to the screen Submitted batch job 754.\nYou will also see that a new file is generated that will look something like this slurm-425707.out. When you submit a batch job using sbatch, Slurm redirects the standard output and standard error streams of your job to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here.\n\nFeel free to explore the content of the log file, do you see how the echo commands are used as well?\n\n\n\n\n\n\nTip: sbatch and better log files\n\n\n\n\n\nWe have seen that by default sbatch redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important, you find below an example to:\n\nStore the standard output and error in two separate files\nRedirect the output into another folder, the logs folder\nIn the code below, the %j is replaced with the job allocation number\n\n\n#!/bin/bash\n#SBATCH --job-name=our_fastqc_job\n#SBATCH --output=logs/fastqc_%j.out\n#SBATCH --error=logs/fastqc_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\n\n\n\n\n\n\n\n\n\nAdvanced tip: sbatch and multiple files\n\n\n\n\n\nWith fastqc we are very lucky that it can identify all the fastq files in the directory we specify with -o and use a wildcard. This is extremely useful for us but by far not all programs work this way.\nFor this section, lets assume that we need to provide each individual file we want to analyse, one by one. How would we run such a job effectively?\nWhat we want to do is created what is called a job array that allows us to:\n\nRun multiple jobs that have the same job definition, i.e. cpus, memory and software used\nRun these job in the most optimal way. I.e. we do not want to run one job after each other but we also want to run jobs in parallel at the same time to optimize resource usage.\n\nLet’s start with making a list with files we want to work with based on what we have already learned:\n\nls data/seq_project/*/*.gz | cut -f4 -d \"/\" &gt; samples.txt\n\nNext, we can use this text file in our job array, the content of which we store in scripts/array.sh:\n\n#!/bin/bash\n\n#SBATCH --job-name=my_array\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --array=1-8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n\n#calculate the index of the current job within the batch\n#in our case the index will store the values 1 to 8\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build array structure via ale file names\nCURRENT_SAMPLE=$(cat samples.txt |  sed -n \"${INDEX}p\")\n\n#print what is actually happening\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\n#run the actual job\nfastqc data/seq_project/*/${CURRENT_SAMPLE} -o results/fastqc  --threads 1\n\nIn the script we use some new SLURM arguments:\n\n#SBATCH --array=1-8: Sets up a job array, specifying the range (1-8). We choose 1-8 because we have exactly 8 fastq.gz files we want to analyse\n#SBATCH --output=logs/array_%A_%a.out: Store the standard output and error. %A represents the job ID assigned by Slurm, and %a represents the array task ID\n\nThe job does the following:\n\nThe INDEX variable is storing the value of the current SLURM_ARRAY_TASK_ID. This represents the ID of the current job within the array. In our case this will be first 1, then 2, …, and finally 3.\nNext, we build the array structure:\n\nThe CURRENT_SAMPLE variable is created by reading the sample_list.txt file with cat.\nWe then use a pipe to extract the sample at the calculated index using sed. Sed is an extremly powerful way to edit text that we have not yet covered but -n 1p is a option that allows us to print one specific line of a file, here the first one when running array 1. So for the first array the actual code run is the following cat samples.txt |  sed -n \"1p\". For the next array, we would run cat samples.txt |  sed -n \"2p\" and so forth.\nThe output of the pipe is stored in a variable, called CURRENT_SAMPLE. For our first sample this will be Sample-DUMMY1_R1.fastq.gz\n\nWe use echo to record what was executed when to store it in the standard output\nWe run our actual fastqc job on the file name that is currently stored in the CURRENT_SAMPLE variable.\n\nIf we check what is happening right after submitting the job with squeue we should see something like this:\n\n\n\n\n\nWe see that jobs 1-4 are already running and the other jobs are currently waiting for space.\nIf we check the log files we should see:\n\n\n\n\n\nWe see that we get an individual output and error file for each job. In the output we see what value is stored in the INDEX, here 1, and the CURRENT_SAMPLE, here Sample-DUMMY1_R1.fastq.gz.",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#installing-software",
    "href": "source/hpc_howto.html#installing-software",
    "title": "Using an HPC",
    "section": "",
    "text": "There might be cases where the software you are interested in is not installed.\nIn the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges. Let’go through a very brief example:\n\n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\n\nLet’s assume we want to install seqkit, a tool that allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length.\nWe can do this as follows:\n\n#check if the tool is installed (should return command not found if the software is not installed)\nseqkit -h\n\n#create an empty environment and name it seqkit\nmamba create -n seqkit\n\n#install some software, i.e. seqkit, into the seqkit environment\nmamba install -n seqkit -c bioconda seqkit\n\n#to run the tool activate the environment first\nmamba activate seqkit\n\n#check if tool is installed\nseqkit -h\n\n#run the tool via srun\nmkdir results/seqkit\nsrun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\nless -S results/seqkit/seqkit_stats.tsv \n\n#leave the environment\nconda deactivate\n\nWhen installing the seqkit package we specify that we want to look for seqkit in the bioconda channel with the option -c. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.\nUnsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page, which also should inform you whether you need to add a specific channel to install the software.\nA full set of mamba/conda commands can be found here\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nDownload and view the file results/seqkit/seqkit_stats.tsv on your own computer\nRun the seqkit again but this time submit the job via a sbatch script instead of using srun\n\n\n\nClick me to see an answer\n\n\n#question 1\nscp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .\n\n#question 2 \nsbatch scripts/seqkit.sh\n\nContent of scripts/seqkit.sh:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit_job\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\necho \"Start seqkit\"\n\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\n\necho \"seqkit finished\"",
    "crumbs": [
      "Using an HPC",
      "Using an HPC"
    ]
  },
  {
    "objectID": "source/bash_intro.html",
    "href": "source/bash_intro.html",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing and starting the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\n\npwd\n\npwd prints the location of the current working directory and tells you where exactly you are in the file system. When we login we typically start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but the path might be slightly different depending on your operating system. Below you find some help to orient yourself better for different terminal interfaces:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder, which gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn there set Persistent home directory to a folder of your choice\n\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\n\n\n\n\n\n\nNow that we know where we are, let’s find out what files and folders exist in our home directory. For this we can use the ls command, which allows us to list directory contents:\n\nls\n\nIn my case this returns something like this:\n\n\n\nThe colors will look different depending on the cli use but in my case I see a list of files (in bold text) and folders (green-highlighted text) in my home directory.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands.\n\n\n\nLet´s start with looking at the general structure of a command, which generally consists of three elements, the command itself and some optional options and arguments:\n\n\n\nUnderstanding better what a command looks like, let’s use the ls command together with the option -l. This option results in ls printing the content of a folder in a long listing format.\n\nls -l\n\nAfter running this, we should see our files and folders again but in a long format, which gives more detailed information and structures our output a bit better:\n\n\n\n\n\n\nIf you want to know what options are available for a command it is always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways to access the manual. These most common ways are:\n\nman ls\nls --help\nls -h\n\n\n\n\nMost of the time you do not want to perform your analyses in the home directory but in a dedicated folder for your project. To get started, we will learn about the cd command that allows us to move around the file system.\nThe file system is a hierarchical system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use a relative path.\nTo start moving around, let’s begin by moving relative to our working directory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory:\n\ncd source/\n\nIf you use pwd after moving around directories, you should see that a different location is printed to the screen.\nWe can also move back to our original directory using cd .., which will move us back one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, ~, which can be used as a shortcut to move directly into our home directory from wherever you are on the file system:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd.\n\n\n\n\n\n\nNow that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and then create and move into a new folder called data_analysis as follows:\n\n#go into the home directory\ncd ~\n\n#in the home directory make a new folder and name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated correctly\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: commenting your code\n\n\n\n\n\nNotice, how in the example below I added the commands to run as well as some explanation about what I did?\nHere, I used a specific character # in front of a line of text to denote the beginning of a single-line comment. Anything coming after the character is considered a commend and won’t be executed by the shell.\nIn the above example I definitely commented the code too much as my comments basically duplicate the code and that should be avoided, however, it is useful to add comments in code to ensure that the purpose of the code is clear to others.\nYou could, for example, add a comment above functions to describe what the function does and you can also add comments to “tricky” code where it is not immediately obvious what you are trying to do. Basically, you want to add comments to any section where you think that the future you might get confused a month later.\nHere you find some examples for python and R but the same logic applies when writing code for bash, some examples for that can be found here.\n\n\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name.\n\n\n\n\n\n\nNext, let’s download our sequencing data into a new data folder. One way to do this is using wget. In the command below we add the option -P to specify into which folder we want to download the data. You an also see how I documented my code here in order to add some more specifics about where the link came from to help future me in case I, for example, need to look for some e-mails about the data later on.\n\nmkdir data\n\n#download data using link provided by the sequencing center on 23.01.2023\nwget -P data https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/\n\n#question 2:\nls -lh data/seq_project.tar.gz\n\n\n\n\n\n\n\n\nThe data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is commonly used when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use with the tar command are:\n\nx tells it to extract files from the archive\nv display verbose information and provide detailed information while creating the tarball\nf specify the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content of the folder we just extracted. Ensure that you explore the content of potential sub-directories.\nFind the path for at least one sequence file that ends on fastq.gz. Hint, to make your life easier, check the Tip: Command-line completion above to not have to type every single word.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project\n\n#question 2:\nls -l data/seq_project/barcode001_User1_ITS_1_L001/\n\n\n\n\n\n\n\n\nTo remove files and folders, we use the rm command. We want to remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!\n\n\n\n\n\nAfter we have explored the folder with our sequencing data with ls, we have seen that data/seq_project contains 4 folders, 2 folders for two different users who each generated two replicates.\nYou also might have also noticed that it gets tedious to figure out how many files are in each folder because we would need to run ls on each single folder and view its content individually.\nLuckily, the shell provides special characters to rapidly specify groups of filenames or patterns. Wild-cards are characters that can be used as a substitute for any class of characters in a search.\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. It allows us to list all files in the seq_project folder as follows:\n\nls data/seq_project/*\n\nYou should have seen before that the sequencing files all end with .fastq.gz we can make this command a bit more specific and at the same time make the output a bit more readable:\n\nls data/seq_project/*/*.gz\n\nIf we wanted to print the absolute and not the relative path, we could do the following (beware that the absolute path will be slightly different depending in you system):\n\nls /home/User_name/data/seq_project/*/*.gz\n\nNow we can easily see for each folder how many files we have.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a wildcard to list files for barcode001 only\nUse a wildcard to list files only for user1\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls data/seq_project/barcode001*/*gz\n\n#question 2:\nls data/seq_project/*_User1_*/*gz\n\n\n\n\n\n\n\n\n\n\n\nTip: More wildcards\n\n\n\n\n\n* is not the only wildcard we can use and a full list can be found here.\nDifferent wildcards can be useful in different contexts, not only only when listing files but also finding patterns in files. To make such searches as specific as possible there are different wildcards available. A few examples are:\nThe [0-9] wildcard = matches any number exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[0-9]*/*.gz\n\nThe [012] wildcard = matches 1 or 2 exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[12]*/*.gz\n\n[A-Z] matches any letter in capitals occurring once\n[a-z]* matches any letter in non-capital letters occurring many times\n\nls data/seq_project/barcode001_User1_ITS_1_[A-Z]001/*.gz\n\n\n\n\n\n\n\nWe have seen by now that by default our commands direct print the standard output to the terminal. For example, when we use ls the list of files and folders is printed. However, we can also redirect the standard output to a file by using the &gt; character.\nFor example, we might want to generate a list with all fastq files and can do this as follows:\n\nls data/seq_project/*/* &gt; fastq_paths.txt\n\n\n\n\nNext, let’s view the content of the list we just generated. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nhead can be used to check the first 10 rows of our files:\n\nhead fastq_paths.txt\n\n\n\n\nIf you want to check the last 10 rows use tail:\n\ntail fastq_paths.txt\n\n\n\n\nless is a program that let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S fastq_paths.txt\n\nOnce opened, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n\n\n\n\nTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano fastq_paths.txt\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter\n\n\n\n\n\n\n\n\nAnother useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an useful tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files.\nWe could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nOne down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command.\n\n\n\nPipes are a powerful utility to connect multiple commands together. Pipes allow us to feed the standard output of one command, such as ls as input into another command such as wc -l and as such combine multiple commands together.\nTherefore, lets use ls to first list all fastq files and then pipe the output from ls into the wc command in order to count with how many files we work with:\n\nls data/seq_project/*/* | wc -l\n\nWe should see again that we work with 8 files, but now we did not have to generate an intermediate text file and have a more condensed command.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse ls and wc to count how many R1 files we have\nCount how many R2 files we have\nCount how many files were generated for User1?\n\n\n\nClick me to see an answer\n\n\n#question 1: We see 4 files\nls data/seq_project/*/*R1*/*gz | wc -l \n\n#question 2: We see 4 files\nls data/seq_project/*/*R2*/*gz | wc -l \n\n#question 3: We see 4 files\nls data/seq_project/*_User1_*/*gz | wc -l \n\nChecking whether we have the same number of R1 and R2 files is a good sanity check, to ensure that we have received all the files from the sequencing center whenever we generate paired sequencing data.\n\n\n\n\n\n\n\nRemember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\nhead fastq_paths.txt\n\n#extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#do the same as above but this time save the output in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as a list with all the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:\n\n\n\n\nsort: sort lines in a file from A-Z and is useful for file organization.\nuniq: remove or find duplicates . For this command to work you need to provide it with a sorted file\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse cut to print the folder names that lead to the fastq.gz files\nPrint the folder names as in 1 but then also extract the User names\nEnsure that when running the code from 2 that we print a unique list of User names\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncut -f3 -d \"/\" fastq_paths.txt\n\n#question 2:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#question 3:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\n\ncat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files\n\nTo view files we do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncat data/seq_project/*/*R1.fastq.gz &gt; combined.fastq.gz\n\n#question 2:\nls -lh data/seq_project/*/*R1.fastq.gz\n\n#question 3: \nls -lh combined.fastq.gz\n\nIf you compare the numbers from part 2 and 3, you should see that the combined.fastq.gz file is roughly the sum of the individual files.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nView the first few rows of any one of the fastq.gz sequence files\nDoes this look like a normal sequence file to you?\n\n\n\nClick me to see an answer\n\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, so what is going on?\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloads folder, you have seen that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to (de)-compress the size of files.\nIn order to view the content of such files, we sometimes need to de-compress them first. We can do this using the gzip command together with the decompress -d option:\n\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows with information for each sequence:\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running the gzip command above, make a list of each sequence file with ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project/*/*\n\n#question 2:\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with sequencing files the data is usually much large and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nls data/seq_project/*/*\n\n\n\nWe have seen that to view the content of a compressed file and make sense of the content we had to first decompress the file. However, sequence files tend to get rather large and we might not want to decompress our files to not clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen called zcat, the original compressed file is kept while doing this. We combine this command with the head command, since do not want to print millions of sequences to the screen but only want to explore the first few rows:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head\n\n\n\n\n\n\nThe grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c for counting.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also combine this with wildcards, for example, if we look for all samples from User1 we could do the following:\n\ngrep \"User1_ITS_*_\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing the pattern “ITS_9” in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files\nCount how often “AAGACG” occurs in any in any of the fastq.gz files\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if if the adapters or primers are still part of your sequence.\n\n\n\n\n\n\n\nAnother useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an easy way to perform sanity checks and in our case allows us to count how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We need to divide the number we see on the screen by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $((`wc -l`/4))\n\nIn the command above we have some new syntax:\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( `wc -l` / 4))\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line.\nThe backticks (``) around wc -l indicate command substitution. This means that the wc -l command is executed, and its output (the number of lines counted) is used in the overall command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”. Command substitution allows you to use the actual result of the command.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences.\n\n\n\n\n\n\n\nImagine we want to count the lines not only from one but all files. Could we do something like the code below?\n\nzcat data/seq_project/*/*.gz | wc -l\n\nWhen running this, we see that the command prints a single number, 869 944, but not the counts for each file, so something did not work right.\nThe problem with this command is that it prints the text from all 8 fastq files and only afterwards performs the counting. We basically concatenated all files and then calculated the sum of all 8 files. However, what we want to do is to repeat the same operation over and over again:\n\nDecompress a first file\nCount the lines in the first file\nDecompress a second file\nCount the lines in the second file\n…\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nLet’s start with a simple example but before that let’s introduce a simple command echo. echo is used to print information, such as Hello to the terminal:\n\necho \"Welcome 1 time!\"\n\nWe can use for-loops to print something to the screen not only one, but two, three, four … times as follows:\n\nfor i in 1 2 3; do \n    echo \"Welcome ${i} times\"\ndone\n\nAn alternative and more condensed way of writing this that you might encounter in the wild is the following:\n\nfor i in 1 2 3; do echo \"Welcome ${i} times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nLet’s try to do the same but for our files by storing the individual files found in data/seq_project/*/*.gz in the variable i and print i to the screen in a for-loop.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"I work with: File ${i}\"\ndone\n\n\nfor i in data/seq_project/*/*.gz; do: This part initializes a loop that iterates over all files matching the pattern data/seq_project/*/*.gz. The variable i is assigned each file in succession.\n\nWe can then use these variables stored in i to perform more useful operations, for example for each file, step-by-step, count the number of lines in each file by using some of the tools we have seen before:\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | wc -l\ndone\n\n\nzcat ${i} | wc -l: This is the action performed inside the loop. zcat is used to concatenate and display the content of compressed files (*.gz). The | (pipe) symbol redirects this output to wc -l, which counts the number of lines in the uncompressed content.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a for loop and count how often grep finds the pattern “TAAGA”\n\n\n\nClick me to see an answer\n\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | grep -c \"TAAGA\"\ndone\n\n\n\n\n\nSometimes it is useful to not store the full path in i, especially when we want to store the output of our loop in a new file. Luckily, we can use the list of files we stored in fastq_files.txt to rewrite this command a bit and make use of the fact that we can use cat to print something in a text file to the screen:\n\nfor i in `cat fastq_files.txt`; do  \n    zcat data/seq_project/*/${i} | wc -l\ndone\n\n\nfor i in `cat fastq_files.txt`; do: This initiates a loop that iterates over each item in the file fastq_files.txt. The backticks ` are used to execute the cat command within the backticks to read the content of the text file and assign its output to the variable i.\nzcat data/seq_project/*/${i} | wc -l: Like before, this line uncompresses (zcat) and counts (wc) the number of lines in the specified file. However, in this case, the file is determined by the content of fastq_files.txt, which contains a list of file names.\n\nAfter this modification of the code, we can very easily store the output in a file instead of printing the results to the screen.\n\n#Count the number of lines in each sequencing file\n#Store the output in a new folder\nmkdir counts \n\nfor i in `cat fastq_files.txt`; do \n    zcat data/seq_project/*/$i | wc -l &gt; counts/${i}.txt\ndone\n\nls counts/*txt \n\nhead counts/Sample-DUMMY1_R1.fastq.gz.txt\n\n\n\n\n\n\n\nTip: Avanced: better counting in for loops\n\n\n\n\n\nLet’s get a bit more advanced to show you some powerful features of bash. For this imagine that you would do this for 100 files. In this case it would be useful to see the file names next to the counts. We can achieve this by using what we have learned in the Better counting tip where we have learned about echo and command substitution.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(zcat $i | wc -l)\"\ndone\n\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on here \"$i: $(zcat $i | wc -l)\"\n$(...): These parentheses are used for command substitution. It means that the command zcat $i | wc -l is executed, and its output (the line count of the uncompressed content) is substituted in that position.\nThe double quotes (““) perform what is called a string concatenation. It concatenates the filename ($i), a colon (:), a space, and the line count obtained from the command substitution. The entire string is then passed as a single argument to the echo command.\n\nAlmost perfect, now we only want to divide this by 4:\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(( $(zcat $i | wc -l) /4 ))\"\ndone\n\nNotice, how we use both single brackets and double brackets?\n\n$((...)) in contrast to $(...) is an arithmetic expansion in Bash. It allows you to perform arithmetic operations and substitute the result into the command line.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#pwd-find-out-where-we-are",
    "href": "source/bash_intro.html#pwd-find-out-where-we-are",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After installing and starting the terminal, let’s orient ourselves by typing our first command, pwd, into the terminal and pressing enter.\n\npwd\n\npwd prints the location of the current working directory and tells you where exactly you are in the file system. When we login we typically start from what is called our home directory.\n\n\n\n\n\n\nTip: finding the desktop on different user systems\n\n\n\n\n\nYour home directory will be something like /Users/YourUserName but the path might be slightly different depending on your operating system. Below you find some help to orient yourself better for different terminal interfaces:\nFor MAC users:\n\nThe home directory should be /Users/YourUserName\nTo access the current folder in Finder you can try using open .\nYour desktop should be here /Users/YourUserName/Desktop\n\nFor Mobaxterm users:\n\nYour home directory is /home/mobaxterm\nBy default this home directory is in a temporary folder, which gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn there set Persistent home directory to a folder of your choice\n\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop\n\nFor WSL2 users:\n\nThe home directory is /home/YourUserName\nTo access the file explorer and get used to where you are you can type explorer.exe .\nThe path to the desktop would be something like this /mnt/c/Users/YourUserName/OneDrive/Desktop or /mnt/c/Users/YourUserName/Desktop",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#ls-list-the-contents-of-a-directory",
    "href": "source/bash_intro.html#ls-list-the-contents-of-a-directory",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know where we are, let’s find out what files and folders exist in our home directory. For this we can use the ls command, which allows us to list directory contents:\n\nls\n\nIn my case this returns something like this:\n\n\n\nThe colors will look different depending on the cli use but in my case I see a list of files (in bold text) and folders (green-highlighted text) in my home directory.\nSince this output can easily become over-whelming if we deal with a lot of files and folders, lets look a bit closer into how we can optimize our commands.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#the-structure-of-a-command",
    "href": "source/bash_intro.html#the-structure-of-a-command",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Let´s start with looking at the general structure of a command, which generally consists of three elements, the command itself and some optional options and arguments:\n\n\n\nUnderstanding better what a command looks like, let’s use the ls command together with the option -l. This option results in ls printing the content of a folder in a long listing format.\n\nls -l\n\nAfter running this, we should see our files and folders again but in a long format, which gives more detailed information and structures our output a bit better:",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#getting-help",
    "href": "source/bash_intro.html#getting-help",
    "title": "Introduction to Bash",
    "section": "",
    "text": "If you want to know what options are available for a command it is always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways to access the manual. These most common ways are:\n\nman ls\nls --help\nls -h",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cd-move-around-folders",
    "href": "source/bash_intro.html#cd-move-around-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Most of the time you do not want to perform your analyses in the home directory but in a dedicated folder for your project. To get started, we will learn about the cd command that allows us to move around the file system.\nThe file system is a hierarchical system used to organize files and directories. It is a tree-like structure that starts with a single directory called the root directory, which is denoted by a forward slash (/). All other files are “descendants” of the root. To move from the root into other folders, we can go via the descendants to reach the john folder as follows: /users/john.\n\n\n\nIf we specify the location of a folder or file starting from the root directory, we use what is called an absolute path. If we specify the location relative to our current directory, such as our home directory, we use a relative path.\nTo start moving around, let’s begin by moving relative to our working directory by moving into any of the folders that you saw listed after you have used ls -l. In my case I want to move into the source directory:\n\ncd source/\n\nIf you use pwd after moving around directories, you should see that a different location is printed to the screen.\nWe can also move back to our original directory using cd .., which will move us back one directory (and move us out of the source and back into the home directory).\n\ncd ..\n\nWe can also move around multiple levels. In the example below, I am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, ~, which can be used as a shortcut to move directly into our home directory from wherever you are on the file system:\n\ncd ~\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nExplore your current location with pwd and ls and move around with cd and try to get used to these three commands. If you are more comfortable, try finding your Deskop based on the tip in the section about pwd.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#mkdir-make-new-folders",
    "href": "source/bash_intro.html#mkdir-make-new-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Now that we know how to explore our surroundings, let’s make a new folder in which we start our data analysis. For this we use the mkdir command.\nTo do this, we will first move into our home directory and then create and move into a new folder called data_analysis as follows:\n\n#go into the home directory\ncd ~\n\n#in the home directory make a new folder and name it data_analysis\nmkdir data_analysis\n\n#check if new folder was generated correctly\nls\n\n#move into the newly generated folder\ncd data_analysis\n\n#check if we correctly changed our location\npwd\n\n\n\n\n\n\n\nTip: commenting your code\n\n\n\n\n\nNotice, how in the example below I added the commands to run as well as some explanation about what I did?\nHere, I used a specific character # in front of a line of text to denote the beginning of a single-line comment. Anything coming after the character is considered a commend and won’t be executed by the shell.\nIn the above example I definitely commented the code too much as my comments basically duplicate the code and that should be avoided, however, it is useful to add comments in code to ensure that the purpose of the code is clear to others.\nYou could, for example, add a comment above functions to describe what the function does and you can also add comments to “tricky” code where it is not immediately obvious what you are trying to do. Basically, you want to add comments to any section where you think that the future you might get confused a month later.\nHere you find some examples for python and R but the same logic applies when writing code for bash, some examples for that can be found here.\n\n\n\n\n\n\n\n\n\nTip: Command-line completion\n\n\n\n\n\nMost command-line interpreters allow to automatically fill in partially typed commands, file paths or file names.\nSo instead of having to type out data_analysis completely when changing the directory, we can let the interpreter do the work for us.\nTo do this, start from the home directory (or wherever you ran the mkdir command) and type cd data_ and press the Tab-key on your keyboard. The cli should have auto-completed the folder name automatically.\nIf there are multiple options, such as data and data_analysis, the cli can not autocomplete the folder name, however, by pressing Tab twice you will see all options to extend the name.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wget-download-data",
    "href": "source/bash_intro.html#wget-download-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s download our sequencing data into a new data folder. One way to do this is using wget. In the command below we add the option -P to specify into which folder we want to download the data. You an also see how I documented my code here in order to add some more specifics about where the link came from to help future me in case I, for example, need to look for some e-mails about the data later on.\n\nmkdir data\n\n#download data using link provided by the sequencing center on 23.01.2023\nwget -P data https://github.com/ndombrowski/cli_workshop/raw/main/data/seq_project.tar.gz\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nCheck with ls if the file was downloaded correctly\nCheck the manual if there are extra options that we can use for ls to check the file size (advanced)\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/\n\n#question 2:\nls -lh data/seq_project.tar.gz",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#tar-work-with-tar-files",
    "href": "source/bash_intro.html#tar-work-with-tar-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The data we downloaded is provided as a tar file:\n\ntar is short for Tape Archive, and sometimes referred to as tarball\nThe TAR file format is commonly used when storing data\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nWe can decompress the data into our data folder as follows:\n\ntar -xvf data/seq_project.tar.gz -C data\n\nThe options we use with the tar command are:\n\nx tells it to extract files from the archive\nv display verbose information and provide detailed information while creating the tarball\nf specify the file name\nC tells tar to change directory (so the package content will be unpacked there)\n\n\n\n\n\n\n\nTip: how to generate a tarball\n\n\n\n\n\nIf you ever want to generate a tarball you can do the following:\n\ntar -cvzf my_tarball.tar.gz folder_to_tar\n\nThe options we use are:\n\nc create an archive by bundling files and directories together\nz use gzip compression when generating the tar file\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter extracting the tarball use ls to explore the content of the folder we just extracted. Ensure that you explore the content of potential sub-directories.\nFind the path for at least one sequence file that ends on fastq.gz. Hint, to make your life easier, check the Tip: Command-line completion above to not have to type every single word.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project\n\n#question 2:\nls -l data/seq_project/barcode001_User1_ITS_1_L001/",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#rm-remove-files-and-folders",
    "href": "source/bash_intro.html#rm-remove-files-and-folders",
    "title": "Introduction to Bash",
    "section": "",
    "text": "To remove files and folders, we use the rm command. We want to remove the seq_project.tar.gz file since we do not need it anymore once we have extracted the data:\n\n#rm a file\nrm data/seq_project.tar.gz\n\n#check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wildcards",
    "href": "source/bash_intro.html#wildcards",
    "title": "Introduction to Bash",
    "section": "",
    "text": "After we have explored the folder with our sequencing data with ls, we have seen that data/seq_project contains 4 folders, 2 folders for two different users who each generated two replicates.\nYou also might have also noticed that it gets tedious to figure out how many files are in each folder because we would need to run ls on each single folder and view its content individually.\nLuckily, the shell provides special characters to rapidly specify groups of filenames or patterns. Wild-cards are characters that can be used as a substitute for any class of characters in a search.\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. It allows us to list all files in the seq_project folder as follows:\n\nls data/seq_project/*\n\nYou should have seen before that the sequencing files all end with .fastq.gz we can make this command a bit more specific and at the same time make the output a bit more readable:\n\nls data/seq_project/*/*.gz\n\nIf we wanted to print the absolute and not the relative path, we could do the following (beware that the absolute path will be slightly different depending in you system):\n\nls /home/User_name/data/seq_project/*/*.gz\n\nNow we can easily see for each folder how many files we have.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a wildcard to list files for barcode001 only\nUse a wildcard to list files only for user1\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls data/seq_project/barcode001*/*gz\n\n#question 2:\nls data/seq_project/*_User1_*/*gz\n\n\n\n\n\n\n\n\n\n\n\nTip: More wildcards\n\n\n\n\n\n* is not the only wildcard we can use and a full list can be found here.\nDifferent wildcards can be useful in different contexts, not only only when listing files but also finding patterns in files. To make such searches as specific as possible there are different wildcards available. A few examples are:\nThe [0-9] wildcard = matches any number exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[0-9]*/*.gz\n\nThe [012] wildcard = matches 1 or 2 exactly once and allows us to extract a range of files.\n\nls data/seq_project/barcode00[12]*/*.gz\n\n[A-Z] matches any letter in capitals occurring once\n[a-z]* matches any letter in non-capital letters occurring many times\n\nls data/seq_project/barcode001_User1_ITS_1_[A-Z]001/*.gz",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#io-redirection-to-new-files",
    "href": "source/bash_intro.html#io-redirection-to-new-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "We have seen by now that by default our commands direct print the standard output to the terminal. For example, when we use ls the list of files and folders is printed. However, we can also redirect the standard output to a file by using the &gt; character.\nFor example, we might want to generate a list with all fastq files and can do this as follows:\n\nls data/seq_project/*/* &gt; fastq_paths.txt",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-text-files",
    "href": "source/bash_intro.html#exploring-the-content-of-text-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Next, let’s view the content of the list we just generated. Viewing the actual files we work with is often important to ensure the integrity of our data.\n\n\nhead can be used to check the first 10 rows of our files:\n\nhead fastq_paths.txt\n\n\n\n\nIf you want to check the last 10 rows use tail:\n\ntail fastq_paths.txt\n\n\n\n\nless is a program that let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S fastq_paths.txt\n\nOnce opened, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n\n\n\n\nTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano fastq_paths.txt\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wc-count-things",
    "href": "source/bash_intro.html#wc-count-things",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an useful tool for sanity checking and here allows us to count how many files we work with:\n\nwc -l fastq_paths.txt\n\nAfter running this we see that we work with 8 files.\nWe could of course easily count this ourselves, however, if we work with hundreds of files its a quick and easy way to get an overview about how much data we work with.\nOne down-side of this approach is that to be able to count the number of files, we first need to generate a file in which we count the number of files. This (i) can create files we do not actually need and (ii) we use two commands while ideally we want to get the information with a single command.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#pipes",
    "href": "source/bash_intro.html#pipes",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Pipes are a powerful utility to connect multiple commands together. Pipes allow us to feed the standard output of one command, such as ls as input into another command such as wc -l and as such combine multiple commands together.\nTherefore, lets use ls to first list all fastq files and then pipe the output from ls into the wc command in order to count with how many files we work with:\n\nls data/seq_project/*/* | wc -l\n\nWe should see again that we work with 8 files, but now we did not have to generate an intermediate text file and have a more condensed command.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse ls and wc to count how many R1 files we have\nCount how many R2 files we have\nCount how many files were generated for User1?\n\n\n\nClick me to see an answer\n\n\n#question 1: We see 4 files\nls data/seq_project/*/*R1*/*gz | wc -l \n\n#question 2: We see 4 files\nls data/seq_project/*/*R2*/*gz | wc -l \n\n#question 3: We see 4 files\nls data/seq_project/*_User1_*/*gz | wc -l \n\nChecking whether we have the same number of R1 and R2 files is a good sanity check, to ensure that we have received all the files from the sequencing center whenever we generate paired sequencing data.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cut-extract-elements-from-strings",
    "href": "source/bash_intro.html#cut-extract-elements-from-strings",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Remember, how we stored a list of sequencing files and the path leading to these files (relative to our home directory in a text file called fastq_paths.txt?\nImagine that we only wanted to have a list of files but not the path, how would we do that?\nThere are different ways to do this, the simplest one is to use cd and go into the folder with our sequence files and generate a list in there.\nHowever, another way in which we do not have move around (and learn some other concepts) is to use the cut command. cut allows us to separate lines of text into different elements using any kind of delimiter, for example the / that we use in the file path. To ensure that / is seen a separator we use the -d option and with -f4 we tell cut to print the fourth element of each separated field.\n\nhead fastq_paths.txt\n\n#extract the file name (i.e. the fourth field when using a / separator)\ncut -f4 -d \"/\" fastq_paths.txt\n\n#do the same as above but this time save the output in a new file\ncut -f4 -d \"/\" fastq_paths.txt &gt; fastq_files.txt\n\nWe can also combine this with pipes in order to extract different pieces of information. Let’s assume we start with extracting a folder name, such as barcode002_User1_ITS_9_L001, and from that we want to extract some other information, such as a list with all the barcode IDs. We can easily do this as follows:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\"\n\nNice, we now only have a list with barcodes. However, its not yet ideal since we have duplicated barcodes. If you want to extract this information for some metadata file this is not ideal and we need to get to know two more commands to make this work:",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#sort-and-uniq-create-unique-lists",
    "href": "source/bash_intro.html#sort-and-uniq-create-unique-lists",
    "title": "Introduction to Bash",
    "section": "",
    "text": "sort: sort lines in a file from A-Z and is useful for file organization.\nuniq: remove or find duplicates . For this command to work you need to provide it with a sorted file\n\nLet’s first ensure that our barcode list is sorted to then extract only the unique information:\n\ncut -f3 -d \"/\" fastq_paths.txt | cut -f1 -d \"_\" | sort | uniq\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse cut to print the folder names that lead to the fastq.gz files\nPrint the folder names as in 1 but then also extract the User names\nEnsure that when running the code from 2 that we print a unique list of User names\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncut -f3 -d \"/\" fastq_paths.txt\n\n#question 2:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\"\n\n#question 3:\ncut -f3 -d \"/\" fastq_paths.txt | cut -f2 -d \"_\" | sort | uniq",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#cat-read-and-combine-files",
    "href": "source/bash_intro.html#cat-read-and-combine-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "cat can do different things:\n\nCreate new files\nDisplay the content of a file\nConcatenate, i.e. combine, several files\n\nTo view files we do:\n\ncat fastq_paths.txt\n\nTo combine files we do:\n\n#combine files \ncat fastq_paths.txt fastq_files.txt &gt; combined_files.txt\n\n#view new file \ncat combined_files.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFastq.gz files can easily be combined using cat and while this is not strictly necessary in our case, you might need to do this with your sequence files in the future. For example if you sequenced a lot of data and for space reasons the sequencing center send you multiple files for a single sample.\n\nUsing wildcards and cat, combine all R1 fastq.gz files into a single file\nUse ls to judge the file size of the individual R1 files and the combined file to assess whether everything worked correctly\n\n\n\nClick me to see an answer\n\n\n#question 1:\ncat data/seq_project/*/*R1.fastq.gz &gt; combined.fastq.gz\n\n#question 2:\nls -lh data/seq_project/*/*R1.fastq.gz\n\n#question 3: \nls -lh combined.fastq.gz\n\nIf you compare the numbers from part 2 and 3, you should see that the combined.fastq.gz file is roughly the sum of the individual files.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#exploring-the-content-of-compressed-fastq-files",
    "href": "source/bash_intro.html#exploring-the-content-of-compressed-fastq-files",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Exercise\n\n\n\n\n\n\nView the first few rows of any one of the fastq.gz sequence files\nDoes this look like a normal sequence file to you?\n\n\n\nClick me to see an answer\n\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nAfter running this command you will see a lot of random letters and numbers but nothing that looks like a sequence, so what is going on?\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloads folder, you have seen that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to (de)-compress the size of files.\nIn order to view the content of such files, we sometimes need to de-compress them first. We can do this using the gzip command together with the decompress -d option:\n\ngzip -d data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz\n\nhead data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nAfter running this, we finally see a sequence and some other information. Notice that for fastq files we always should see 4 rows with information for each sequence:\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nAfter running the gzip command above, make a list of each sequence file with ls\nUse ls with an option to also view the file size and compare the size of our compressed and uncompressed files.\n\n\n\nClick me to see an answer\n\n\n#question 1:\nls -l data/seq_project/*/*\n\n#question 2:\nls -lh data/seq_project/*/*\n\nWe see that our file has a file size of about ~25M after compression while the compressed files are around ~5M.\nWhen working with sequencing files the data is usually much large and its best to keep the files compressed to not clutter your computer.\n\n\n\n\nTo keep our files small its best to work with the compressed files. If we want to compress a file, we can do this as follows:\n\ngzip data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq\n\nls data/seq_project/*/*\n\n\n\nWe have seen that to view the content of a compressed file and make sense of the content we had to first decompress the file. However, sequence files tend to get rather large and we might not want to decompress our files to not clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen called zcat, the original compressed file is kept while doing this. We combine this command with the head command, since do not want to print millions of sequences to the screen but only want to explore the first few rows:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | head",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#grep-find-patterns-in-data",
    "href": "source/bash_intro.html#grep-find-patterns-in-data",
    "title": "Introduction to Bash",
    "section": "",
    "text": "The grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\nLet’s first search for some things in the text files we generated, for example, we might want to only print information for the R1 files:\n\n#grep a pattern, here R1, in fastq_paths.txt\ngrep \"R1\" fastq_paths.txt\n\nWe see the list of files that match our pattern. If we simply are interested in the number of files that match our pattern, we could add the option -c for counting.\n\ngrep -c \"R1\" fastq_paths.txt\n\nWe can also combine this with wildcards, for example, if we look for all samples from User1 we could do the following:\n\ngrep \"User1_ITS_*_\" fastq_paths.txt\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nGrep for paths containing the pattern “ITS_9” in fastq_paths.txt\nGrep for the pattern “AAGACG” in any of the fastq.gz files\nCount how often “AAGACG” occurs in any in any of the fastq.gz files\n\n\n\nClick me to see an answer\n\n\n#1\ngrep \"ITS_9\" fastq_paths.txt\n\n#2\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep \"AAGACG\"\n\n#3\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz | grep -c \"AAGACG\"\n\nThe last two commands can be very useful to check if if the adapters or primers are still part of your sequence.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#wc-count-things-1",
    "href": "source/bash_intro.html#wc-count-things-1",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Another useful tool is the wc (= wordcount) command that allows us to count the number of lines in a file. It is an easy way to perform sanity checks and in our case allows us to count how many sequences we work with:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | wc -l\n\nAfter running this we see that we work with 179,384 / 4 sequences. We need to divide the number we see on the screen by 4 since each sequence is represented by 4 lines of information in our fastq file.\n\n\n\n\n\n\nAvanced Tip: better counting\n\n\n\n\n\nWe have seen that we need to divide the output of wc by four to get the total number of sequences. We can do this with a calculator but actually, some intermediate bash can also be used to do this for us:\n\nzcat data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R2.fastq.gz | echo $((`wc -l`/4))\n\nIn the command above we have some new syntax:\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on in this section $(( `wc -l` / 4))\n$((...)): This is an arithmetic expansion in Bash. It allows you to perform arithmetic operations inside the brackets and substitute the result into the command line.\nThe backticks (``) around wc -l indicate command substitution. This means that the wc -l command is executed, and its output (the number of lines counted) is used in the overall command. Without command substitution (wc -l alone), you would not capture the output; instead, you would only see the literal text “wc -l”. Command substitution allows you to use the actual result of the command.\n/4: This is dividing the result obtained from wc -l by 4. Since each sequence in a FASTQ file is represented by four lines (identifier, sequence, separator, and quality scores), dividing the total number of lines by 4 gives the number of sequences.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "source/bash_intro.html#for-loops",
    "href": "source/bash_intro.html#for-loops",
    "title": "Introduction to Bash",
    "section": "",
    "text": "Imagine we want to count the lines not only from one but all files. Could we do something like the code below?\n\nzcat data/seq_project/*/*.gz | wc -l\n\nWhen running this, we see that the command prints a single number, 869 944, but not the counts for each file, so something did not work right.\nThe problem with this command is that it prints the text from all 8 fastq files and only afterwards performs the counting. We basically concatenated all files and then calculated the sum of all 8 files. However, what we want to do is to repeat the same operation over and over again:\n\nDecompress a first file\nCount the lines in the first file\nDecompress a second file\nCount the lines in the second file\n…\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nLet’s start with a simple example but before that let’s introduce a simple command echo. echo is used to print information, such as Hello to the terminal:\n\necho \"Welcome 1 time!\"\n\nWe can use for-loops to print something to the screen not only one, but two, three, four … times as follows:\n\nfor i in 1 2 3; do \n    echo \"Welcome ${i} times\"\ndone\n\nAn alternative and more condensed way of writing this that you might encounter in the wild is the following:\n\nfor i in 1 2 3; do echo \"Welcome ${i} times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nLet’s try to do the same but for our files by storing the individual files found in data/seq_project/*/*.gz in the variable i and print i to the screen in a for-loop.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"I work with: File ${i}\"\ndone\n\n\nfor i in data/seq_project/*/*.gz; do: This part initializes a loop that iterates over all files matching the pattern data/seq_project/*/*.gz. The variable i is assigned each file in succession.\n\nWe can then use these variables stored in i to perform more useful operations, for example for each file, step-by-step, count the number of lines in each file by using some of the tools we have seen before:\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | wc -l\ndone\n\n\nzcat ${i} | wc -l: This is the action performed inside the loop. zcat is used to concatenate and display the content of compressed files (*.gz). The | (pipe) symbol redirects this output to wc -l, which counts the number of lines in the uncompressed content.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUse a for loop and count how often grep finds the pattern “TAAGA”\n\n\n\nClick me to see an answer\n\n\nfor i in data/seq_project/*/*.gz; do \n    zcat ${i} | grep -c \"TAAGA\"\ndone\n\n\n\n\n\nSometimes it is useful to not store the full path in i, especially when we want to store the output of our loop in a new file. Luckily, we can use the list of files we stored in fastq_files.txt to rewrite this command a bit and make use of the fact that we can use cat to print something in a text file to the screen:\n\nfor i in `cat fastq_files.txt`; do  \n    zcat data/seq_project/*/${i} | wc -l\ndone\n\n\nfor i in `cat fastq_files.txt`; do: This initiates a loop that iterates over each item in the file fastq_files.txt. The backticks ` are used to execute the cat command within the backticks to read the content of the text file and assign its output to the variable i.\nzcat data/seq_project/*/${i} | wc -l: Like before, this line uncompresses (zcat) and counts (wc) the number of lines in the specified file. However, in this case, the file is determined by the content of fastq_files.txt, which contains a list of file names.\n\nAfter this modification of the code, we can very easily store the output in a file instead of printing the results to the screen.\n\n#Count the number of lines in each sequencing file\n#Store the output in a new folder\nmkdir counts \n\nfor i in `cat fastq_files.txt`; do \n    zcat data/seq_project/*/$i | wc -l &gt; counts/${i}.txt\ndone\n\nls counts/*txt \n\nhead counts/Sample-DUMMY1_R1.fastq.gz.txt\n\n\n\n\n\n\n\nTip: Avanced: better counting in for loops\n\n\n\n\n\nLet’s get a bit more advanced to show you some powerful features of bash. For this imagine that you would do this for 100 files. In this case it would be useful to see the file names next to the counts. We can achieve this by using what we have learned in the Better counting tip where we have learned about echo and command substitution.\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(zcat $i | wc -l)\"\ndone\n\n\nThe echo command is used to display messages or print information to the terminal. In our case it will print whatever is going on here \"$i: $(zcat $i | wc -l)\"\n$(...): These parentheses are used for command substitution. It means that the command zcat $i | wc -l is executed, and its output (the line count of the uncompressed content) is substituted in that position.\nThe double quotes (““) perform what is called a string concatenation. It concatenates the filename ($i), a colon (:), a space, and the line count obtained from the command substitution. The entire string is then passed as a single argument to the echo command.\n\nAlmost perfect, now we only want to divide this by 4:\n\nfor i in data/seq_project/*/*.gz; do \n    echo \"$i: $(( $(zcat $i | wc -l) /4 ))\"\ndone\n\nNotice, how we use both single brackets and double brackets?\n\n$((...)) in contrast to $(...) is an arithmetic expansion in Bash. It allows you to perform arithmetic operations and substitute the result into the command line.",
    "crumbs": [
      "Cli basics",
      "Introduction to Bash"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will provide guidance in how to use bash and the command line interface.\nThe first few pages will introduce you how to set up a terminal and document your code.\nAfterwards, the actual tutorial will begin, in which we will assume the role of a researcher who just received amplicon sequencing data from a sequencing center and wants to starts analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, we will learn how to organize and how to view and exploring the data on the filesystem. We will start with some basic operations to assess with how much data we work with and extract information from our files.\nAfter becoming familiar with the command line, we will learn how to analyze our data on an HPC (since often analysing large datasets on a personal computer can become quite resource intensive). We will learn how to connect to an HPC, download some software to assess the quality of our sequencing data and submit jobs to computational noted. If you do not have access to an HPC you can follow most parts on your own computer as the dataset we provide is a small one.\n\nFor information not covered in this tutorial, check out IBED’s bioinformatics guidance page.",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "index.html#welcome-page",
    "href": "index.html#welcome-page",
    "title": "Introduction to the cli",
    "section": "",
    "text": "On this website you will find a short tutorial that will provide guidance in how to use bash and the command line interface.\nThe first few pages will introduce you how to set up a terminal and document your code.\nAfterwards, the actual tutorial will begin, in which we will assume the role of a researcher who just received amplicon sequencing data from a sequencing center and wants to starts analysing this data using the command line interface. This tutorial is divided into two main parts:\n\nIn the first section, we will learn how to organize and how to view and exploring the data on the filesystem. We will start with some basic operations to assess with how much data we work with and extract information from our files.\nAfter becoming familiar with the command line, we will learn how to analyze our data on an HPC (since often analysing large datasets on a personal computer can become quite resource intensive). We will learn how to connect to an HPC, download some software to assess the quality of our sequencing data and submit jobs to computational noted. If you do not have access to an HPC you can follow most parts on your own computer as the dataset we provide is a small one.\n\nFor information not covered in this tutorial, check out IBED’s bioinformatics guidance page.",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "tba"
  },
  {
    "objectID": "readme.html#title",
    "href": "readme.html#title",
    "title": "Introduction to the cli",
    "section": "",
    "text": "tba"
  },
  {
    "objectID": "source/code_documentation.html",
    "href": "source/code_documentation.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might work with your code. Documentation serves as a roadmap for your code. It helps others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R code but most are general enough to apply to other computational langauges (and scientific disciplines).\nIf you want to see an example for documented code, check out [this example file]tba. The example file is a markdown file (qmd) generated with VSCode. Some examples for text editors to use can be found below.\n\n\n\n\n\n\nNote\n\n\n\nThe information in this section is not part of the actual tutorial but was added to give you a starting point for how to document your code.\nIf you follow the in-person tutorial it is best to record your notes using a plain text editor but feel free to explore the more advanced options after the tutorial.\n\n\n\n\n\n\n\nWhen documenting code, its best to avoid visual editors, such as word, as they are not designed for writing code and easily destroy the formating by for example changing ` to ’, which when writing code is quite a big difference.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as adding headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown that allows you to integrate R code directly into your documentation.\nIf you have not install R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nKnit the Document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio but can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different compuational languages.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.\n\n\n\n\n\n\nMarkdown is a lightweight markup language that’s easy to read and write. It allows you to add formatting elements to plain text documents.\nHeaders:\nUse # for headers. The more # symbols, the smaller the header. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers don’t have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nCreate links to external resources or within your documentation.\n[Link Text](https://www.example.com)\nEmphasis:\nUse * or _ for italic and ** or __ for bold.\n*italic*\n**bold**\nPictures\nYou can easily add images to your documentation as well:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/ifrom spamage.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/code_documentation.html#documenting-code",
    "href": "source/code_documentation.html#documenting-code",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might work with your code. Documentation serves as a roadmap for your code. It helps others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R code but most are general enough to apply to other computational langauges (and scientific disciplines).\nIf you want to see an example for documented code, check out [this example file]tba. The example file is a markdown file (qmd) generated with VSCode. Some examples for text editors to use can be found below.\n\n\n\n\n\n\nNote\n\n\n\nThe information in this section is not part of the actual tutorial but was added to give you a starting point for how to document your code.\nIf you follow the in-person tutorial it is best to record your notes using a plain text editor but feel free to explore the more advanced options after the tutorial.",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/code_documentation.html#choose-your-editor",
    "href": "source/code_documentation.html#choose-your-editor",
    "title": "Introduction to the cli",
    "section": "",
    "text": "When documenting code, its best to avoid visual editors, such as word, as they are not designed for writing code and easily destroy the formating by for example changing ` to ’, which when writing code is quite a big difference.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as adding headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown that allows you to integrate R code directly into your documentation.\nIf you have not install R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nKnit the Document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio but can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different compuational languages.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/code_documentation.html#markdown-for-documentation",
    "href": "source/code_documentation.html#markdown-for-documentation",
    "title": "Introduction to the cli",
    "section": "",
    "text": "Markdown is a lightweight markup language that’s easy to read and write. It allows you to add formatting elements to plain text documents.\nHeaders:\nUse # for headers. The more # symbols, the smaller the header. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers don’t have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nCreate links to external resources or within your documentation.\n[Link Text](https://www.example.com)\nEmphasis:\nUse * or _ for italic and ** or __ for bold.\n*italic*\n**bold**\nPictures\nYou can easily add images to your documentation as well:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/ifrom spamage.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "Cli basics",
      "Documenting code"
    ]
  },
  {
    "objectID": "source/hpc_intro.html",
    "href": "source/hpc_intro.html",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes are used to actually run a program.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run jobs that request many cpus and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that deleting files which are in a snapshot will not be available for another 2 weeks. It also means that if you accidentally remove a file it can be restored up to 2 weeks after removal.\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. The data is not replicated and you are responsible for backing up and/or archiving your data.\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM.",
    "crumbs": [
      "Using an HPC",
      "HPC introduction"
    ]
  },
  {
    "objectID": "source/hpc_intro.html#hpc-introduction",
    "href": "source/hpc_intro.html#hpc-introduction",
    "title": "Introduction to the cli",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes are used to actually run a program.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run jobs that request many cpus and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that deleting files which are in a snapshot will not be available for another 2 weeks. It also means that if you accidentally remove a file it can be restored up to 2 weeks after removal.\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. The data is not replicated and you are responsible for backing up and/or archiving your data.\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM.",
    "crumbs": [
      "Using an HPC",
      "HPC introduction"
    ]
  }
]